{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e613346-c162-4bf8-908d-8e6a5cff0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from pyod.utils.data import precision_n_scores\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Per l'uso della memoria degli algoritmi\n",
    "from memory_profiler import memory_usage\n",
    "# Per la metrica sul tempo di Addestramento e Inferenza\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba180484-282d-4605-990e-bf354cf53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc90bc09-5125-4641-bf52-cd0d4cc7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mean\", \"var\", \"std\", \"len\", \"duration\", \"len_weighted\", \"gaps_squared\", \"n_peaks\",\n",
    "    \"smooth10_n_peaks\", \"smooth20_n_peaks\", \"var_div_duration\", \"var_div_len\",\n",
    "    \"diff_peaks\", \"diff2_peaks\", \"diff_var\", \"diff2_var\", \"kurtosis\", \"skew\",\n",
    "]\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fffebd2-36a4-4c5f-82e0-3b9e4f6c3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "6       0\n",
      "       ..\n",
      "2118    0\n",
      "2120    0\n",
      "2121    0\n",
      "2122    0\n",
      "2123    1\n",
      "Name: anomaly, Length: 1594, dtype: int64\n",
      "X_train (1594, 18)\n",
      "X_test (529, 18)\n",
      "X_train2 (1594, 18)\n",
      "X_test2 (529, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", index_col=\"segment\")\n",
    "\n",
    "X_train, y_train = df.loc[df.train==1, features], df.loc[df.train==1, \"anomaly\"]\n",
    "print(y_train)\n",
    "X_test, y_test = df.loc[df.train==0, features], df.loc[df.train==0, \"anomaly\"]\n",
    "X_train_nominal = df.loc[(df.anomaly==0)&(df.train==1), features]\n",
    "\n",
    "prep = StandardScaler()\n",
    "X_train_nominal2 = prep.fit_transform(X_train_nominal)\n",
    "X_train2 = prep.transform(X_train)\n",
    "X_test2 = prep.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"X_train2\", X_train2.shape)\n",
    "print(\"X_test2\", X_test2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5cb2f65-dba5-431f-a7c1-dc4db5d1fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_numpy(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938333f9",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396d74e0-6e0c-40c2-beaf-ea856b6a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(random_state=2137) \n",
      " {'Accuracy': 0.934, 'Precision': 0.89, 'Recall': 0.788, 'F1': 0.836, 'MCC': 0.797, 'AUC_PR': 0.923, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.841}\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=SEED)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2e8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.957, 'Precision': 0.959, 'Recall': 0.832, 'F1': 0.891, 'MCC': 0.867, 'AUC_PR': 0.961, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_score = model.predict_proba(X_test)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa6aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.953, 'Precision': 0.94, 'Recall': 0.832, 'F1': 0.883, 'MCC': 0.856, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.predict_proba(X_test_scaled)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f258319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.928, 'Precision': 0.921, 'Recall': 0.726, 'F1': 0.812, 'MCC': 0.777, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LinearSVC()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d90807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.924, 'Precision': 0.92, 'Recall': 0.708, 'F1': 0.8, 'MCC': 0.764, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f6a6",
   "metadata": {},
   "source": [
    "# Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d242",
   "metadata": {},
   "source": [
    "MO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d742e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmo_gaal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MO_GAAL\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\mo_gaal.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install torch first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _utils\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\graph_settings.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[0;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_random_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_sharding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_graph_pipes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe \u001b[38;5;28;01mas\u001b[39;00m dataframe, \u001b[38;5;28miter\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      3\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Concater,\n\u001b[0;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Demultiplexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ZipperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Zipper,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilelister\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     FileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m FileLister,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "model = MO_GAAL()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n",
    " # {'Accuracy': 0.896, 'Precision': 0.939, 'Recall': 0.549, 'F1': 0.693, 'MCC': 0.669, 'AUC_PR': 0.771, 'AUC_ROC': 0.849, 'PREC_N_SCORES': 0.699}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a030",
   "metadata": {},
   "source": [
    "ANO-GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter: 1\n",
      "Train iter: 2\n",
      "Train iter: 3\n",
      "Train iter: 4\n",
      "Train iter: 5\n",
      "Train iter: 6\n",
      "Train iter: 7\n",
      "Train iter: 8\n",
      "Train iter: 9\n",
      "Train iter: 10\n",
      "Train iter: 11\n",
      "Train iter: 12\n",
      "Train iter: 13\n",
      "Train iter: 14\n",
      "Train iter: 15\n",
      "Train iter: 16\n",
      "Train iter: 17\n",
      "Train iter: 18\n",
      "Train iter: 19\n",
      "Train iter: 20\n",
      "Train iter: 21\n",
      "Train iter: 22\n",
      "Train iter: 23\n",
      "Train iter: 24\n",
      "Train iter: 25\n",
      "Train iter: 26\n",
      "Train iter: 27\n",
      "Train iter: 28\n",
      "Train iter: 29\n",
      "Train iter: 30\n",
      "Train iter: 31\n",
      "Train iter: 32\n",
      "Train iter: 33\n",
      "Train iter: 34\n",
      "Train iter: 35\n",
      "Train iter: 36\n",
      "Train iter: 37\n",
      "Train iter: 38\n",
      "Train iter: 39\n",
      "Train iter: 40\n",
      "Train iter: 41\n",
      "Train iter: 42\n",
      "Train iter: 43\n",
      "Train iter: 44\n",
      "Train iter: 45\n",
      "Train iter: 46\n",
      "Train iter: 47\n",
      "Train iter: 48\n",
      "Train iter: 49\n",
      "Train iter: 50\n",
      "Train iter: 51\n",
      "Train iter: 52\n",
      "Train iter: 53\n",
      "Train iter: 54\n",
      "Train iter: 55\n",
      "Train iter: 56\n",
      "Train iter: 57\n",
      "Train iter: 58\n",
      "Train iter: 59\n",
      "Train iter: 60\n",
      "Train iter: 61\n",
      "Train iter: 62\n",
      "Train iter: 63\n",
      "Train iter: 64\n",
      "Train iter: 65\n",
      "Train iter: 66\n",
      "Train iter: 67\n",
      "Train iter: 68\n",
      "Train iter: 69\n",
      "Train iter: 70\n",
      "Train iter: 71\n",
      "Train iter: 72\n",
      "Train iter: 73\n",
      "Train iter: 74\n",
      "Train iter: 75\n",
      "Train iter: 76\n",
      "Train iter: 77\n",
      "Train iter: 78\n",
      "Train iter: 79\n",
      "Train iter: 80\n",
      "Train iter: 81\n",
      "Train iter: 82\n",
      "Train iter: 83\n",
      "Train iter: 84\n",
      "Train iter: 85\n",
      "Train iter: 86\n",
      "Train iter: 87\n",
      "Train iter: 88\n",
      "Train iter: 89\n",
      "Train iter: 90\n",
      "Train iter: 91\n",
      "Train iter: 92\n",
      "Train iter: 93\n",
      "Train iter: 94\n",
      "Train iter: 95\n",
      "Train iter: 96\n",
      "Train iter: 97\n",
      "Train iter: 98\n",
      "Train iter: 99\n",
      "Train iter: 100\n",
      "Train iter: 101\n",
      "Train iter: 102\n",
      "Train iter: 103\n",
      "Train iter: 104\n",
      "Train iter: 105\n",
      "Train iter: 106\n",
      "Train iter: 107\n",
      "Train iter: 108\n",
      "Train iter: 109\n",
      "Train iter: 110\n",
      "Train iter: 111\n",
      "Train iter: 112\n",
      "Train iter: 113\n",
      "Train iter: 114\n",
      "Train iter: 115\n",
      "Train iter: 116\n",
      "Train iter: 117\n",
      "Train iter: 118\n",
      "Train iter: 119\n",
      "Train iter: 120\n",
      "Train iter: 121\n",
      "Train iter: 122\n",
      "Train iter: 123\n",
      "Train iter: 124\n",
      "Train iter: 125\n",
      "Train iter: 126\n",
      "Train iter: 127\n",
      "Train iter: 128\n",
      "Train iter: 129\n",
      "Train iter: 130\n",
      "Train iter: 131\n",
      "Train iter: 132\n",
      "Train iter: 133\n",
      "Train iter: 134\n",
      "Train iter: 135\n",
      "Train iter: 136\n",
      "Train iter: 137\n",
      "Train iter: 138\n",
      "Train iter: 139\n",
      "Train iter: 140\n",
      "Train iter: 141\n",
      "Train iter: 142\n",
      "Train iter: 143\n",
      "Train iter: 144\n",
      "Train iter: 145\n",
      "Train iter: 146\n",
      "Train iter: 147\n",
      "Train iter: 148\n",
      "Train iter: 149\n",
      "Train iter: 150\n",
      "Train iter: 151\n",
      "Train iter: 152\n",
      "Train iter: 153\n",
      "Train iter: 154\n",
      "Train iter: 155\n",
      "Train iter: 156\n",
      "Train iter: 157\n",
      "Train iter: 158\n",
      "Train iter: 159\n",
      "Train iter: 160\n",
      "Train iter: 161\n",
      "Train iter: 162\n",
      "Train iter: 163\n",
      "Train iter: 164\n",
      "Train iter: 165\n",
      "Train iter: 166\n",
      "Train iter: 167\n",
      "Train iter: 168\n",
      "Train iter: 169\n",
      "Train iter: 170\n",
      "Train iter: 171\n",
      "Train iter: 172\n",
      "Train iter: 173\n",
      "Train iter: 174\n",
      "Train iter: 175\n",
      "Train iter: 176\n",
      "Train iter: 177\n",
      "Train iter: 178\n",
      "Train iter: 179\n",
      "Train iter: 180\n",
      "Train iter: 181\n",
      "Train iter: 182\n",
      "Train iter: 183\n",
      "Train iter: 184\n",
      "Train iter: 185\n",
      "Train iter: 186\n",
      "Train iter: 187\n",
      "Train iter: 188\n",
      "Train iter: 189\n",
      "Train iter: 190\n",
      "Train iter: 191\n",
      "Train iter: 192\n",
      "Train iter: 193\n",
      "Train iter: 194\n",
      "Train iter: 195\n",
      "Train iter: 196\n",
      "Train iter: 197\n",
      "Train iter: 198\n",
      "Train iter: 199\n",
      "Train iter: 200\n",
      "Train iter: 201\n",
      "Train iter: 202\n",
      "Train iter: 203\n",
      "Train iter: 204\n",
      "Train iter: 205\n",
      "Train iter: 206\n",
      "Train iter: 207\n",
      "Train iter: 208\n",
      "Train iter: 209\n",
      "Train iter: 210\n",
      "Train iter: 211\n",
      "Train iter: 212\n",
      "Train iter: 213\n",
      "Train iter: 214\n",
      "Train iter: 215\n",
      "Train iter: 216\n",
      "Train iter: 217\n",
      "Train iter: 218\n",
      "Train iter: 219\n",
      "Train iter: 220\n",
      "Train iter: 221\n",
      "Train iter: 222\n",
      "Train iter: 223\n",
      "Train iter: 224\n",
      "Train iter: 225\n",
      "Train iter: 226\n",
      "Train iter: 227\n",
      "Train iter: 228\n",
      "Train iter: 229\n",
      "Train iter: 230\n",
      "Train iter: 231\n",
      "Train iter: 232\n",
      "Train iter: 233\n",
      "Train iter: 234\n",
      "Train iter: 235\n",
      "Train iter: 236\n",
      "Train iter: 237\n",
      "Train iter: 238\n",
      "Train iter: 239\n",
      "Train iter: 240\n",
      "Train iter: 241\n",
      "Train iter: 242\n",
      "Train iter: 243\n",
      "Train iter: 244\n",
      "Train iter: 245\n",
      "Train iter: 246\n",
      "Train iter: 247\n",
      "Train iter: 248\n",
      "Train iter: 249\n",
      "Train iter: 250\n",
      "Train iter: 251\n",
      "Train iter: 252\n",
      "Train iter: 253\n",
      "Train iter: 254\n",
      "Train iter: 255\n",
      "Train iter: 256\n",
      "Train iter: 257\n",
      "Train iter: 258\n",
      "Train iter: 259\n",
      "Train iter: 260\n",
      "Train iter: 261\n",
      "Train iter: 262\n",
      "Train iter: 263\n",
      "Train iter: 264\n",
      "Train iter: 265\n",
      "Train iter: 266\n",
      "Train iter: 267\n",
      "Train iter: 268\n",
      "Train iter: 269\n",
      "Train iter: 270\n",
      "Train iter: 271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AnoGAN(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# per stampare più cose\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     12\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:314\u001b[0m, in \u001b[0;36mAnoGAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    310\u001b[0m latent_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(X_train_sel\u001b[38;5;241m.\u001b[39msize(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim_G, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(latent_noise)\n\u001b[1;32m--> 314\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_data\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    317\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(real_output, torch\u001b[38;5;241m.\u001b[39mones_like(\n\u001b[0;32m    318\u001b[0m     real_output) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:87\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:327\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\"\n",
    "\n",
    "# Ora importa PyOD e usa AnoGAN come prima\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import tensorflow as tf\n",
    "\n",
    "model = AnoGAN(verbose=1) # per stampare più cose\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682324",
   "metadata": {},
   "source": [
    "SO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione X_train: (1594, 18)\n",
      "Dimensione y_train: (1594,)\n",
      "Dimensione X_test: (529, 18)\n",
      "Dimensione y_test: (529,)\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "SO_GAAL(contamination=0.1, lr_d=0.01, lr_g=0.0001, momentum=0.9,\n",
      "    stop_epochs=20) \n",
      " {'Accuracy': 0.887, 'Precision': 0.934, 'Recall': 0.504, 'F1': 0.655, 'MCC': 0.635, 'AUC_PR': 0.757, 'AUC_ROC': 0.839, 'PREC_N_SCORES': 0.681}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "\n",
    "# Verifica le dimensioni dei dati generati\n",
    "print(\"Dimensione X_train:\", X_train.shape)\n",
    "print(\"Dimensione y_train:\", y_train.shape)\n",
    "print(\"Dimensione X_test:\", X_test.shape)\n",
    "print(\"Dimensione y_test:\", y_test.shape)\n",
    "\n",
    "model = SO_GAAL()\n",
    "model.fit(X_train2[:len(X_train2) // 500 * 500])\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4017",
   "metadata": {},
   "source": [
    "RF+ICCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Inizializza e addestra il modello\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Previsioni e probabilità di previsione\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "# Predizione\n",
    "y_test_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae849e",
   "metadata": {},
   "source": [
    "Linear+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.902, 'Precision': 0.969, 'Recall': 0.558, 'F1': 0.708, 'MCC': 0.69, 'AUC_PR': 0.889, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.814}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Inizializza e addestra il modello Ridge Classifier (Linear + L2)\n",
    "model = RidgeClassifier(alpha=1.0)  # 'alpha' è il parametro di regolarizzazione L2\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predizione delle etichette di classe\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "# Ottieni le probabilità della classe positiva per AUC (si utilizza decision_function per ottenere punteggi di decisione)\n",
    "y_test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Calcola e stampa le metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_test_scores)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed48a",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69cca-f485-4810-b146-d00d216c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest(behaviour='old', bootstrap=False, contamination=0.2, max_features=1.0,\n",
      "    max_samples='auto', n_estimators=100, n_jobs=1, random_state=2137,\n",
      "    verbose=0) \n",
      " {'Accuracy': 0.701, 'Precision': 0.297, 'Recall': 0.292, 'F1': 0.295, 'MCC': 0.105, 'AUC_PR': 0.347, 'AUC_ROC': 0.635, 'PREC_N_SCORES': 0.301}\n"
     ]
    }
   ],
   "source": [
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f31c8",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3608e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0) \n",
      " {'Accuracy': 0.849, 'Precision': 0.78, 'Recall': 0.407, 'F1': 0.535, 'MCC': 0.489, 'AUC_PR': 0.658, 'AUC_ROC': 0.852, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28682eb3",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCSVM(cache_size=200, coef0=0.0, contamination=0.1, degree=3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False) \n",
      " {'Accuracy': 0.837, 'Precision': 0.721, 'Recall': 0.389, 'F1': 0.506, 'MCC': 0.447, 'AUC_PR': 0.659, 'AUC_ROC': 0.788, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "model = OCSVM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40d2e",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOD(contamination=0.1, method='fast', n_neighbors=5) \n",
      " {'Accuracy': 0.845, 'Precision': 0.782, 'Recall': 0.381, 'F1': 0.512, 'MCC': 0.472, 'AUC_PR': 0.644, 'AUC_ROC': 0.843, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "\n",
    "model = ABOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03191a05",
   "metadata": {},
   "source": [
    "INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE(contamination=0.1, max_samples='auto', n_estimators=200,\n",
      "   random_state=None) \n",
      " {'Accuracy': 0.832, 'Precision': 0.694, 'Recall': 0.381, 'F1': 0.491, 'MCC': 0.427, 'AUC_PR': 0.636, 'AUC_ROC': 0.805, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "model = INNE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5366a",
   "metadata": {},
   "source": [
    "ALAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALAD(activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
      "   add_disc_zz_loss=True, add_recon_loss=False, batch_size=32,\n",
      "   contamination=0.1, dec_layers=[5, 10, 25], device=device(type='cpu'),\n",
      "   disc_xx_layers=[25, 10, 5], disc_xz_layers=[25, 10, 5],\n",
      "   disc_zz_layers=[25, 10, 5], dropout_rate=0.2, enc_layers=[25, 10, 5],\n",
      "   epochs=200, lambda_recon_loss=0.1, latent_dim=2,\n",
      "   learning_rate_disc=0.0001, learning_rate_gen=0.0001,\n",
      "   output_activation=None, preprocessing=False,\n",
      "   spectral_normalization=False, verbose=0) \n",
      " {'Accuracy': 0.783, 'Precision': 0.485, 'Recall': 0.283, 'F1': 0.358, 'MCC': 0.25, 'AUC_PR': 0.426, 'AUC_ROC': 0.626, 'PREC_N_SCORES': 0.407}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "\n",
    "model = ALAD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff966da",
   "metadata": {},
   "source": [
    "LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDD(contamination=0.1, dis_measure='aad', n_iter=50, random_state=None) \n",
      " {'Accuracy': 0.822, 'Precision': 1.0, 'Recall': 0.168, 'F1': 0.288, 'MCC': 0.37, 'AUC_PR': 0.624, 'AUC_ROC': 0.765, 'PREC_N_SCORES': 0.663}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "\n",
    "model = LMDD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca5e0",
   "metadata": {},
   "source": [
    "SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOD(alpha=0.8, contamination=0.1, n_neighbors=20, ref_set=10) \n",
      " {'Accuracy': 0.826, 'Precision': 0.611, 'Recall': 0.513, 'F1': 0.558, 'MCC': 0.453, 'AUC_PR': 0.621, 'AUC_ROC': 0.797, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sod import SOD\n",
    "\n",
    "model = SOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df12fb",
   "metadata": {},
   "source": [
    "COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578deac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COF(contamination=0.1, method='fast', n_neighbors=20) \n",
      " {'Accuracy': 0.834, 'Precision': 0.667, 'Recall': 0.442, 'F1': 0.532, 'MCC': 0.449, 'AUC_PR': 0.603, 'AUC_ROC': 0.774, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cof import COF\n",
    "\n",
    "model = COF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35130602",
   "metadata": {},
   "source": [
    "LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12782922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LODA(contamination=0.1, n_bins=10, n_random_cuts=100) \n",
      " {'Accuracy': 0.83, 'Precision': 0.689, 'Recall': 0.372, 'F1': 0.483, 'MCC': 0.418, 'AUC_PR': 0.549, 'AUC_ROC': 0.692, 'PREC_N_SCORES': 0.522}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.loda import LODA\n",
    "\n",
    "model = LODA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f73c",
   "metadata": {},
   "source": [
    "LUNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
      "   n_epochs=200, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
      "   scaler=MinMaxScaler(), val_size=0.1, verbose=0, wd=0.1) \n",
      " {'Accuracy': 0.815, 'Precision': 0.742, 'Recall': 0.204, 'F1': 0.319, 'MCC': 0.322, 'AUC_PR': 0.539, 'AUC_ROC': 0.796, 'PREC_N_SCORES': 0.451}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "model = LUNAR()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d43ae",
   "metadata": {},
   "source": [
    "CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
      "   contamination=0.1, n_clusters=8, n_jobs=None, random_state=None,\n",
      "   use_weights=False) \n",
      " {'Accuracy': 0.802, 'Precision': 0.569, 'Recall': 0.292, 'F1': 0.386, 'MCC': 0.304, 'AUC_PR': 0.45, 'AUC_ROC': 0.574, 'PREC_N_SCORES': 0.372}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "\n",
    "model = CBLOF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d538",
   "metadata": {},
   "source": [
    "DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF(batch_size=1000, contamination=0.1, device=device(type='cpu'),\n",
      "  hidden_activation='tanh', hidden_neurons=[500, 100], max_samples=256,\n",
      "  n_ensemble=50, n_estimators=6, random_state=None, representation_dim=20,\n",
      "  skip_connection=False) \n",
      " {'Accuracy': 0.786, 'Precision': 0.5, 'Recall': 0.009, 'F1': 0.017, 'MCC': 0.043, 'AUC_PR': 0.541, 'AUC_ROC': 0.836, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.dif import DIF\n",
    "\n",
    "model = DIF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.predict_proba(X_test2)[:,1]\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12a8b",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(batch_norm=False, batch_size=32, beta=1.0, capacity=0.0,\n",
      "  compile_mode='default', contamination=0.1,\n",
      "  decoder_neuron_list=[32, 64, 128], device=device(type='cpu'),\n",
      "  dropout_rate=0.2, encoder_neuron_list=[128, 64, 32], epoch_num=30,\n",
      "  hidden_activation_name='relu', latent_dim=2, lr=0.001,\n",
      "  optimizer_name='adam', optimizer_params={'weight_decay': 1e-05},\n",
      "  output_activation_name='sigmoid', preprocessing=True, random_state=42,\n",
      "  use_compile=False, verbose=1) \n",
      " {'Accuracy': 0.794, 'Precision': 0.532, 'Recall': 0.292, 'F1': 0.377, 'MCC': 0.283, 'AUC_PR': 0.446, 'AUC_ROC': 0.687, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "model = VAE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c38a",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(contamination=0.1, covariance_type='full', init_params='kmeans',\n",
      "  max_iter=100, means_init=None, n_components=1, n_init=1,\n",
      "  precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001,\n",
      "  warm_start=False, weights_init=None) \n",
      " {'Accuracy': 0.783, 'Precision': 0.482, 'Recall': 0.239, 'F1': 0.32, 'MCC': 0.225, 'AUC_PR': 0.426, 'AUC_ROC': 0.713, 'PREC_N_SCORES': 0.389}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "\n",
    "model = GMM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8a4e4",
   "metadata": {},
   "source": [
    "DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f094a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.17359483242035\n",
      "Epoch 2/100, Loss: 36.19166633486748\n",
      "Epoch 3/100, Loss: 36.2466336786747\n",
      "Epoch 4/100, Loss: 36.13528761267662\n",
      "Epoch 5/100, Loss: 36.165921211242676\n",
      "Epoch 6/100, Loss: 36.13916572928429\n",
      "Epoch 7/100, Loss: 36.189294904470444\n",
      "Epoch 8/100, Loss: 36.17238187789917\n",
      "Epoch 9/100, Loss: 36.2117395401001\n",
      "Epoch 10/100, Loss: 36.185857594013214\n",
      "Epoch 11/100, Loss: 36.13321906328201\n",
      "Epoch 12/100, Loss: 36.1584706902504\n",
      "Epoch 13/100, Loss: 36.17630282044411\n",
      "Epoch 14/100, Loss: 36.17380636930466\n",
      "Epoch 15/100, Loss: 36.25334322452545\n",
      "Epoch 16/100, Loss: 36.1712027490139\n",
      "Epoch 17/100, Loss: 36.12485006451607\n",
      "Epoch 18/100, Loss: 36.4436274766922\n",
      "Epoch 19/100, Loss: 36.22374951839447\n",
      "Epoch 20/100, Loss: 36.2115415930748\n",
      "Epoch 21/100, Loss: 36.16678577661514\n",
      "Epoch 22/100, Loss: 36.20809951424599\n",
      "Epoch 23/100, Loss: 36.228652626276016\n",
      "Epoch 24/100, Loss: 36.154085248708725\n",
      "Epoch 25/100, Loss: 36.138443648815155\n",
      "Epoch 26/100, Loss: 36.5161928832531\n",
      "Epoch 27/100, Loss: 36.136161506175995\n",
      "Epoch 28/100, Loss: 36.181707948446274\n",
      "Epoch 29/100, Loss: 36.141745775938034\n",
      "Epoch 30/100, Loss: 36.1334473490715\n",
      "Epoch 31/100, Loss: 36.193426355719566\n",
      "Epoch 32/100, Loss: 36.15622678399086\n",
      "Epoch 33/100, Loss: 36.199489802122116\n",
      "Epoch 34/100, Loss: 36.11734637618065\n",
      "Epoch 35/100, Loss: 36.160643100738525\n",
      "Epoch 36/100, Loss: 36.1936252117157\n",
      "Epoch 37/100, Loss: 36.16784855723381\n",
      "Epoch 38/100, Loss: 36.19024500250816\n",
      "Epoch 39/100, Loss: 36.2072534263134\n",
      "Epoch 40/100, Loss: 36.19248494505882\n",
      "Epoch 41/100, Loss: 36.18511536717415\n",
      "Epoch 42/100, Loss: 36.156825214624405\n",
      "Epoch 43/100, Loss: 36.18466040492058\n",
      "Epoch 44/100, Loss: 36.14989456534386\n",
      "Epoch 45/100, Loss: 36.18341547250748\n",
      "Epoch 46/100, Loss: 36.13255634903908\n",
      "Epoch 47/100, Loss: 36.44247457385063\n",
      "Epoch 48/100, Loss: 36.20795226097107\n",
      "Epoch 49/100, Loss: 36.16933789849281\n",
      "Epoch 50/100, Loss: 36.155869632959366\n",
      "Epoch 51/100, Loss: 36.17461675405502\n",
      "Epoch 52/100, Loss: 36.14994007349014\n",
      "Epoch 53/100, Loss: 36.176823407411575\n",
      "Epoch 54/100, Loss: 36.16330271959305\n",
      "Epoch 55/100, Loss: 36.18516033887863\n",
      "Epoch 56/100, Loss: 36.17514684796333\n",
      "Epoch 57/100, Loss: 36.11868315935135\n",
      "Epoch 58/100, Loss: 36.16933134198189\n",
      "Epoch 59/100, Loss: 36.193585991859436\n",
      "Epoch 60/100, Loss: 36.30585631728172\n",
      "Epoch 61/100, Loss: 36.124624133110046\n",
      "Epoch 62/100, Loss: 36.41590037941933\n",
      "Epoch 63/100, Loss: 36.16250681877136\n",
      "Epoch 64/100, Loss: 36.13125276565552\n",
      "Epoch 65/100, Loss: 36.290554732084274\n",
      "Epoch 66/100, Loss: 36.19485479593277\n",
      "Epoch 67/100, Loss: 36.192596822977066\n",
      "Epoch 68/100, Loss: 36.19311338663101\n",
      "Epoch 69/100, Loss: 36.15330824255943\n",
      "Epoch 70/100, Loss: 36.15977245569229\n",
      "Epoch 71/100, Loss: 36.17040690779686\n",
      "Epoch 72/100, Loss: 36.20549160242081\n",
      "Epoch 73/100, Loss: 36.14463156461716\n",
      "Epoch 74/100, Loss: 36.23132652044296\n",
      "Epoch 75/100, Loss: 36.14879962801933\n",
      "Epoch 76/100, Loss: 36.246677339076996\n",
      "Epoch 77/100, Loss: 36.14988797903061\n",
      "Epoch 78/100, Loss: 36.13583964109421\n",
      "Epoch 79/100, Loss: 36.220797061920166\n",
      "Epoch 80/100, Loss: 36.12322652339935\n",
      "Epoch 81/100, Loss: 36.13682180643082\n",
      "Epoch 82/100, Loss: 36.136348247528076\n",
      "Epoch 83/100, Loss: 36.21580085158348\n",
      "Epoch 84/100, Loss: 36.154904037714005\n",
      "Epoch 85/100, Loss: 36.17132344841957\n",
      "Epoch 86/100, Loss: 36.27107375860214\n",
      "Epoch 87/100, Loss: 36.149519234895706\n",
      "Epoch 88/100, Loss: 36.13255423307419\n",
      "Epoch 89/100, Loss: 36.17941099405289\n",
      "Epoch 90/100, Loss: 36.24904045462608\n",
      "Epoch 91/100, Loss: 36.19086942076683\n",
      "Epoch 92/100, Loss: 36.16237214207649\n",
      "Epoch 93/100, Loss: 36.12625986337662\n",
      "Epoch 94/100, Loss: 36.16925394535065\n",
      "Epoch 95/100, Loss: 36.25732374191284\n",
      "Epoch 96/100, Loss: 36.15719136595726\n",
      "Epoch 97/100, Loss: 36.21809810400009\n",
      "Epoch 98/100, Loss: 36.19173404574394\n",
      "Epoch 99/100, Loss: 36.41532385349274\n",
      "Epoch 100/100, Loss: 36.2065212726593\n",
      "DeepSVDD(batch_size=32, c=0.0, contamination=0.1, dropout_rate=0.2,\n",
      "     epochs=100, hidden_activation='relu', hidden_neurons=[64, 32],\n",
      "     l2_regularizer=0.1, n_features=18, optimizer='adam',\n",
      "     output_activation='sigmoid', preprocessing=True, random_state=None,\n",
      "     use_ae=False, validation_size=0.1, verbose=1) \n",
      " {'Accuracy': 0.76, 'Precision': 0.394, 'Recall': 0.23, 'F1': 0.291, 'MCC': 0.166, 'AUC_PR': 0.333, 'AUC_ROC': 0.598, 'PREC_N_SCORES': 0.319}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "# Determina il numero di feature\n",
    "n_features = X_train2.shape[1]\n",
    "\n",
    "model = DeepSVDD(n_features=n_features)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48687c",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(contamination=0.1, copy=True, iterated_power='auto', n_components=None,\n",
      "  n_selected_components=None, random_state=None, standardization=True,\n",
      "  svd_solver='auto', tol=0.0, weighted=True, whiten=False) \n",
      " {'Accuracy': 0.779, 'Precision': 0.464, 'Recall': 0.23, 'F1': 0.308, 'MCC': 0.21, 'AUC_PR': 0.373, 'AUC_ROC': 0.612, 'PREC_N_SCORES': 0.363}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.pca import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f4ce",
   "metadata": {},
   "source": [
    "COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.4, 'Recall': 0.177, 'F1': 0.245, 'MCC': 0.147, 'AUC_PR': 0.328, 'AUC_ROC': 0.627, 'PREC_N_SCORES': 0.257}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78adc",
   "metadata": {},
   "source": [
    "SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5) \n",
      " {'Accuracy': 0.758, 'Precision': 0.364, 'Recall': 0.177, 'F1': 0.238, 'MCC': 0.125, 'AUC_PR': 0.308, 'AUC_ROC': 0.524, 'PREC_N_SCORES': 0.274}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sos import SOS\n",
    "\n",
    "model = SOS()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203a345",
   "metadata": {},
   "source": [
    "ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "model = ECOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0d61",
   "metadata": {},
   "source": [
    "# XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:32:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.968, 'Precision': 0.953, 'Recall': 0.894, 'F1': 0.922, 'MCC': 0.903, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n",
    "\n",
    "#n_estimators=50,\n",
    "#max_depth=3,\n",
    "#learning_rate=0.1,\n",
    "#random_state=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d61e",
   "metadata": {},
   "source": [
    "#### Con metiche di Memoria e Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595e73b",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d685",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:13:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3419463634490967 secondi\n",
      "Uso della memoria durante l'addestramento: 815.8125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.605494499206543 secondi\n",
      "Uso della memoria durante l'inferenza: 815.79296875 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a13a44",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised e Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models,\n",
    "              n_estimators=100,\n",
    "              max_depth=3,\n",
    "              learning_rate=0.2,\n",
    "              n_jobs=-1,\n",
    "              random_state=SEED\n",
    "            )\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796179ff",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:14:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.611022472381592 secondi\n",
      "Uso della memoria durante l'addestramento: 816.11328125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.9620587825775146 secondi\n",
      "Uso della memoria durante l'inferenza: 816.078125 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=2137, reg_alpha=0,\n",
      "   reg_lambda=1, scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=100, max_depth=3, learning_rate=0.2, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39459cc5",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Termina l'esecuzione anticipatamente se per un numero prestabilito di round non migliorano più i parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0157b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 12\n",
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100      # Numero massimo di cicli del'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcc0b0",
   "metadata": {},
   "source": [
    "### XGBOD con ricerca iperparametri con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:16:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:17:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.947, 'Precision': 0.989, 'Recall': 0.761, 'F1': 0.86, 'MCC': 0.839, 'AUC_PR': 0.898, 'AUC_ROC': 0.945, 'PREC_N_SCORES': 0.967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia di parametri\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Inizializza il modello\n",
    "model = XGBOD()\n",
    "\n",
    "# Randomized search con meno iterazioni e parallelizzazione\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Riaddestramento del modello con i migliori parametri\n",
    "model = XGBOD(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35867",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8006 - loss: 0.4877 - val_accuracy: 0.8885 - val_loss: 0.2546\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2390 - val_accuracy: 0.9244 - val_loss: 0.1969\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9334 - loss: 0.1862 - val_accuracy: 0.9168 - val_loss: 0.1949\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.1831 - val_accuracy: 0.9452 - val_loss: 0.1793\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.1629 - val_accuracy: 0.9471 - val_loss: 0.1570\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9424 - loss: 0.1595 - val_accuracy: 0.9546 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9514 - loss: 0.1251 - val_accuracy: 0.9509 - val_loss: 0.1471\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.1225 - val_accuracy: 0.9546 - val_loss: 0.1322\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9428 - loss: 0.1436 - val_accuracy: 0.9565 - val_loss: 0.1249\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9612 - loss: 0.1117 - val_accuracy: 0.9622 - val_loss: 0.1135\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "<Sequential name=sequential, built=True> {'Accuracy': 0.962, 'Precision': 0.927, 'Recall': 0.894, 'F1': 0.91, 'MCC': 0.886, 'AUC_PR': 0.966, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.903}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci il modello FCNN\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Poiché si tratta di una classificazione binaria\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestra il modello\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "y_predicted_score = model.predict(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed2dc",
   "metadata": {},
   "source": [
    "# Rockad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7c494",
   "metadata": {},
   "source": [
    "### 2° Prova un canale -> miglioramento predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8a5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_9824\\51312176.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "End Train\n",
      "score_test:  (15,)\n",
      "[ 59.52569593  97.91287411 104.37438939 306.12021652 117.83811987\n",
      " 206.00498512  96.27331365 112.26971517  85.6070828  154.73770951\n",
      "  68.09927101  68.40500052  69.69060948  70.24381485  62.94952254]\n",
      "RISULTATI:  [0 0 1 1 1 0 0 1 1 1 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.733, 'Precision': 0.764, 'Recall': 0.733, 'F1': 0.736, 'MCC': 0.491, 'AUC_PR': 0.911, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.778}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, average_precision_score\n",
    "\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"F1\": f1_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba, average='weighted').round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_neighbors=5 , n_jobs=-1, n_estimators=10, n_kernels=10000, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "\n",
    "# print(\"mean_train\", mean_train)\n",
    "# print(\"std_train\", std_train)\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55225ada",
   "metadata": {},
   "source": [
    "#### NORMALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddb2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (53, 250, 1)\n",
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_6284\\2734640787.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "score_test:  (15,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.6, 'Recall': 1.0, 'F1': 0.75, 'MCC': 0.0, 'AUC_PR': 0.6, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "\n",
    "# y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# # Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "# X_train_filtered, y_train_filtered = zip(*[\n",
    "#     (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "# ])\n",
    "# X_train_filtered = np.array(X_train_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=100, n_kernels=100, random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f115",
   "metadata": {},
   "source": [
    "### Più Canali e Miglioramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d81df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 1, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_3636\\290305579.py:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  (130,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test: \u001b[39m\u001b[38;5;124m\"\u001b[39m,y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ======================= PRE-PROCESSING =============================\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[0;32m     78\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "print(X_train.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "# print(\"score_test: \", score_test.shape)\n",
    "# print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# Senza parametri con standard scalar: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.407, 'AUC_ROC': 0.297, 'PREC_N_SCORES': 0.29}\n",
    "# Senza standard scalar e parametri: {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.758, 'F1': 0.603, 'MCC': 0.075, 'AUC_PR': 0.753, 'AUC_ROC': 0.707, 'PREC_N_SCORES': 0.677}\n",
    "# con parametri: {'Accuracy': 0.546, 'Precision': 0.515, 'Recall': 0.823, 'F1': 0.634, 'MCC': 0.137, 'AUC_PR': 0.757, 'AUC_ROC': 0.704, 'PREC_N_SCORES': 0.677}\n",
    "# con standard scaler e parametri: {'Accuracy': 0.492, 'Precision': 0.476, 'Recall': 0.645, 'F1': 0.548, 'MCC': -0.002, 'AUC_PR': 0.407, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29} (10,10000)\n",
    "\n",
    "# scaler (20, 10000): {'Accuracy': 0.531, 'Precision': 0.506, 'Recall': 0.694, 'F1': 0.585, 'MCC': 0.08, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "# solo param: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.758, 'AUC_ROC': 0.705, 'PREC_N_SCORES': 0.677}\n",
    "# (30, 10000): {'Accuracy': 0.5, 'Precision': 0.481, 'Recall': 0.613, 'F1': 0.539, 'MCC': 0.01, 'AUC_PR': 0.406, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (35, 10000): {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.597, 'F1': 0.544, 'MCC': 0.053, 'AUC_PR': 0.407, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (40, 10000): {'Accuracy': 0.454, 'Precision': 0.443, 'Recall': 0.565, 'F1': 0.496, 'MCC': -0.084, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "\n",
    "# (10, 20000): \n",
    "# (10, 20000) + no StandardScalar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221baa2",
   "metadata": {},
   "source": [
    "## ROCKAD su NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33b127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 250, 25)\n",
      "====================================================== End Train\n",
      "TEST:  (31, 250, 25)\n",
      "[188.33394021 184.22106524 180.06022805 189.28653757 187.29862371\n",
      " 175.5046242  178.16808366 192.54167875 190.22890328 185.37333139\n",
      " 170.06519542 180.8084682  194.7262789  190.33104492 194.01060727\n",
      " 171.50184742 182.280231   193.96076413 194.62371264 180.41964899\n",
      " 185.93476758 193.34802666 187.11112602 184.39235773 179.15894819\n",
      " 185.45303522 183.63233041 182.32004255 176.03040784 185.05605738\n",
      " 188.93759156]\n",
      "RISULTATI:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.948, 'PREC_N_SCORES': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OUTPUT_FILE = \"risultatiNASA.csv\"\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(train)\n",
    "print(\"====================================================== End Train\")\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "print(\"TEST: \", Test.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(Test)\n",
    "# print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "print(\"output: \", output)\n",
    "metrics = evaluate_metrics(output, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11911284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: A-1\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 60. 24.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.236, 'AUC_PR': 0.75, 'AUC_ROC': 0.969, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-2\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_8292\\2517294170.py:113: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.\n",
      " 60.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.806, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.101, 'AUC_PR': 0.625, 'AUC_ROC': 0.897, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-3\n",
      "==== End Train ====\n",
      "Train:  (50, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 175.  10.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.25, 'Precision': 0.042, 'Recall': 0.5, 'F1': 0.077, 'MCC': -0.149, 'AUC_PR': 0.533, 'AUC_ROC': 0.533, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-4\n",
      "==== End Train ====\n",
      "Train:  (49, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 110.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.969, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.059, 'AUC_ROC': 0.484, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (10, 250, 25)\n",
      "Test:  (18, 250, 25)\n",
      "output:  (18, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.944, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (9, 250, 25)\n",
      "Test:  (17, 250, 25)\n",
      "output:  (17, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-7\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  50. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.9, 'F1': 0.947, 'MCC': 0.93, 'AUC_PR': 0.187, 'AUC_ROC': 0.071, 'PREC_N_SCORES': 0.1}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-8\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.576, 'Precision': 0.6, 'Recall': 0.2, 'F1': 0.3, 'MCC': 0.123, 'AUC_PR': 0.47, 'AUC_ROC': 0.519, 'PREC_N_SCORES': 0.6}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-9\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.455, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.289, 'AUC_PR': 0.684, 'AUC_ROC': 0.822, 'PREC_N_SCORES': 0.733}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: B-1\n",
      "==== End Train ====\n",
      "Train:  (44, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. 70.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.438, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.191, 'AUC_PR': 0.083, 'AUC_ROC': 0.645, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (39, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0. 200.   0.   0.   0.   0.   0. 110.]\n",
      "output:  [0 0 1 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.5, 'Recall': 0.5, 'F1': 0.5, 'MCC': 0.357, 'AUC_PR': 0.361, 'AUC_ROC': 0.429, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-2\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0. 100.   0.   0.   0.   0.  35.   0.]\n",
      "output:  [0 1 0 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.643, 'AUC_ROC': 0.583, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (52, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.559, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.197, 'AUC_PR': 0.956, 'AUC_ROC': 0.938, 'PREC_N_SCORES': 0.923}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-11\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 60.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.448, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.196, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-12\n",
      "==== End Train ====\n",
      "Train:  (2, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  72. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.129, 'Precision': 0.1, 'Recall': 0.182, 'F1': 0.129, 'MCC': -0.718, 'AUC_PR': 0.828, 'AUC_ROC': 0.873, 'PREC_N_SCORES': 0.818}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-13\n",
      "==== End Train ====\n",
      "Train:  (25, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 160.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.467, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.186, 'AUC_PR': 0.1, 'AUC_ROC': 0.69, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (69, 250, 55)\n",
      "Test:  (10, 250, 55)\n",
      "output:  (10, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.  20. 200.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 1 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.8, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (37, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.583, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-16\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (25, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0. 150. 250. 250.   0.   0.   0.]\n",
      "output:  [0 0 1 1 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.625, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.383, 'AUC_ROC': 0.4, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.147, 'Precision': 0.071, 'Recall': 0.059, 'F1': 0.065, 'MCC': -0.717, 'AUC_PR': 0.983, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.941}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  25. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.167, 'Recall': 0.071, 'F1': 0.1, 'MCC': -0.231, 'AUC_PR': 0.849, 'AUC_ROC': 0.825, 'PREC_N_SCORES': 0.714}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-4\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  25. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.299, 'AUC_PR': 0.891, 'AUC_ROC': 0.912, 'PREC_N_SCORES': 0.769}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-5\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0. 50.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.967, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.045, 'AUC_ROC': 0.276, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (47, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0. 80.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.419, 'Precision': 0.053, 'Recall': 1.0, 'F1': 0.1, 'MCC': 0.145, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-7\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  60. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.167, 'Precision': 0.208, 'Recall': 0.455, 'F1': 0.286, 'MCC': -0.657, 'AUC_PR': 0.853, 'AUC_ROC': 0.861, 'PREC_N_SCORES': 0.727}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-8\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.065, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.558, 'AUC_PR': 0.333, 'AUC_ROC': 0.933, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-9\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 250. 250. 250.\n",
      " 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.069, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.783, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-1\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  30.   0. 140. 250.  86.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.091, 'AUC_PR': 0.643, 'AUC_ROC': 0.767, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-10\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 149. 121.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.078, 'AUC_PR': 0.705, 'AUC_ROC': 0.753, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-11\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 136. 107.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.078, 'AUC_PR': 0.701, 'AUC_ROC': 0.72, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-12\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 140. 250. 141.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.765, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.133, 'AUC_PR': 0.229, 'AUC_ROC': 0.517, 'PREC_N_SCORES': 0.25}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-13\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 101.  40.   0.   0.  51.  69.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.559, 'Precision': 0.077, 'Recall': 0.25, 'F1': 0.118, 'MCC': -0.099, 'AUC_PR': 0.177, 'AUC_ROC': 0.542, 'PREC_N_SCORES': 0.25}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 152. 250. 250. 250. 250. 245.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.595, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 156. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.3, 'Recall': 0.231, 'F1': 0.261, 'MCC': -0.127, 'AUC_PR': 0.766, 'AUC_ROC': 0.742, 'PREC_N_SCORES': 0.615}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-4\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  50. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.636, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.277, 'AUC_ROC': 0.274, 'PREC_N_SCORES': 0.083}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 150. 170.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.879, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.065, 'AUC_PR': 0.167, 'AUC_ROC': 0.726, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-6\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0. 65.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.031, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-7\n",
      "==== End Train ====\n",
      "Train:  (51, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 106. 174.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 100. 250. 250.  22.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.091, 'AUC_PR': 0.589, 'AUC_ROC': 0.725, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-9\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 200. 150.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.361, 'AUC_ROC': 0.871, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.03, 'AUC_PR': 0.111, 'AUC_ROC': 0.758, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  81. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.35, 'Recall': 0.583, 'F1': 0.438, 'MCC': -0.007, 'AUC_PR': 0.388, 'AUC_ROC': 0.439, 'PREC_N_SCORES': 0.417}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.212, 'Precision': 0.037, 'Recall': 1.0, 'F1': 0.071, 'MCC': 0.083, 'AUC_PR': 0.125, 'AUC_ROC': 0.781, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-4\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (13, 250, 55)\n",
      "output:  (13, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50. 20.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.462, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.337, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-5\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 55)\n",
      "Test:  (15, 250, 55)\n",
      "output:  (15, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 150.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.189, 'AUC_PR': 0.333, 'AUC_ROC': 0.857, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-7\n",
      "==== End Train ====\n",
      "Train:  (46, 250, 55)\n",
      "Test:  (20, 250, 55)\n",
      "output:  (20, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 200.   0.   0.   0.   0.  80.  40.   0. 100.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.327, 'AUC_PR': 0.888, 'AUC_ROC': 0.969, 'PREC_N_SCORES': 0.75}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-8\n",
      "==== End Train ====\n",
      "Train:  (62, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.  50. 250.]\n",
      "output:  [0 0 0 0 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.333, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.478, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-1\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0. 120.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.061, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.559, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-2\n",
      "==== End Train ====\n",
      "Train:  (45, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 40.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.034, 'Precision': 0.034, 'Recall': 1.0, 'F1': 0.067, 'MCC': 0.0, 'AUC_PR': 0.125, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-3\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.645, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.126, 'AUC_PR': 0.059, 'AUC_ROC': 0.467, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-4\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 30.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-6\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.206, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.314, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-7\n",
      "==== End Train ====\n",
      "Train:  (44, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 100.   0.   0.   0.   0.   0.  50.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. 115.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.594, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.217, 'AUC_PR': 0.867, 'AUC_ROC': 0.977, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-1\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0. 140. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.75, 'Recall': 0.6, 'F1': 0.667, 'MCC': 0.35, 'AUC_PR': 0.619, 'AUC_ROC': 0.4, 'PREC_N_SCORES': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-2\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0. 140. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.967, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.8}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-3\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (36, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.143, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-4\n",
      "==== End Train ====\n",
      "Train:  (37, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (36, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.  50.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.375, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (27, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0. 150.]\n",
      "output:  [0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (27, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0. 60. 40.  0.  0.  0.]\n",
      "output:  [0 0 0 1 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.833, 'AUC_ROC': 0.917, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0. 101.  99.   0.   0.   0.   0.\n",
      " 211.  29.   0.   0. 214.  94.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.123, 'AUC_ROC': 0.173, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (82, 250, 55)\n",
      "Test:  (24, 250, 55)\n",
      "output:  (24, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 130.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.958, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.167, 'AUC_ROC': 0.783, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (75, 250, 55)\n",
      "Test:  (14, 250, 55)\n",
      "output:  (14, 250)\n",
      "counts:  [  0.   0.   0.   0.  12.  94.   0. 120.   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.643, 'Precision': 0.25, 'Recall': 0.333, 'F1': 0.286, 'MCC': 0.055, 'AUC_PR': 0.369, 'AUC_ROC': 0.606, 'PREC_N_SCORES': 0.333}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-14\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 55)\n",
      "Test:  (24, 250, 55)\n",
      "output:  (24, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 175.   5.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.792, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.114, 'AUC_PR': 0.267, 'AUC_ROC': 0.818, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-15\n",
      "==== End Train ====\n",
      "Train:  (69, 250, 55)\n",
      "Test:  (11, 250, 55)\n",
      "output:  (11, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0. 20.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.727, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.149, 'AUC_PR': 0.167, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-2\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 150. 250. 250. 250. 250.  75.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.812, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.868, 'AUC_ROC': 0.853, 'PREC_N_SCORES': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  99. 250. 250. 250. 250. 236.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.818, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.931, 'AUC_ROC': 0.981, 'PREC_N_SCORES': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-4\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [  0.   0.   0.  50.  80.   0.   0.   0. 100. 100.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0. 110.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n",
      "output:  [0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 1.0, 'Recall': 0.2, 'F1': 0.333, 'MCC': 0.416, 'AUC_PR': 0.647, 'AUC_ROC': 0.754, 'PREC_N_SCORES': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-7\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  50. 250. 250. 250. 250. 250. 250. 100.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.646, 'AUC_ROC': 0.714, 'PREC_N_SCORES': 0.625}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: R-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (28, 250, 25)\n",
      "output:  (28, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 80.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.964, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.926, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 200. 247.   0.   0.   0.   0.   0.\n",
      "   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.276, 'Precision': 0.087, 'Recall': 1.0, 'F1': 0.16, 'MCC': 0.139, 'AUC_PR': 0.067, 'AUC_ROC': 0.241, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-2\n",
      "==== End Train ====\n",
      "Train:  (14, 250, 55)\n",
      "Test:  (7, 250, 55)\n",
      "output:  (7, 250)\n",
      "counts:  [ 0.  0.  0. 10.  0.  0.  0.]\n",
      "output:  [0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.857, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0. 101. 250. 250. 250. 250.\n",
      " 250. 148.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.647, 'Precision': 0.375, 'Recall': 0.75, 'F1': 0.5, 'MCC': 0.311, 'AUC_PR': 0.292, 'AUC_ROC': 0.322, 'PREC_N_SCORES': 0.125}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-12\n",
      "==== End Train ====\n",
      "Train:  (18, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0. 120.   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.316, 'AUC_PR': 0.25, 'AUC_ROC': 0.625, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-13\n",
      "==== End Train ====\n",
      "Train:  (18, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.  60.  40.   0.   0.   0. 100.  50.]\n",
      "output:  [0 0 1 1 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.444, 'Recall': 1.0, 'F1': 0.615, 'MCC': 0.0, 'AUC_PR': 0.635, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 160.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.143, 'Recall': 0.286, 'F1': 0.19, 'MCC': -0.13, 'AUC_PR': 0.383, 'AUC_ROC': 0.608, 'PREC_N_SCORES': 0.286}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0. 82.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. 50. 50.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.667, 'F1': 0.8, 'MCC': 0.804, 'AUC_PR': 0.31, 'AUC_ROC': 0.656, 'PREC_N_SCORES': 0.333}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-4\n",
      "==== End Train ====\n",
      "Train:  (41, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0.  0. 68.  0.  0.  0.]\n",
      "output:  [0 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.143, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (41, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0.  0. 25.  0.  0.  0.]\n",
      "output:  [0 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-8\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (10, 250, 55)\n",
      "Test:  (6, 250, 55)\n",
      "output:  (6, 250)\n",
      "counts:  [ 0.  0.  0. 60.  0. 40.]\n",
      "output:  [0 0 0 1 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.583, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-9\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (4, 250, 55)\n",
      "Test:  (4, 250, 55)\n",
      "output:  (4, 250)\n",
      "counts:  [  0.   0.   0. 110.]\n",
      "output:  [0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Risultati salvati in risultatiNASA_ROCKAD.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OFFSET = 50\n",
    "OUTPUT_FILE = \"risultatiNASA_ROCKAD.csv\"\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "# Itera su tutti i canali del dataset\n",
    "for channel_id in NASA.channel_ids:\n",
    "    # if channel_id == \"D-12\" or channel_id == \"T-10\" or channel_id == \"T-9\":   # Non uso questi perchè NeirestNeigthbor dato che necessita di avere più \n",
    "    #     continue\n",
    "    if channel_id == \"T-10\":\n",
    "        continue\n",
    "    print(f\"Processing channel: {channel_id}\")\n",
    "\n",
    "    # Lista per memorizzare i segmenti di training\n",
    "    X_train_final = []\n",
    "\n",
    "    # Uso del dataset NASA per tutti i canali\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\")\n",
    "    # print(dataset.data.shape)\n",
    "    data = dataset.data\n",
    "    train = []\n",
    "    for i in range(0, data.shape[0] - STEP +1, OFFSET): \n",
    "        train.append(data[i:i+STEP])\n",
    "\n",
    "    train = np.stack(train)\n",
    "    # print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "    # Inizializza e addestra il modello ROCKAD\n",
    "    rockad = ROCKAD(n_neighbors=1, n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "    rockad.fit(train)\n",
    "    print(\"==== End Train ====\")\n",
    "\n",
    "    # Predict anomaly scores\n",
    "    score_train = rockad.predict_proba(train)\n",
    "    # print(\"Score:\", scores)\n",
    "\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\", train=False)\n",
    "    data = dataset.data\n",
    "    Test = []\n",
    "    output = []\n",
    "    o = np.zeros(data.shape[0])\n",
    "    for start,end in dataset.anomalies:\n",
    "        o[start:end] = 1\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        Test.append(data[i:i+STEP])\n",
    "        output.append(o[i:i+STEP])\n",
    "\n",
    "    output = np.stack(output)\n",
    "    Test = np.stack(Test)\n",
    "    # print(\"TEST: \", Test.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "    decision_func = NearestNeighborOCC().fit(score_train)\n",
    "    \n",
    "    print(\"Train: \", train.shape)\n",
    "    print(\"Test: \", Test.shape)\n",
    "    print(\"output: \", output.shape)\n",
    "    score_test = rockad.predict_proba(Test)\n",
    "    # print(\"score_test: \", score_test.shape)\n",
    "    # print(score_test)\n",
    "\n",
    "    result = decision_func.predict(score_test)\n",
    "    result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "\n",
    "    # Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "    threshold = 1 # -> 10%\n",
    "    # Conta il numero di 1 in ogni lista\n",
    "    counts = np.sum(output, axis=1)\n",
    "    output = np.where(counts >= threshold, 1, 0)\n",
    "    print(\"counts: \", counts)\n",
    "    print(\"output: \", output)\n",
    "    metrics = evaluate_metrics(output, result_binary, score_test)\n",
    "    print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        \"Channel\": channel_id,\n",
    "        \"Accuracy\": metrics.get(\"Accuracy\", 0),\n",
    "        \"Precision\": metrics.get(\"Precision\", 0),\n",
    "        \"Recall\": metrics.get(\"Recall\", 0),\n",
    "        \"MCC\": metrics.get(\"MCC\", 0),\n",
    "        \"AUC_PR\": metrics.get(\"AUC_PR\", 0),\n",
    "        \"AUC_ROC\": metrics.get(\"AUC_ROC\", 0),\n",
    "        \"F1\": metrics.get(\"F1\", 0),\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    print(\"=========================FINE CHANNEL=============================\")\n",
    "# ======================= SALVATAGGIO RISULTATI =============================\n",
    "results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Risultati salvati in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77704e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie delle colonne numeriche:\n",
      "Accuracy     0.616765\n",
      "Precision    0.093679\n",
      "Recall       0.145272\n",
      "F1           0.086543\n",
      "MCC         -0.099062\n",
      "AUC_ROC      0.745765\n",
      "AUC_PR       0.589877\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "df = pd.read_csv(\"risultatiNASA_ROCKAD.csv\")\n",
    "\n",
    "# Calcola la media delle colonne numeriche\n",
    "column_means = df.mean(numeric_only=True)\n",
    "\n",
    "# Stampa le medie\n",
    "print(\"Medie delle colonne numeriche:\")\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a14d98",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3a9d2",
   "metadata": {},
   "source": [
    "# ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cafd0adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_8292\\3020148067.py:54: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "features_train:  (53, 20000)\n",
      "features_test:  (15, 20000)\n",
      "(array([0, 1]), array([14,  1], dtype=int64))\n",
      "Anomalie rilevate nel training set: 0     1\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    1\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "30    0\n",
      "31    0\n",
      "32    0\n",
      "33    0\n",
      "34    0\n",
      "35    0\n",
      "36    0\n",
      "37    0\n",
      "38    0\n",
      "39    0\n",
      "40    0\n",
      "41    0\n",
      "42    0\n",
      "43    0\n",
      "44    0\n",
      "45    0\n",
      "46    0\n",
      "47    0\n",
      "48    1\n",
      "49    0\n",
      "50    0\n",
      "51    0\n",
      "52    0\n",
      "dtype: int32\n",
      "Anomalie rilevate nel test set: 0     0\n",
      "1     0\n",
      "2     0\n",
      "3     1\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "dtype: int32\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.467, 'Precision': 1.0, 'Recall': 0.111, 'F1': 0.2, 'MCC': 0.218}\n"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "STEP = 250\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "print(\"features_train: \", features_train.shape)\n",
    "print(\"features_test: \", features_test.shape)\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "print(np.unique(anomaly_labels_test, return_counts=True))\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409f2d8",
   "metadata": {},
   "source": [
    "## Unsupervised con Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ffb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1594, 18)\n",
      "(529, 18)\n",
      "Anomalie rilevate nel training set: (1594,)\n",
      "Anomalie rilevate nel test set: (529,)\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train.shape)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test.shape)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)\n",
    "# {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e056df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_23708\\2536707538.py:52: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "X_train shape: (53, 250)\n",
      "X_test shape: (15, 250)\n",
      "Anomalie rilevate nel training set: [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Anomalie rilevate nel test set: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.467, 'Precision': 1.0, 'Recall': 0.111, 'F1': 0.2, 'MCC': 0.218}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "STEP = 250\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # Appiattimento in 2D\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train, kernels)\n",
    "features_test = apply_kernels(X_test, kernels)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036570a",
   "metadata": {},
   "source": [
    "### KNN ( UNSUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4756a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: (529,)\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.845, 'Precision': 0.754, 'Recall': 0.407, 'F1': 0.529, 'MCC': 0.476, 'AUC_PR': 0.611, 'AUC_ROC': 0.804, 'PREC_N_SCORES': 0.531}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred.shape)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.845, 'Precision': 0.763, 'Recall': 0.398, 'F1': 0.523, 'MCC': 0.475, 'AUC_PR': 0.619, 'AUC_ROC': 0.811, 'PREC_N_SCORES': 0.54}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4c45a",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie ROCKET SUPERVISED\n",
    "Utilizzo di vari algoritmi unsupervised e non con kernel ROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eed09",
   "metadata": {},
   "source": [
    "### Regressione Logistica -> Classificatore lineare ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.972, 'Precision': 0.945, 'Recall': 0.92, 'F1': 0.933, 'MCC': 0.915, 'AUC_PR': 0.975, 'AUC_ROC': 0.993, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539711",
   "metadata": {},
   "source": [
    "### Prova con Dettagli dal GitHub del Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.879, 'Precision': 0.98, 'Recall': 0.442, 'F1': 0.61, 'MCC': 0.611, 'AUC_PR': 0.932, 'AUC_ROC': 0.965, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "anomaly_scores_test = model.predict(features_test)\n",
    "anomaly_scores_train = model.predict(features_train)\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "#  {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501c69b",
   "metadata": {},
   "source": [
    "## LogisticClassifierCV ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7b5c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_8292\\3872063633.py:27: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_final.append(y_trainS[i])\n",
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_8292\\3872063633.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "features_train:      0         1      2         3         4         5      6          7      \\\n",
      "0   0.225  4.124670  0.952  6.125443  0.383117  7.832538  0.940  13.035572   \n",
      "1   0.000 -2.807308  1.000  3.077709  0.811688  1.757264  0.984   2.071925   \n",
      "2   0.675  1.213135  0.956  4.720337  0.675325  3.792974  0.960   4.592155   \n",
      "3   0.975  2.809944  0.952  3.151166  0.383117  1.254861  0.960   3.584738   \n",
      "4   0.400  0.753760  0.860  4.895823  0.623377  3.664900  0.948   9.445580   \n",
      "5   0.100  0.217014  1.000  1.190639  0.435065  1.253239  0.996   3.147383   \n",
      "6   0.975  2.094759  0.924  3.811384  0.214286  6.318000  0.964  11.139218   \n",
      "7   0.275  0.742934  0.932  7.718774  0.402597  1.485129  0.952   4.986872   \n",
      "8   1.000  2.478657  0.960  1.363665  0.012987  0.036954  0.996   3.232711   \n",
      "9   0.000 -1.871216  1.000  2.061351  0.824675  1.793436  0.992   2.447036   \n",
      "10  0.100  0.216995  1.000  1.426345  0.454545  1.431891  0.996   2.823552   \n",
      "11  0.000 -1.613735  0.956  2.758942  0.584416  2.519339  0.984   3.365148   \n",
      "12  0.000 -1.029435  1.000  2.222569  0.551948  1.791034  0.992   2.829357   \n",
      "13  0.000 -1.473114  1.000  1.856364  0.577922  1.617442  0.980   4.218562   \n",
      "14  0.725  0.619157  0.948  1.718076  0.441558  1.330954  0.964   3.750869   \n",
      "15  1.000  1.848108  0.960  1.578557  0.045455  0.276849  0.984   2.510947   \n",
      "16  0.975  2.805194  0.952  4.539575  0.129870  3.995087  0.956   5.652407   \n",
      "17  1.000  1.883573  0.964  1.547120  0.110390  0.699986  0.988   2.162252   \n",
      "18  1.000  1.528240  0.960  1.357982  0.006494  0.007472  1.000   2.460377   \n",
      "19  0.000 -2.326959  1.000  1.879643  0.707792  1.573272  0.992   3.644958   \n",
      "20  0.075  0.578381  1.000  1.360224  0.487013  1.112437  0.996   2.619706   \n",
      "21  0.000 -1.763466  0.964  4.402963  0.668831  2.698991  0.956   9.866340   \n",
      "22  0.000 -2.795057  1.000  2.791509  0.779221  1.957355  0.992   2.094152   \n",
      "23  0.000 -0.150519  0.940  1.783718  0.402597  2.099232  0.964   2.650432   \n",
      "24  0.900  1.323898  0.968  1.997343  0.220779  1.583562  0.964   3.848594   \n",
      "25  0.000 -0.862098  0.932  3.917680  0.824675  2.181611  0.916   2.994995   \n",
      "26  0.000 -0.229775  0.976  2.013759  0.376623  1.830477  0.956   4.735912   \n",
      "27  0.000 -2.138891  1.000  3.204890  0.720779  2.091689  0.988   2.884460   \n",
      "28  0.000 -2.246773  1.000  2.988305  0.987013  1.424608  0.984   2.100666   \n",
      "29  1.000  2.215304  0.960  1.359454  0.000000 -0.119400  1.000   2.866679   \n",
      "30  0.475  1.111004  0.976  2.604441  0.551948  1.687110  0.972   3.040960   \n",
      "31  0.925  2.062675  0.944  4.390741  0.448052  4.748924  0.952   5.121444   \n",
      "32  0.850  3.245346  0.976  3.772193  0.441558  1.425873  0.976   4.018868   \n",
      "33  1.000  2.637459  0.940  2.004087  0.363636  5.219978  0.984   8.554475   \n",
      "34  0.000 -0.935431  1.000  1.522108  0.772727  1.428216  0.992   1.561358   \n",
      "35  0.000 -0.477437  0.964  2.734991  0.740260  1.807050  0.960   6.307903   \n",
      "36  0.125  0.172317  0.984  1.209611  0.662338  1.354594  0.988   2.617200   \n",
      "37  0.000 -0.113150  0.964  1.966536  0.668831  2.034217  0.936   2.311452   \n",
      "38  1.000  2.971076  0.956  1.349299  0.344156  1.409711  0.988   2.568198   \n",
      "39  0.100  0.297160  0.968  3.803837  0.629870  1.652305  0.956   3.937789   \n",
      "40  0.000 -0.983243  0.996  2.258124  0.850649  2.011701  0.988   1.816651   \n",
      "41  1.000  2.355904  0.948  2.270311  0.045455  0.792365  0.968   2.543284   \n",
      "42  0.450  1.079451  0.964  1.786240  0.532468  1.659881  0.980   3.156595   \n",
      "43  0.975  1.671760  0.984  1.239143  0.422078  1.852390  0.988   2.967397   \n",
      "44  0.000 -0.678026  1.000  1.992653  0.746753  1.268997  0.988   2.339604   \n",
      "45  1.000  2.619456  0.960  1.170314  0.045455  0.242765  0.996   2.898947   \n",
      "46  0.850  1.831509  0.928  6.190387  0.506494  2.621793  0.960   7.694448   \n",
      "47  1.000  2.354539  0.940  2.660652  0.428571  2.308298  0.948   3.130498   \n",
      "48  0.025  2.852356  0.956  5.167665  0.746753  6.427413  0.960  11.247437   \n",
      "49  0.675  1.156176  0.980  1.200921  0.558442  1.540353  0.984   3.093832   \n",
      "50  1.000  3.041852  0.956  2.282347  0.389610  1.652843  0.968   2.714972   \n",
      "51  1.000  3.057079  0.960  1.517504  0.175325  0.704654  0.996   3.104821   \n",
      "52  0.000 -0.829264  0.956  2.339997  0.500000  2.900047  0.996   2.143494   \n",
      "\n",
      "    8         9      ...     19990     19991     19992      19993     19994  \\\n",
      "0   0.412  1.949091  ...  0.011111  0.015461  0.061983  10.990389  0.142857   \n",
      "1   0.372  0.865929  ...  0.733333  1.684544  0.028926   1.157260  0.114286   \n",
      "2   0.156  0.580518  ...  0.522222  3.682142  0.045455   4.028012  0.228571   \n",
      "3   0.376  1.776026  ...  0.000000 -1.019188  0.103306   1.642297  0.442857   \n",
      "4   0.276  1.542378  ...  0.511111  3.632534  0.082645   6.294425  0.485714   \n",
      "5   0.444  1.716421  ...  0.000000 -0.703575  0.020661   0.095058  0.309524   \n",
      "6   0.400  1.380058  ...  0.311111  3.197519  0.037190   4.038456  0.352381   \n",
      "7   0.436  0.727490  ...  0.077778  0.519333  0.169421   4.602681  0.138095   \n",
      "8   0.516  1.543373  ...  0.000000 -1.374279  0.008264   0.102767  0.347619   \n",
      "9   0.412  1.061784  ...  0.655556  1.719117  0.070248   0.215501  0.238095   \n",
      "10  0.436  1.798501  ...  0.000000 -0.588852  0.037190   0.260103  0.319048   \n",
      "11  0.396  1.314380  ...  0.377778  1.173297  0.053719   2.628960  0.280952   \n",
      "12  0.408  1.545717  ...  0.122222  0.303492  0.024793   0.146955  0.352381   \n",
      "13  0.388  1.762786  ...  0.111111  0.599107  0.078512   0.431052  0.357143   \n",
      "14  0.072  0.172899  ...  0.600000  3.134154  0.219008   0.574390  0.052381   \n",
      "15  0.540  1.337687  ...  0.000000 -2.314435  0.037190   0.666185  0.223810   \n",
      "16  0.520  1.986638  ...  0.022222  1.287280  0.177686   3.434411  0.352381   \n",
      "17  0.520  1.507668  ...  0.000000 -2.247242  0.057851   0.549874  0.352381   \n",
      "18  0.520  1.384237  ...  0.000000 -2.053236  0.041322   0.617975  0.295238   \n",
      "19  0.388  1.779834  ...  0.222222  1.331221  0.037190   0.183502  0.323810   \n",
      "20  0.436  1.780780  ...  0.000000 -1.298393  0.024793   0.121830  0.285714   \n",
      "21  0.360  1.750450  ...  0.200000  4.530577  0.045455   5.402389  0.290476   \n",
      "22  0.428  1.001722  ...  0.755556  1.589979  0.041322   0.257073  0.300000   \n",
      "23  0.404  1.813280  ...  0.000000 -0.883216  0.144628   1.232174  0.352381   \n",
      "24  0.500  1.325904  ...  0.000000 -0.856427  0.074380   1.055896  0.290476   \n",
      "25  0.144  0.484338  ...  0.933333  3.118496  0.309917   0.797862  0.038095   \n",
      "26  0.440  1.706314  ...  0.011111  0.113808  0.148760   1.253587  0.342857   \n",
      "27  0.384  1.271775  ...  0.577778  1.912274  0.066116   0.394491  0.304762   \n",
      "28  0.384  0.721646  ...  0.800000  1.762866  0.049587   1.349107  0.085714   \n",
      "29  0.532  1.536361  ...  0.000000 -1.960049  0.016529   0.282730  0.290476   \n",
      "30  0.416  0.669844  ...  0.544444  1.979195  0.066116   2.052109  0.190476   \n",
      "31  0.396  1.133608  ...  0.500000  2.145836  0.144628   4.209440  0.300000   \n",
      "32  0.404  2.353533  ...  0.355556  1.996426  0.070248   0.512726  0.195238   \n",
      "33  0.400  1.188567  ...  0.277778  3.842279  0.053719   5.767396  0.319048   \n",
      "34  0.324  0.673632  ...  1.000000  1.592684  0.012397   0.039095  0.000000   \n",
      "35  0.292  0.559286  ...  0.977778  3.982593  0.061983   3.244541  0.033333   \n",
      "36  0.356  0.595336  ...  0.688889  2.383204  0.016529   0.247613  0.047619   \n",
      "37  0.340  0.731601  ...  0.977778  2.338696  0.231405   0.702413  0.023810   \n",
      "38  0.396  1.659144  ...  0.155556  0.922447  0.008264   0.276780  0.347619   \n",
      "39  0.400  0.844578  ...  0.833333  1.766818  0.074380   4.041708  0.104762   \n",
      "40  0.292  0.717837  ...  1.000000  2.108538  0.177686   0.467994  0.000000   \n",
      "41  0.524  1.437764  ...  0.000000 -1.340386  0.074380   1.493984  0.257143   \n",
      "42  0.392  0.642525  ...  0.477778  2.629006  0.066116   2.449135  0.190476   \n",
      "43  0.424  0.814220  ...  0.477778  2.104103  0.082645   0.165508  0.185714   \n",
      "44  0.292  0.673344  ...  1.000000  1.852970  0.016529   0.011105  0.000000   \n",
      "45  0.476  1.579639  ...  0.000000 -0.772697  0.008264   0.190207  0.352381   \n",
      "46  0.380  1.895006  ...  0.433333  2.318391  0.045455   4.629905  0.271429   \n",
      "47  0.408  1.319618  ...  0.177778  1.510805  0.173554   1.505843  0.261905   \n",
      "48  0.220  0.593116  ...  0.777778  6.092296  0.045455   3.752431  0.085714   \n",
      "49  0.396  0.589492  ...  0.500000  2.757448  0.037190   0.324833  0.142857   \n",
      "50  0.408  1.650196  ...  0.133333  1.133555  0.140496   0.640611  0.323810   \n",
      "51  0.424  1.685905  ...  0.011111  0.113895  0.045455   0.321805  0.319048   \n",
      "52  0.400  1.382332  ...  0.233333  0.765386  0.086777   0.442725  0.280952   \n",
      "\n",
      "       19995     19996     19997     19998     19999  \n",
      "0   5.040885  0.104545  8.178266  0.047414  7.344664  \n",
      "1   3.031009  0.054545  1.386093  0.025862  1.007854  \n",
      "2   4.543707  0.072727  2.808431  0.034483  1.317713  \n",
      "3   2.409122  0.122727  2.136744  0.068966  0.787853  \n",
      "4   5.525714  0.122727  2.332767  0.068966  3.192486  \n",
      "5   1.014457  0.004545  0.010266  0.000000 -0.255709  \n",
      "6   1.541043  0.077273  2.428391  0.021552  5.590744  \n",
      "7   2.857497  0.050000  2.722342  0.056034  4.544005  \n",
      "8   0.543174  0.004545  0.018021  0.000000 -0.177267  \n",
      "9   1.141718  0.050000  0.260841  0.004310  0.016308  \n",
      "10  1.216245  0.031818  0.193308  0.004310  0.006941  \n",
      "11  1.241210  0.040909  0.425082  0.012931  0.534787  \n",
      "12  1.048406  0.000000 -0.031069  0.000000 -0.216028  \n",
      "13  1.140503  0.072727  0.226591  0.004310  0.014204  \n",
      "14  0.712459  0.013636  0.385241  0.004310  0.000788  \n",
      "15  0.610671  0.031818  0.341395  0.000000 -0.023635  \n",
      "16  3.099758  0.100000  3.469029  0.051724  2.526781  \n",
      "17  0.709723  0.018182  0.320162  0.000000 -0.086549  \n",
      "18  0.576711  0.000000 -0.010213  0.000000 -0.146113  \n",
      "19  1.675428  0.100000  0.224157  0.004310  0.031276  \n",
      "20  1.324837  0.068182  0.241303  0.000000 -0.116591  \n",
      "21  4.998482  0.109091  2.932241  0.056034  4.982311  \n",
      "22  0.435136  0.000000 -0.069663  0.000000 -0.162612  \n",
      "23  2.102202  0.195455  0.605843  0.056034  0.736732  \n",
      "24  0.748878  0.031818  0.614914  0.004310  0.058828  \n",
      "25  0.376162  0.031818  0.626743  0.017241  0.249401  \n",
      "26  1.371874  0.177273  0.837626  0.025862  0.201091  \n",
      "27  0.829366  0.004545  0.131570  0.000000 -0.077485  \n",
      "28  2.776919  0.054545  1.387142  0.030172  1.019622  \n",
      "29  0.478172  0.009091  0.115273  0.000000 -0.087606  \n",
      "30  0.992230  0.063636  1.290613  0.025862  0.740651  \n",
      "31  4.448688  0.113636  2.009183  0.047414  1.671087  \n",
      "32  0.627664  0.031818  2.410528  0.017241  1.171834  \n",
      "33  0.808440  0.054545  2.081203  0.017241  2.681953  \n",
      "34 -0.103343  0.000000 -0.370984  0.000000 -0.292036  \n",
      "35  0.612525  0.050000  1.834891  0.021552  1.993870  \n",
      "36  0.216828  0.000000 -0.118598  0.000000 -0.055391  \n",
      "37  0.557385  0.045455  0.496132  0.025862  0.269751  \n",
      "38  1.060265  0.040909  0.128213  0.000000 -0.168725  \n",
      "39  5.031882  0.077273  3.666937  0.056034  1.992913  \n",
      "40 -0.303086  0.004545  0.008761  0.004310  0.000272  \n",
      "41  1.620480  0.086364  1.076924  0.030172  0.239245  \n",
      "42  1.459902  0.031818  1.598811  0.017241  0.399590  \n",
      "43  0.659693  0.013636  0.066675  0.000000 -0.201169  \n",
      "44 -0.404282  0.000000 -0.411029  0.000000 -0.431942  \n",
      "45  0.755379  0.004545  0.114676  0.000000 -0.097606  \n",
      "46  3.165677  0.090909  4.510868  0.043103  2.903255  \n",
      "47  2.313895  0.104545  1.499917  0.060345  0.479592  \n",
      "48  4.892790  0.063636  4.354063  0.034483  6.641572  \n",
      "49  0.534419  0.004545  0.038264  0.000000 -0.009191  \n",
      "50  1.563423  0.090909  0.853157  0.025862  0.300207  \n",
      "51  1.586329  0.077273  0.585116  0.000000 -0.066874  \n",
      "52  0.814088  0.000000 -0.027855  0.000000 -0.082708  \n",
      "\n",
      "[53 rows x 20000 columns]\n",
      "Predizioni nel test set: [0 0 0 1 1 1 0 1 0 1 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.833, 'Recall': 0.556, 'F1': 0.667, 'MCC': 0.389, 'AUC_PR': 0.865, 'AUC_ROC': 0.778, 'PREC_N_SCORES': 0.778}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "STEP = 250\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    y_trainS = dfSegment.loc[mask, \"anomaly\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        X_train_final.append(X_trainS[i:i + STEP])\n",
    "        y_train_final.append(y_trainS[i])\n",
    "        \n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "y_train = np.array(y_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "print(\"features_train: \", features_train)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = RidgeClassifierCV(alphas = np.logspace(-3, 3, 10))\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "# Per separare multiclasse o monoclasse\n",
    "if  len(np.unique(y_test)) > 2:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=1)\n",
    "else:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=0)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1688a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train:  (53, 20000)\n",
      "y_train:  (0,)\n"
     ]
    }
   ],
   "source": [
    "print(\"features_train: \", features_train.shape)\n",
    "print(\"y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522d8c",
   "metadata": {},
   "source": [
    "# Test Rocket su NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbedd15",
   "metadata": {},
   "source": [
    "### ROCKET con NASA -> Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a85f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n",
      "Percentuale di anomalie rilevate: 6.451612903225806\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [39.34577467 36.10637661 32.64133316 36.35686125 35.17089137 34.22729672\n",
      " 34.3235441  40.1292137  41.66561789 33.69131247 32.59504588 34.15536612\n",
      " 38.24332809 38.96896401 29.29555863 33.06146931 34.47508609 34.56070073\n",
      " 34.81417879 33.43194699 35.46488308 34.60237567 37.23227165 34.80146113\n",
      " 47.73908761 32.82958253 32.60580054 31.43605806 34.47250574 36.07161459\n",
      " 30.1217598 ]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.069}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OUTPUT_FILE = \"risultatiNASA_ROCKET.csv\"\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "# input_length = train.shape[0]\n",
    "num_kernels = 10000\n",
    "\n",
    "train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "Test = Test.reshape(Test.shape[0], -1)\n",
    "print(\"train: \", train.shape) \n",
    "print(\"TEST: \", Test.shape)\n",
    "\n",
    "kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "train = train.astype(np.float64)\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "Test = Test.astype(np.float64)\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "threshold = np.percentile(y_proba, 95)  # Soglia al 95° percentile\n",
    "predicted_anomalies = y_proba > threshold\n",
    "print(\"Percentuale di anomalie rilevate:\", predicted_anomalies.mean() * 100)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "print(\"Predizioni nel test set:\", y_proba)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "metrics = evaluate_metrics(output, predicted_anomalies)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3927b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: A-1\n",
      "(2880, 25)\n",
      "train:  (11, 6250)\n",
      "TEST:  (34, 6250)\n",
      "Percentuale di anomalie rilevate: 5.88235294117647\n",
      "Predizioni nel test set: [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "Predizioni nel test set: [18.33543992 20.90955435 19.7170929  13.67316349 18.13719694 25.04026811\n",
      " 20.08963078 17.04746814 16.69631453 14.23326539 16.49585339 25.02108608\n",
      " 16.0548561  17.29007103 16.7440533  14.81597321 22.7248683  23.52491098\n",
      " 22.18369744 18.75362211 17.98687596 18.99861365 22.21794342 18.2326525\n",
      " 18.85961504 12.4573738  23.32769471 19.64676119 20.45612912 19.81123338\n",
      " 30.4030754  19.74648274 16.15748844 21.77979617]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.912, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.044}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-2\n",
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_8292\\2033848077.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuale di anomalie rilevate: 6.451612903225806\n",
      "Predizioni nel test set: [0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0]\n",
      "Predizioni nel test set: [28.89564656 33.4807405  23.03301614 25.15847188 30.80584259 27.88451496\n",
      " 22.61984682 27.56667093 20.9344756  29.49424966 25.77373573 28.24021928\n",
      " 24.10449011 21.45720891 21.79096132 27.84987167 21.98467674 29.31744725\n",
      " 23.40769281 25.68252963 22.98914928 27.25459836 22.42709833 30.97936404\n",
      " 31.32231227 22.73843068 29.14226221 27.66289435 23.58447464 26.16345155\n",
      " 24.25772895]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.069}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-3\n",
      "(2736, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (32, 6250)\n",
      "Percentuale di anomalie rilevate: 6.25\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [32.03432089 26.23075979 22.46609553 29.43896964 20.7087671  25.69775622\n",
      " 22.98116891 20.7202819  27.30742807 25.86490586 27.03118017 23.80678882\n",
      " 26.48280412 21.01567127 18.89483488 23.4198753  25.24155687 26.3360467\n",
      " 21.98254593 28.22828583 24.66718322 31.7291844  20.8935674  22.76737039\n",
      " 26.24102422 26.36464328 28.98772188 26.61195428 26.07712036 25.54022379\n",
      " 27.0833263  28.99493896]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.906, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.046}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-4\n",
      "(2690, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (32, 6250)\n",
      "Percentuale di anomalie rilevate: 6.25\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [25.78910227 24.64196946 20.45997744 26.10609967 20.37777122 26.43911365\n",
      " 23.17048786 24.54428785 27.22643026 26.62306823 26.06559419 18.54212472\n",
      " 28.33945778 24.83819678 19.2978175  24.75844757 21.52589151 25.93365605\n",
      " 26.11434833 28.91097869 23.59490447 22.63996669 24.49424703 28.48121357\n",
      " 21.39802725 26.25061394 29.21258754 21.90716979 25.12744559 23.24042741\n",
      " 29.11887302 23.99489832]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.906, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.046}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-5\n",
      "(705, 25)\n",
      "train:  (2, 6250)\n",
      "TEST:  (18, 6250)\n",
      "Percentuale di anomalie rilevate: 5.555555555555555\n",
      "Predizioni nel test set: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predizioni nel test set: [144.88410258 144.88321838 144.88322055 144.88259762 144.88187186\n",
      " 144.88229999 144.88229295 144.88307881 144.88307746 144.88302265\n",
      " 144.88312771 118.49950539 216.63313256 216.63450613 216.70608537\n",
      " 216.70599971 216.70601649 216.7063005 ]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.889, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.059}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-6\n",
      "(682, 25)\n",
      "train:  (2, 6250)\n",
      "TEST:  (17, 6250)\n",
      "Percentuale di anomalie rilevate: 5.88235294117647\n",
      "Predizioni nel test set: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predizioni nel test set: [260.92197145 260.92118421 260.92047191 260.84859521 260.92063317\n",
      " 260.92063317 260.92063317 102.68967446 218.45316026 218.53167278\n",
      " 218.60248911 218.67274773 218.67308979 218.67369905 218.67409614\n",
      " 218.67404563 218.6741575 ]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.882, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.062}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-7\n",
      "(2879, 25)\n",
      "train:  (11, 6250)\n",
      "TEST:  (34, 6250)\n",
      "Percentuale di anomalie rilevate: 5.88235294117647\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [52.70955378 10.46304918  7.7224837   5.38541229  7.36279484  2.75575468\n",
      "  9.94559983  7.3047752  10.65280086  6.80886116  4.06773887  7.40634519\n",
      " 10.75518775  9.20095821 11.62294803 16.32333976 16.34869966 16.36917664\n",
      " 16.45558727 15.67263771 15.80400094 15.7820513  16.58783733 15.52937112\n",
      " 20.4545168  33.75466014 33.50817761 33.65240792 33.6210829  33.83233846\n",
      " 33.77972166 33.53635441 33.63748444 28.29680748]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.706, 'Precision': 0.5, 'Recall': 0.1, 'F1': 0.167, 'MCC': 0.113}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-8\n",
      "(762, 25)\n",
      "train:  (3, 6250)\n",
      "TEST:  (33, 6250)\n",
      "Percentuale di anomalie rilevate: 6.0606060606060606\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [45.3867966  47.87479438 49.14493566 47.85677111 49.00689211 77.00663504\n",
      " 67.42783528 57.90535067 48.32932393 48.37935586 48.40027866 48.28568503\n",
      " 48.50921372 48.2768144  48.26338012 57.57176003 48.25535594 48.36149597\n",
      " 48.12448659 29.01516473 19.33284765  2.73434342  2.74289165  2.38717696\n",
      "  1.51397664  1.58141775  9.64204215  9.65525443  9.92550808 10.04000549\n",
      " 10.46521092 20.10961973 39.59364532]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.232}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-9\n",
      "(762, 25)\n",
      "train:  (3, 6250)\n",
      "TEST:  (33, 6250)\n",
      "Percentuale di anomalie rilevate: 6.0606060606060606\n",
      "Predizioni nel test set: [0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [ 76.4829317   75.27040224  85.1755056   81.00538642  80.15025406\n",
      " 126.15734441 117.2517976  108.24064098 105.67828207 103.29159454\n",
      " 101.37019272  99.91699907  98.47121847  98.98715281  98.97180733\n",
      "  98.90133746  98.5047506   94.77317348  81.0304466   69.28616787\n",
      "  63.02253478  62.78870578  59.52853183  58.82243159  58.31063859\n",
      "  58.12382936  57.66163031  56.36743149  54.6615908   53.560701\n",
      "  54.84323558  57.15984941  58.38317936]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.232}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: B-1\n",
      "(2435, 25)\n",
      "train:  (9, 6250)\n",
      "TEST:  (32, 6250)\n",
      "Percentuale di anomalie rilevate: 0.0\n",
      "Predizioni nel test set: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predizioni nel test set: [21.01850975 33.05845125 57.2795367   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         51.19801466 59.07767153 59.07767153 59.07767153\n",
      " 59.07767153 59.07767153 59.07767153 59.07767153 59.07767153 59.07767153\n",
      " 59.07767153 59.07767153]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.969, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-1\n",
      "(2158, 55)\n",
      "train:  (8, 13750)\n",
      "TEST:  (9, 13750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuale di anomalie rilevate: 11.11111111111111\n",
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [23.0756684  30.05407848 34.90960003 32.2598797  24.70496345 32.09181994\n",
      " 33.10435843 24.09130424 35.51510883]\n",
      "[0 0 1 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.889, 'Precision': 1.0, 'Recall': 0.5, 'F1': 0.667, 'MCC': 0.661}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-2\n",
      "(764, 55)\n",
      "train:  (3, 13750)\n",
      "TEST:  (8, 13750)\n",
      "Percentuale di anomalie rilevate: 12.5\n",
      "Predizioni nel test set: [0 1 0 1 0 0 0 0]\n",
      "Predizioni nel test set: [15.79729445 44.35559123 22.3510321  35.27945942 31.32262345 30.10263322\n",
      " 31.98266952 24.28955607]\n",
      "[0 1 0 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 1.0, 'Recall': 0.5, 'F1': 0.667, 'MCC': 0.655}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-1\n",
      "(2849, 25)\n",
      "train:  (11, 6250)\n",
      "TEST:  (34, 6250)\n",
      "Percentuale di anomalie rilevate: 5.88235294117647\n",
      "Predizioni nel test set: [0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predizioni nel test set: [ 24.64684279  33.68116634  20.63427459  20.81178404  25.91940577\n",
      "  28.01442378  15.25691669  29.80189047  20.85903454  23.36193982\n",
      "  22.17339023  30.12284136  19.30129026  18.22377627  19.15403342\n",
      "  22.49234918  24.20074283  31.33891502  25.39658289  20.50775646\n",
      "  22.36482259  35.63556438  45.69927514 182.0135471  220.50699497\n",
      " 220.3708334  197.2644389  189.88425095 197.16704537 197.00564107\n",
      " 196.95391673 197.12953557 197.07090539 197.10393944]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 1.0, 'Recall': 0.154, 'F1': 0.267, 'MCC': 0.318}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-11\n",
      "(2611, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (29, 6250)\n",
      "Percentuale di anomalie rilevate: 6.896551724137931\n",
      "Predizioni nel test set: [1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [80.60597405 39.10667202 34.71214067 33.76671956 31.87807245 28.22856351\n",
      " 26.33527672 22.57160883 16.1304537   9.73508566  4.03977743  2.65098148\n",
      "  4.30761189  7.65774986  8.53486604  9.20024338  8.38581868 36.42543806\n",
      " 29.6171022  27.84997829 27.87972153 26.97661585 26.10136536 24.35522228\n",
      " 13.51914257 11.06029548  9.56498667  9.73429828  8.75678393]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.897, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.051}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-12\n",
      "(312, 25)\n",
      "train:  (1, 6250)\n",
      "TEST:  (31, 6250)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors < n_samples_fit, but n_neighbors = 1, n_samples_fit = 1, n_samples = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Addestramento del modello supervisionato\u001b[39;00m\n\u001b[0;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m KNN(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Predizione delle anomalie nei dati di test\u001b[39;00m\n\u001b[0;32m     83\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_test)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\pyod\\models\\knn.py:209\u001b[0m, in \u001b[0;36mKNN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_ \u001b[38;5;241m=\u001b[39m BallTree(X, leaf_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaf_size,\n\u001b[0;32m    207\u001b[0m                               metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric)\n\u001b[1;32m--> 209\u001b[0m dist_arr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneigh_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dist_by_method(dist_arr)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_scores_ \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:834\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m         inequality_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors <= n_samples_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    835\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minequality_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_neighbors = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_neighbors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_samples_fit = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples_fit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    837\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# include n_samples for common tests\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     )\n\u001b[0;32m    840\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    841\u001b[0m chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors < n_samples_fit, but n_neighbors = 1, n_samples_fit = 1, n_samples = 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "# Itera su tutti i canali del dataset\n",
    "for channel_id in NASA.channel_ids:\n",
    "    print(f\"Processing channel: {channel_id}\")\n",
    "    # Lista per memorizzare i segmenti di training\n",
    "    X_train_final = []\n",
    "\n",
    "    # Uso del dataset NASA per tutti i canali\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\")\n",
    "    print(dataset.data.shape)\n",
    "    data = dataset.data\n",
    "    train = []\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        train.append(data[i:i+STEP])\n",
    "\n",
    "    train = np.stack(train)\n",
    "\n",
    "\n",
    "    # ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\", train=False)\n",
    "    data = dataset.data\n",
    "    Test = []\n",
    "    output = []\n",
    "    o = np.zeros(data.shape[0])\n",
    "    for start,end in dataset.anomalies:\n",
    "        o[start:end] = 1\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        Test.append(data[i:i+STEP])\n",
    "        output.append(o[i:i+STEP])\n",
    "\n",
    "    output = np.stack(output)\n",
    "    Test = np.stack(Test)\n",
    "\n",
    "    # ======================= FIT e PREDICT e SCORE =============================\n",
    "    # input_length = train.shape[0]\n",
    "    num_kernels = 10000\n",
    "\n",
    "    train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "    Test = Test.reshape(Test.shape[0], -1)\n",
    "    print(\"train: \", train.shape) \n",
    "    print(\"TEST: \", Test.shape)\n",
    "\n",
    "    kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "    train = train.astype(np.float64)\n",
    "    features_train = apply_kernels(train, kernels)\n",
    "\n",
    "    Test = Test.astype(np.float64)\n",
    "    features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "    # Applica i kernel alle serie temporali\n",
    "    features_train = apply_kernels(train, kernels)\n",
    "\n",
    "    features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "    # RImozioni valori infiniti\n",
    "    features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "    features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "    # Addestramento del modello supervisionato\n",
    "    model = KNN(n_neighbors=1)\n",
    "    model.fit(features_train)\n",
    "\n",
    "    # Predizione delle anomalie nei dati di test\n",
    "    y_pred = model.predict(features_test)\n",
    "    y_proba = model.decision_function(features_test)\n",
    "\n",
    "    threshold = np.percentile(y_proba, 95)  # Soglia al 95° percentile\n",
    "    predicted_anomalies = y_proba > threshold\n",
    "    print(\"Percentuale di anomalie rilevate:\", predicted_anomalies.mean() * 100)\n",
    "\n",
    "    # Visualizzazione dei risultati\n",
    "    print(\"Predizioni nel test set:\", y_pred)\n",
    "    print(\"Predizioni nel test set:\", y_proba)\n",
    "\n",
    "    # Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "    threshold = 25 # -> 10%\n",
    "    # Conta il numero di 1 in ogni lista\n",
    "    counts = np.sum(output, axis=1)\n",
    "    output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "    print(output)\n",
    "    metrics = evaluate_metrics(output, predicted_anomalies)\n",
    "    print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "            \"Channel\": channel_id,\n",
    "            \"Accuracy\": metrics.get(\"Accuracy\", 0),\n",
    "            \"Precision\": metrics.get(\"Precision\", 0),\n",
    "            \"Recall\": metrics.get(\"Recall\", 0),\n",
    "            \"MCC\": metrics.get(\"MCC\", 0),\n",
    "            \"AUC_PR\": metrics.get(\"AUC_PR\", 0),\n",
    "            \"AUC_ROC\": metrics.get(\"AUC_ROC\", 0),\n",
    "            \"F1\": metrics.get(\"F1\", 0),\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "    print(\"=========================FINE CHANNEL=============================\")\n",
    "# ======================= SALVATAGGIO RISULTATI =============================\n",
    "results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Risultati salvati in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec005e",
   "metadata": {},
   "source": [
    "#### Senza KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d276378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "# input_length = train.shape[0]\n",
    "num_kernels = 10000\n",
    "\n",
    "train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "Test = Test.reshape(Test.shape[0], -1)\n",
    "print(\"train: \", train.shape) \n",
    "print(\"TEST: \", Test.shape)\n",
    "\n",
    "kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "train = train.astype(np.float64)\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "Test = Test.astype(np.float64)\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "metrics = evaluate_metrics(output, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d652e18",
   "metadata": {},
   "source": [
    "# Finale Prove ROCKAD -> ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e776fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_26484\\4218593192.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.477, 'Precision': 0.477, 'Recall': 1.0, 'F1': 0.646, 'MCC': 0.0, 'AUC_PR': 0.405, 'AUC_ROC': 0.299, 'PREC_N_SCORES': 0.29}\n"
     ]
    }
   ],
   "source": [
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"]\n",
    "        y_trainS = dfSegment.loc[mask, \"anomaly\"].reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            X_train_final.append(X_trainS[i:i + STEP])\n",
    "            y_train_final.append(y_trainS[i])\n",
    "\n",
    "X_train = np.array(X_train_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_train = np.array(y_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "X_test = np.array(X_test_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_test = np.array(y_test_final)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=20000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "\n",
    "score_train = rockad.predict_proba(X_train).reshape(-1, 1)\n",
    "score_test = rockad.predict_proba(X_test).reshape(-1, 1)\n",
    "\n",
    "decision_func = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "decision_func.fit(score_train, y_train)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
