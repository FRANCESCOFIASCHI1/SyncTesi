{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e613346-c162-4bf8-908d-8e6a5cff0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from pyod.utils.data import precision_n_scores\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Per l'uso della memoria degli algoritmi\n",
    "from memory_profiler import memory_usage\n",
    "# Per la metrica sul tempo di Addestramento e Inferenza\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba180484-282d-4605-990e-bf354cf53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc90bc09-5125-4641-bf52-cd0d4cc7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mean\", \"var\", \"std\", \"len\", \"duration\", \"len_weighted\", \"gaps_squared\", \"n_peaks\",\n",
    "    \"smooth10_n_peaks\", \"smooth20_n_peaks\", \"var_div_duration\", \"var_div_len\",\n",
    "    \"diff_peaks\", \"diff2_peaks\", \"diff_var\", \"diff2_var\", \"kurtosis\", \"skew\",\n",
    "]\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fffebd2-36a4-4c5f-82e0-3b9e4f6c3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "6       0\n",
      "       ..\n",
      "2118    0\n",
      "2120    0\n",
      "2121    0\n",
      "2122    0\n",
      "2123    1\n",
      "Name: anomaly, Length: 1594, dtype: int64\n",
      "X_train (1594, 18)\n",
      "X_test (529, 18)\n",
      "X_train2 (1594, 18)\n",
      "X_test2 (529, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", index_col=\"segment\")\n",
    "\n",
    "X_train, y_train = df.loc[df.train==1, features], df.loc[df.train==1, \"anomaly\"]\n",
    "print(y_train)\n",
    "X_test, y_test = df.loc[df.train==0, features], df.loc[df.train==0, \"anomaly\"]\n",
    "X_train_nominal = df.loc[(df.anomaly==0)&(df.train==1), features]\n",
    "\n",
    "prep = StandardScaler()\n",
    "X_train_nominal2 = prep.fit_transform(X_train_nominal)\n",
    "X_train2 = prep.transform(X_train)\n",
    "X_test2 = prep.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"X_train2\", X_train2.shape)\n",
    "print(\"X_test2\", X_test2.shape)\n",
    "\n",
    "# # Dataset NASA -> SOlar Orbiter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# featuresNASA = [\n",
    "#     \"Radial Distance from Sun(AU)\", \n",
    "#     \"Electronic Box Temperature(DegC)\",\n",
    "#     \"Out Board Sensor Temperature(DegC)\",\n",
    "#     \"In Board Sensor Temperature(DegC)\",\n",
    "#     \"Search Coil Magnetometers Temperature(DegC)\",\n",
    "#     \"Solar Array Angle(Deg)\",\n",
    "#     \"High Gain Antenna azimuth(Deg)\",\n",
    "#     \"High Gain Antenna Elevation(Deg)\",\n",
    "#     \"IBS_R\",\n",
    "#     \"IBS_T\",\n",
    "#     \"IBS_N\",\n",
    "#     \"IBS_time\",\n",
    "#     \"OBS_R\",\n",
    "#     \"OBS_T\",\n",
    "#     \"OBS_N\"\n",
    "# ]\n",
    "\n",
    "# dNASA = pd.read_csv(\"data/Solar_Orbiter.csv\", index_col=\"Date\")\n",
    "\n",
    "# # Filtraggio delle colonne specificate\n",
    "# X = dNASA[featuresNASA]\n",
    "\n",
    "# # Rimozione delle righe con valori mancanti (opzionale)\n",
    "# X_cleaned = X.dropna()\n",
    "\n",
    "# # Divisione dei dati in training e test set\n",
    "# X_trainNASA, X_testNASA = train_test_split(X, test_size=0.2, random_state=42)\n",
    "# # Output dei risultati\n",
    "# print(\"Train set:\", X_trainNASA.shape)\n",
    "# print(\"Test set:\", X_testNASA.shape)\n",
    "\n",
    "# X_trainNASA = X_trainNASA.fillna(X_trainNASA.mean())\n",
    "# X_testNASA = X_testNASA.fillna(X_trainNASA.mean())\n",
    "\n",
    "# prep.fit(X_trainNASA)  # Calcolo delle statistiche (media, deviazione standard)\n",
    "# X_trainNASA2 = prep.transform(X_trainNASA)\n",
    "# X_testNASA2 = prep.transform(X_testNASA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cb2f65-dba5-431f-a7c1-dc4db5d1fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_numpy(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938333f9",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396d74e0-6e0c-40c2-beaf-ea856b6a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(random_state=2137) \n",
      " {'Accuracy': 0.934, 'Precision': 0.89, 'Recall': 0.788, 'F1': 0.836, 'MCC': 0.797, 'AUC_PR': 0.923, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.841}\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=SEED)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2e8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.957, 'Precision': 0.959, 'Recall': 0.832, 'F1': 0.891, 'MCC': 0.867, 'AUC_PR': 0.961, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_score = model.predict_proba(X_test)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa6aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.953, 'Precision': 0.94, 'Recall': 0.832, 'F1': 0.883, 'MCC': 0.856, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.predict_proba(X_test_scaled)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f258319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.928, 'Precision': 0.921, 'Recall': 0.726, 'F1': 0.812, 'MCC': 0.777, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LinearSVC()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d90807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.924, 'Precision': 0.92, 'Recall': 0.708, 'F1': 0.8, 'MCC': 0.764, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f6a6",
   "metadata": {},
   "source": [
    "# Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d242",
   "metadata": {},
   "source": [
    "MO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d742e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmo_gaal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MO_GAAL\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\mo_gaal.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install torch first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _utils\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\graph_settings.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[0;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_random_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_sharding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_graph_pipes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe \u001b[38;5;28;01mas\u001b[39;00m dataframe, \u001b[38;5;28miter\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      3\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Concater,\n\u001b[0;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Demultiplexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ZipperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Zipper,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilelister\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     FileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m FileLister,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "model = MO_GAAL()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n",
    " # {'Accuracy': 0.896, 'Precision': 0.939, 'Recall': 0.549, 'F1': 0.693, 'MCC': 0.669, 'AUC_PR': 0.771, 'AUC_ROC': 0.849, 'PREC_N_SCORES': 0.699}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a030",
   "metadata": {},
   "source": [
    "ANO-GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter: 1\n",
      "Train iter: 2\n",
      "Train iter: 3\n",
      "Train iter: 4\n",
      "Train iter: 5\n",
      "Train iter: 6\n",
      "Train iter: 7\n",
      "Train iter: 8\n",
      "Train iter: 9\n",
      "Train iter: 10\n",
      "Train iter: 11\n",
      "Train iter: 12\n",
      "Train iter: 13\n",
      "Train iter: 14\n",
      "Train iter: 15\n",
      "Train iter: 16\n",
      "Train iter: 17\n",
      "Train iter: 18\n",
      "Train iter: 19\n",
      "Train iter: 20\n",
      "Train iter: 21\n",
      "Train iter: 22\n",
      "Train iter: 23\n",
      "Train iter: 24\n",
      "Train iter: 25\n",
      "Train iter: 26\n",
      "Train iter: 27\n",
      "Train iter: 28\n",
      "Train iter: 29\n",
      "Train iter: 30\n",
      "Train iter: 31\n",
      "Train iter: 32\n",
      "Train iter: 33\n",
      "Train iter: 34\n",
      "Train iter: 35\n",
      "Train iter: 36\n",
      "Train iter: 37\n",
      "Train iter: 38\n",
      "Train iter: 39\n",
      "Train iter: 40\n",
      "Train iter: 41\n",
      "Train iter: 42\n",
      "Train iter: 43\n",
      "Train iter: 44\n",
      "Train iter: 45\n",
      "Train iter: 46\n",
      "Train iter: 47\n",
      "Train iter: 48\n",
      "Train iter: 49\n",
      "Train iter: 50\n",
      "Train iter: 51\n",
      "Train iter: 52\n",
      "Train iter: 53\n",
      "Train iter: 54\n",
      "Train iter: 55\n",
      "Train iter: 56\n",
      "Train iter: 57\n",
      "Train iter: 58\n",
      "Train iter: 59\n",
      "Train iter: 60\n",
      "Train iter: 61\n",
      "Train iter: 62\n",
      "Train iter: 63\n",
      "Train iter: 64\n",
      "Train iter: 65\n",
      "Train iter: 66\n",
      "Train iter: 67\n",
      "Train iter: 68\n",
      "Train iter: 69\n",
      "Train iter: 70\n",
      "Train iter: 71\n",
      "Train iter: 72\n",
      "Train iter: 73\n",
      "Train iter: 74\n",
      "Train iter: 75\n",
      "Train iter: 76\n",
      "Train iter: 77\n",
      "Train iter: 78\n",
      "Train iter: 79\n",
      "Train iter: 80\n",
      "Train iter: 81\n",
      "Train iter: 82\n",
      "Train iter: 83\n",
      "Train iter: 84\n",
      "Train iter: 85\n",
      "Train iter: 86\n",
      "Train iter: 87\n",
      "Train iter: 88\n",
      "Train iter: 89\n",
      "Train iter: 90\n",
      "Train iter: 91\n",
      "Train iter: 92\n",
      "Train iter: 93\n",
      "Train iter: 94\n",
      "Train iter: 95\n",
      "Train iter: 96\n",
      "Train iter: 97\n",
      "Train iter: 98\n",
      "Train iter: 99\n",
      "Train iter: 100\n",
      "Train iter: 101\n",
      "Train iter: 102\n",
      "Train iter: 103\n",
      "Train iter: 104\n",
      "Train iter: 105\n",
      "Train iter: 106\n",
      "Train iter: 107\n",
      "Train iter: 108\n",
      "Train iter: 109\n",
      "Train iter: 110\n",
      "Train iter: 111\n",
      "Train iter: 112\n",
      "Train iter: 113\n",
      "Train iter: 114\n",
      "Train iter: 115\n",
      "Train iter: 116\n",
      "Train iter: 117\n",
      "Train iter: 118\n",
      "Train iter: 119\n",
      "Train iter: 120\n",
      "Train iter: 121\n",
      "Train iter: 122\n",
      "Train iter: 123\n",
      "Train iter: 124\n",
      "Train iter: 125\n",
      "Train iter: 126\n",
      "Train iter: 127\n",
      "Train iter: 128\n",
      "Train iter: 129\n",
      "Train iter: 130\n",
      "Train iter: 131\n",
      "Train iter: 132\n",
      "Train iter: 133\n",
      "Train iter: 134\n",
      "Train iter: 135\n",
      "Train iter: 136\n",
      "Train iter: 137\n",
      "Train iter: 138\n",
      "Train iter: 139\n",
      "Train iter: 140\n",
      "Train iter: 141\n",
      "Train iter: 142\n",
      "Train iter: 143\n",
      "Train iter: 144\n",
      "Train iter: 145\n",
      "Train iter: 146\n",
      "Train iter: 147\n",
      "Train iter: 148\n",
      "Train iter: 149\n",
      "Train iter: 150\n",
      "Train iter: 151\n",
      "Train iter: 152\n",
      "Train iter: 153\n",
      "Train iter: 154\n",
      "Train iter: 155\n",
      "Train iter: 156\n",
      "Train iter: 157\n",
      "Train iter: 158\n",
      "Train iter: 159\n",
      "Train iter: 160\n",
      "Train iter: 161\n",
      "Train iter: 162\n",
      "Train iter: 163\n",
      "Train iter: 164\n",
      "Train iter: 165\n",
      "Train iter: 166\n",
      "Train iter: 167\n",
      "Train iter: 168\n",
      "Train iter: 169\n",
      "Train iter: 170\n",
      "Train iter: 171\n",
      "Train iter: 172\n",
      "Train iter: 173\n",
      "Train iter: 174\n",
      "Train iter: 175\n",
      "Train iter: 176\n",
      "Train iter: 177\n",
      "Train iter: 178\n",
      "Train iter: 179\n",
      "Train iter: 180\n",
      "Train iter: 181\n",
      "Train iter: 182\n",
      "Train iter: 183\n",
      "Train iter: 184\n",
      "Train iter: 185\n",
      "Train iter: 186\n",
      "Train iter: 187\n",
      "Train iter: 188\n",
      "Train iter: 189\n",
      "Train iter: 190\n",
      "Train iter: 191\n",
      "Train iter: 192\n",
      "Train iter: 193\n",
      "Train iter: 194\n",
      "Train iter: 195\n",
      "Train iter: 196\n",
      "Train iter: 197\n",
      "Train iter: 198\n",
      "Train iter: 199\n",
      "Train iter: 200\n",
      "Train iter: 201\n",
      "Train iter: 202\n",
      "Train iter: 203\n",
      "Train iter: 204\n",
      "Train iter: 205\n",
      "Train iter: 206\n",
      "Train iter: 207\n",
      "Train iter: 208\n",
      "Train iter: 209\n",
      "Train iter: 210\n",
      "Train iter: 211\n",
      "Train iter: 212\n",
      "Train iter: 213\n",
      "Train iter: 214\n",
      "Train iter: 215\n",
      "Train iter: 216\n",
      "Train iter: 217\n",
      "Train iter: 218\n",
      "Train iter: 219\n",
      "Train iter: 220\n",
      "Train iter: 221\n",
      "Train iter: 222\n",
      "Train iter: 223\n",
      "Train iter: 224\n",
      "Train iter: 225\n",
      "Train iter: 226\n",
      "Train iter: 227\n",
      "Train iter: 228\n",
      "Train iter: 229\n",
      "Train iter: 230\n",
      "Train iter: 231\n",
      "Train iter: 232\n",
      "Train iter: 233\n",
      "Train iter: 234\n",
      "Train iter: 235\n",
      "Train iter: 236\n",
      "Train iter: 237\n",
      "Train iter: 238\n",
      "Train iter: 239\n",
      "Train iter: 240\n",
      "Train iter: 241\n",
      "Train iter: 242\n",
      "Train iter: 243\n",
      "Train iter: 244\n",
      "Train iter: 245\n",
      "Train iter: 246\n",
      "Train iter: 247\n",
      "Train iter: 248\n",
      "Train iter: 249\n",
      "Train iter: 250\n",
      "Train iter: 251\n",
      "Train iter: 252\n",
      "Train iter: 253\n",
      "Train iter: 254\n",
      "Train iter: 255\n",
      "Train iter: 256\n",
      "Train iter: 257\n",
      "Train iter: 258\n",
      "Train iter: 259\n",
      "Train iter: 260\n",
      "Train iter: 261\n",
      "Train iter: 262\n",
      "Train iter: 263\n",
      "Train iter: 264\n",
      "Train iter: 265\n",
      "Train iter: 266\n",
      "Train iter: 267\n",
      "Train iter: 268\n",
      "Train iter: 269\n",
      "Train iter: 270\n",
      "Train iter: 271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AnoGAN(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# per stampare più cose\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     12\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:314\u001b[0m, in \u001b[0;36mAnoGAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    310\u001b[0m latent_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(X_train_sel\u001b[38;5;241m.\u001b[39msize(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim_G, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(latent_noise)\n\u001b[1;32m--> 314\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_data\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    317\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(real_output, torch\u001b[38;5;241m.\u001b[39mones_like(\n\u001b[0;32m    318\u001b[0m     real_output) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:87\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:327\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\"\n",
    "\n",
    "# Ora importa PyOD e usa AnoGAN come prima\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import tensorflow as tf\n",
    "\n",
    "model = AnoGAN(verbose=1) # per stampare più cose\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682324",
   "metadata": {},
   "source": [
    "SO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione X_train: (1594, 18)\n",
      "Dimensione y_train: (1594,)\n",
      "Dimensione X_test: (529, 18)\n",
      "Dimensione y_test: (529,)\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "SO_GAAL(contamination=0.1, lr_d=0.01, lr_g=0.0001, momentum=0.9,\n",
      "    stop_epochs=20) \n",
      " {'Accuracy': 0.887, 'Precision': 0.934, 'Recall': 0.504, 'F1': 0.655, 'MCC': 0.635, 'AUC_PR': 0.757, 'AUC_ROC': 0.839, 'PREC_N_SCORES': 0.681}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "\n",
    "# Verifica le dimensioni dei dati generati\n",
    "print(\"Dimensione X_train:\", X_train.shape)\n",
    "print(\"Dimensione y_train:\", y_train.shape)\n",
    "print(\"Dimensione X_test:\", X_test.shape)\n",
    "print(\"Dimensione y_test:\", y_test.shape)\n",
    "\n",
    "model = SO_GAAL()\n",
    "model.fit(X_train2[:len(X_train2) // 500 * 500])\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4017",
   "metadata": {},
   "source": [
    "RF+ICCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Inizializza e addestra il modello\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Previsioni e probabilità di previsione\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "# Predizione\n",
    "y_test_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae849e",
   "metadata": {},
   "source": [
    "Linear+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.902, 'Precision': 0.969, 'Recall': 0.558, 'F1': 0.708, 'MCC': 0.69, 'AUC_PR': 0.889, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.814}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Inizializza e addestra il modello Ridge Classifier (Linear + L2)\n",
    "model = RidgeClassifier(alpha=1.0)  # 'alpha' è il parametro di regolarizzazione L2\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predizione delle etichette di classe\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "# Ottieni le probabilità della classe positiva per AUC (si utilizza decision_function per ottenere punteggi di decisione)\n",
    "y_test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Calcola e stampa le metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_test_scores)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed48a",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69cca-f485-4810-b146-d00d216c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest(behaviour='old', bootstrap=False, contamination=0.2, max_features=1.0,\n",
      "    max_samples='auto', n_estimators=100, n_jobs=1, random_state=2137,\n",
      "    verbose=0) \n",
      " {'Accuracy': 0.701, 'Precision': 0.297, 'Recall': 0.292, 'F1': 0.295, 'MCC': 0.105, 'AUC_PR': 0.347, 'AUC_ROC': 0.635, 'PREC_N_SCORES': 0.301}\n"
     ]
    }
   ],
   "source": [
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f31c8",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3608e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0) \n",
      " {'Accuracy': 0.849, 'Precision': 0.78, 'Recall': 0.407, 'F1': 0.535, 'MCC': 0.489, 'AUC_PR': 0.658, 'AUC_ROC': 0.852, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28682eb3",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCSVM(cache_size=200, coef0=0.0, contamination=0.1, degree=3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False) \n",
      " {'Accuracy': 0.837, 'Precision': 0.721, 'Recall': 0.389, 'F1': 0.506, 'MCC': 0.447, 'AUC_PR': 0.659, 'AUC_ROC': 0.788, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "model = OCSVM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40d2e",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOD(contamination=0.1, method='fast', n_neighbors=5) \n",
      " {'Accuracy': 0.845, 'Precision': 0.782, 'Recall': 0.381, 'F1': 0.512, 'MCC': 0.472, 'AUC_PR': 0.644, 'AUC_ROC': 0.843, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "\n",
    "model = ABOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03191a05",
   "metadata": {},
   "source": [
    "INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE(contamination=0.1, max_samples='auto', n_estimators=200,\n",
      "   random_state=None) \n",
      " {'Accuracy': 0.832, 'Precision': 0.694, 'Recall': 0.381, 'F1': 0.491, 'MCC': 0.427, 'AUC_PR': 0.636, 'AUC_ROC': 0.805, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "model = INNE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5366a",
   "metadata": {},
   "source": [
    "ALAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALAD(activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
      "   add_disc_zz_loss=True, add_recon_loss=False, batch_size=32,\n",
      "   contamination=0.1, dec_layers=[5, 10, 25], device=device(type='cpu'),\n",
      "   disc_xx_layers=[25, 10, 5], disc_xz_layers=[25, 10, 5],\n",
      "   disc_zz_layers=[25, 10, 5], dropout_rate=0.2, enc_layers=[25, 10, 5],\n",
      "   epochs=200, lambda_recon_loss=0.1, latent_dim=2,\n",
      "   learning_rate_disc=0.0001, learning_rate_gen=0.0001,\n",
      "   output_activation=None, preprocessing=False,\n",
      "   spectral_normalization=False, verbose=0) \n",
      " {'Accuracy': 0.783, 'Precision': 0.485, 'Recall': 0.283, 'F1': 0.358, 'MCC': 0.25, 'AUC_PR': 0.426, 'AUC_ROC': 0.626, 'PREC_N_SCORES': 0.407}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "\n",
    "model = ALAD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff966da",
   "metadata": {},
   "source": [
    "LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDD(contamination=0.1, dis_measure='aad', n_iter=50, random_state=None) \n",
      " {'Accuracy': 0.822, 'Precision': 1.0, 'Recall': 0.168, 'F1': 0.288, 'MCC': 0.37, 'AUC_PR': 0.624, 'AUC_ROC': 0.765, 'PREC_N_SCORES': 0.663}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "\n",
    "model = LMDD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca5e0",
   "metadata": {},
   "source": [
    "SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOD(alpha=0.8, contamination=0.1, n_neighbors=20, ref_set=10) \n",
      " {'Accuracy': 0.826, 'Precision': 0.611, 'Recall': 0.513, 'F1': 0.558, 'MCC': 0.453, 'AUC_PR': 0.621, 'AUC_ROC': 0.797, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sod import SOD\n",
    "\n",
    "model = SOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df12fb",
   "metadata": {},
   "source": [
    "COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578deac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COF(contamination=0.1, method='fast', n_neighbors=20) \n",
      " {'Accuracy': 0.834, 'Precision': 0.667, 'Recall': 0.442, 'F1': 0.532, 'MCC': 0.449, 'AUC_PR': 0.603, 'AUC_ROC': 0.774, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cof import COF\n",
    "\n",
    "model = COF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35130602",
   "metadata": {},
   "source": [
    "LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12782922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LODA(contamination=0.1, n_bins=10, n_random_cuts=100) \n",
      " {'Accuracy': 0.83, 'Precision': 0.689, 'Recall': 0.372, 'F1': 0.483, 'MCC': 0.418, 'AUC_PR': 0.549, 'AUC_ROC': 0.692, 'PREC_N_SCORES': 0.522}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.loda import LODA\n",
    "\n",
    "model = LODA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f73c",
   "metadata": {},
   "source": [
    "LUNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
      "   n_epochs=200, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
      "   scaler=MinMaxScaler(), val_size=0.1, verbose=0, wd=0.1) \n",
      " {'Accuracy': 0.815, 'Precision': 0.742, 'Recall': 0.204, 'F1': 0.319, 'MCC': 0.322, 'AUC_PR': 0.539, 'AUC_ROC': 0.796, 'PREC_N_SCORES': 0.451}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "model = LUNAR()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d43ae",
   "metadata": {},
   "source": [
    "CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
      "   contamination=0.1, n_clusters=8, n_jobs=None, random_state=None,\n",
      "   use_weights=False) \n",
      " {'Accuracy': 0.802, 'Precision': 0.569, 'Recall': 0.292, 'F1': 0.386, 'MCC': 0.304, 'AUC_PR': 0.45, 'AUC_ROC': 0.574, 'PREC_N_SCORES': 0.372}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "\n",
    "model = CBLOF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d538",
   "metadata": {},
   "source": [
    "DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF(batch_size=1000, contamination=0.1, device=device(type='cpu'),\n",
      "  hidden_activation='tanh', hidden_neurons=[500, 100], max_samples=256,\n",
      "  n_ensemble=50, n_estimators=6, random_state=None, representation_dim=20,\n",
      "  skip_connection=False) \n",
      " {'Accuracy': 0.786, 'Precision': 0.5, 'Recall': 0.009, 'F1': 0.017, 'MCC': 0.043, 'AUC_PR': 0.541, 'AUC_ROC': 0.836, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.dif import DIF\n",
    "\n",
    "model = DIF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.predict_proba(X_test2)[:,1]\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12a8b",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(batch_norm=False, batch_size=32, beta=1.0, capacity=0.0,\n",
      "  compile_mode='default', contamination=0.1,\n",
      "  decoder_neuron_list=[32, 64, 128], device=device(type='cpu'),\n",
      "  dropout_rate=0.2, encoder_neuron_list=[128, 64, 32], epoch_num=30,\n",
      "  hidden_activation_name='relu', latent_dim=2, lr=0.001,\n",
      "  optimizer_name='adam', optimizer_params={'weight_decay': 1e-05},\n",
      "  output_activation_name='sigmoid', preprocessing=True, random_state=42,\n",
      "  use_compile=False, verbose=1) \n",
      " {'Accuracy': 0.794, 'Precision': 0.532, 'Recall': 0.292, 'F1': 0.377, 'MCC': 0.283, 'AUC_PR': 0.446, 'AUC_ROC': 0.687, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "model = VAE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c38a",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(contamination=0.1, covariance_type='full', init_params='kmeans',\n",
      "  max_iter=100, means_init=None, n_components=1, n_init=1,\n",
      "  precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001,\n",
      "  warm_start=False, weights_init=None) \n",
      " {'Accuracy': 0.783, 'Precision': 0.482, 'Recall': 0.239, 'F1': 0.32, 'MCC': 0.225, 'AUC_PR': 0.426, 'AUC_ROC': 0.713, 'PREC_N_SCORES': 0.389}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "\n",
    "model = GMM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8a4e4",
   "metadata": {},
   "source": [
    "DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f094a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.17359483242035\n",
      "Epoch 2/100, Loss: 36.19166633486748\n",
      "Epoch 3/100, Loss: 36.2466336786747\n",
      "Epoch 4/100, Loss: 36.13528761267662\n",
      "Epoch 5/100, Loss: 36.165921211242676\n",
      "Epoch 6/100, Loss: 36.13916572928429\n",
      "Epoch 7/100, Loss: 36.189294904470444\n",
      "Epoch 8/100, Loss: 36.17238187789917\n",
      "Epoch 9/100, Loss: 36.2117395401001\n",
      "Epoch 10/100, Loss: 36.185857594013214\n",
      "Epoch 11/100, Loss: 36.13321906328201\n",
      "Epoch 12/100, Loss: 36.1584706902504\n",
      "Epoch 13/100, Loss: 36.17630282044411\n",
      "Epoch 14/100, Loss: 36.17380636930466\n",
      "Epoch 15/100, Loss: 36.25334322452545\n",
      "Epoch 16/100, Loss: 36.1712027490139\n",
      "Epoch 17/100, Loss: 36.12485006451607\n",
      "Epoch 18/100, Loss: 36.4436274766922\n",
      "Epoch 19/100, Loss: 36.22374951839447\n",
      "Epoch 20/100, Loss: 36.2115415930748\n",
      "Epoch 21/100, Loss: 36.16678577661514\n",
      "Epoch 22/100, Loss: 36.20809951424599\n",
      "Epoch 23/100, Loss: 36.228652626276016\n",
      "Epoch 24/100, Loss: 36.154085248708725\n",
      "Epoch 25/100, Loss: 36.138443648815155\n",
      "Epoch 26/100, Loss: 36.5161928832531\n",
      "Epoch 27/100, Loss: 36.136161506175995\n",
      "Epoch 28/100, Loss: 36.181707948446274\n",
      "Epoch 29/100, Loss: 36.141745775938034\n",
      "Epoch 30/100, Loss: 36.1334473490715\n",
      "Epoch 31/100, Loss: 36.193426355719566\n",
      "Epoch 32/100, Loss: 36.15622678399086\n",
      "Epoch 33/100, Loss: 36.199489802122116\n",
      "Epoch 34/100, Loss: 36.11734637618065\n",
      "Epoch 35/100, Loss: 36.160643100738525\n",
      "Epoch 36/100, Loss: 36.1936252117157\n",
      "Epoch 37/100, Loss: 36.16784855723381\n",
      "Epoch 38/100, Loss: 36.19024500250816\n",
      "Epoch 39/100, Loss: 36.2072534263134\n",
      "Epoch 40/100, Loss: 36.19248494505882\n",
      "Epoch 41/100, Loss: 36.18511536717415\n",
      "Epoch 42/100, Loss: 36.156825214624405\n",
      "Epoch 43/100, Loss: 36.18466040492058\n",
      "Epoch 44/100, Loss: 36.14989456534386\n",
      "Epoch 45/100, Loss: 36.18341547250748\n",
      "Epoch 46/100, Loss: 36.13255634903908\n",
      "Epoch 47/100, Loss: 36.44247457385063\n",
      "Epoch 48/100, Loss: 36.20795226097107\n",
      "Epoch 49/100, Loss: 36.16933789849281\n",
      "Epoch 50/100, Loss: 36.155869632959366\n",
      "Epoch 51/100, Loss: 36.17461675405502\n",
      "Epoch 52/100, Loss: 36.14994007349014\n",
      "Epoch 53/100, Loss: 36.176823407411575\n",
      "Epoch 54/100, Loss: 36.16330271959305\n",
      "Epoch 55/100, Loss: 36.18516033887863\n",
      "Epoch 56/100, Loss: 36.17514684796333\n",
      "Epoch 57/100, Loss: 36.11868315935135\n",
      "Epoch 58/100, Loss: 36.16933134198189\n",
      "Epoch 59/100, Loss: 36.193585991859436\n",
      "Epoch 60/100, Loss: 36.30585631728172\n",
      "Epoch 61/100, Loss: 36.124624133110046\n",
      "Epoch 62/100, Loss: 36.41590037941933\n",
      "Epoch 63/100, Loss: 36.16250681877136\n",
      "Epoch 64/100, Loss: 36.13125276565552\n",
      "Epoch 65/100, Loss: 36.290554732084274\n",
      "Epoch 66/100, Loss: 36.19485479593277\n",
      "Epoch 67/100, Loss: 36.192596822977066\n",
      "Epoch 68/100, Loss: 36.19311338663101\n",
      "Epoch 69/100, Loss: 36.15330824255943\n",
      "Epoch 70/100, Loss: 36.15977245569229\n",
      "Epoch 71/100, Loss: 36.17040690779686\n",
      "Epoch 72/100, Loss: 36.20549160242081\n",
      "Epoch 73/100, Loss: 36.14463156461716\n",
      "Epoch 74/100, Loss: 36.23132652044296\n",
      "Epoch 75/100, Loss: 36.14879962801933\n",
      "Epoch 76/100, Loss: 36.246677339076996\n",
      "Epoch 77/100, Loss: 36.14988797903061\n",
      "Epoch 78/100, Loss: 36.13583964109421\n",
      "Epoch 79/100, Loss: 36.220797061920166\n",
      "Epoch 80/100, Loss: 36.12322652339935\n",
      "Epoch 81/100, Loss: 36.13682180643082\n",
      "Epoch 82/100, Loss: 36.136348247528076\n",
      "Epoch 83/100, Loss: 36.21580085158348\n",
      "Epoch 84/100, Loss: 36.154904037714005\n",
      "Epoch 85/100, Loss: 36.17132344841957\n",
      "Epoch 86/100, Loss: 36.27107375860214\n",
      "Epoch 87/100, Loss: 36.149519234895706\n",
      "Epoch 88/100, Loss: 36.13255423307419\n",
      "Epoch 89/100, Loss: 36.17941099405289\n",
      "Epoch 90/100, Loss: 36.24904045462608\n",
      "Epoch 91/100, Loss: 36.19086942076683\n",
      "Epoch 92/100, Loss: 36.16237214207649\n",
      "Epoch 93/100, Loss: 36.12625986337662\n",
      "Epoch 94/100, Loss: 36.16925394535065\n",
      "Epoch 95/100, Loss: 36.25732374191284\n",
      "Epoch 96/100, Loss: 36.15719136595726\n",
      "Epoch 97/100, Loss: 36.21809810400009\n",
      "Epoch 98/100, Loss: 36.19173404574394\n",
      "Epoch 99/100, Loss: 36.41532385349274\n",
      "Epoch 100/100, Loss: 36.2065212726593\n",
      "DeepSVDD(batch_size=32, c=0.0, contamination=0.1, dropout_rate=0.2,\n",
      "     epochs=100, hidden_activation='relu', hidden_neurons=[64, 32],\n",
      "     l2_regularizer=0.1, n_features=18, optimizer='adam',\n",
      "     output_activation='sigmoid', preprocessing=True, random_state=None,\n",
      "     use_ae=False, validation_size=0.1, verbose=1) \n",
      " {'Accuracy': 0.76, 'Precision': 0.394, 'Recall': 0.23, 'F1': 0.291, 'MCC': 0.166, 'AUC_PR': 0.333, 'AUC_ROC': 0.598, 'PREC_N_SCORES': 0.319}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "# Determina il numero di feature\n",
    "n_features = X_train2.shape[1]\n",
    "\n",
    "model = DeepSVDD(n_features=n_features)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48687c",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(contamination=0.1, copy=True, iterated_power='auto', n_components=None,\n",
      "  n_selected_components=None, random_state=None, standardization=True,\n",
      "  svd_solver='auto', tol=0.0, weighted=True, whiten=False) \n",
      " {'Accuracy': 0.779, 'Precision': 0.464, 'Recall': 0.23, 'F1': 0.308, 'MCC': 0.21, 'AUC_PR': 0.373, 'AUC_ROC': 0.612, 'PREC_N_SCORES': 0.363}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.pca import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f4ce",
   "metadata": {},
   "source": [
    "COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.4, 'Recall': 0.177, 'F1': 0.245, 'MCC': 0.147, 'AUC_PR': 0.328, 'AUC_ROC': 0.627, 'PREC_N_SCORES': 0.257}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78adc",
   "metadata": {},
   "source": [
    "SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5) \n",
      " {'Accuracy': 0.758, 'Precision': 0.364, 'Recall': 0.177, 'F1': 0.238, 'MCC': 0.125, 'AUC_PR': 0.308, 'AUC_ROC': 0.524, 'PREC_N_SCORES': 0.274}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sos import SOS\n",
    "\n",
    "model = SOS()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203a345",
   "metadata": {},
   "source": [
    "ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "model = ECOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0d61",
   "metadata": {},
   "source": [
    "# XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:32:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.968, 'Precision': 0.953, 'Recall': 0.894, 'F1': 0.922, 'MCC': 0.903, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n",
    "\n",
    "#n_estimators=50,\n",
    "#max_depth=3,\n",
    "#learning_rate=0.1,\n",
    "#random_state=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d61e",
   "metadata": {},
   "source": [
    "#### Con metiche di Memoria e Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595e73b",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d685",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:13:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3419463634490967 secondi\n",
      "Uso della memoria durante l'addestramento: 815.8125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.605494499206543 secondi\n",
      "Uso della memoria durante l'inferenza: 815.79296875 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a13a44",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised e Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models,\n",
    "              n_estimators=100,\n",
    "              max_depth=3,\n",
    "              learning_rate=0.2,\n",
    "              n_jobs=-1,\n",
    "              random_state=SEED\n",
    "            )\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796179ff",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:14:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.611022472381592 secondi\n",
      "Uso della memoria durante l'addestramento: 816.11328125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.9620587825775146 secondi\n",
      "Uso della memoria durante l'inferenza: 816.078125 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=2137, reg_alpha=0,\n",
      "   reg_lambda=1, scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=100, max_depth=3, learning_rate=0.2, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39459cc5",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Termina l'esecuzione anticipatamente se per un numero prestabilito di round non migliorano più i parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0157b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 12\n",
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100      # Numero massimo di cicli del'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcc0b0",
   "metadata": {},
   "source": [
    "### XGBOD con ricerca iperparametri con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:16:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:17:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.947, 'Precision': 0.989, 'Recall': 0.761, 'F1': 0.86, 'MCC': 0.839, 'AUC_PR': 0.898, 'AUC_ROC': 0.945, 'PREC_N_SCORES': 0.967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia di parametri\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Inizializza il modello\n",
    "model = XGBOD()\n",
    "\n",
    "# Randomized search con meno iterazioni e parallelizzazione\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Riaddestramento del modello con i migliori parametri\n",
    "model = XGBOD(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35867",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8006 - loss: 0.4877 - val_accuracy: 0.8885 - val_loss: 0.2546\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2390 - val_accuracy: 0.9244 - val_loss: 0.1969\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9334 - loss: 0.1862 - val_accuracy: 0.9168 - val_loss: 0.1949\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.1831 - val_accuracy: 0.9452 - val_loss: 0.1793\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.1629 - val_accuracy: 0.9471 - val_loss: 0.1570\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9424 - loss: 0.1595 - val_accuracy: 0.9546 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9514 - loss: 0.1251 - val_accuracy: 0.9509 - val_loss: 0.1471\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.1225 - val_accuracy: 0.9546 - val_loss: 0.1322\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9428 - loss: 0.1436 - val_accuracy: 0.9565 - val_loss: 0.1249\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9612 - loss: 0.1117 - val_accuracy: 0.9622 - val_loss: 0.1135\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "<Sequential name=sequential, built=True> {'Accuracy': 0.962, 'Precision': 0.927, 'Recall': 0.894, 'F1': 0.91, 'MCC': 0.886, 'AUC_PR': 0.966, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.903}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci il modello FCNN\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Poiché si tratta di una classificazione binaria\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestra il modello\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "y_predicted_score = model.predict(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed2dc",
   "metadata": {},
   "source": [
    "# Rockad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7c494",
   "metadata": {},
   "source": [
    "### 2° Prova un canale -> miglioramento predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8a5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_9824\\51312176.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "End Train\n",
      "score_test:  (15,)\n",
      "[ 59.52569593  97.91287411 104.37438939 306.12021652 117.83811987\n",
      " 206.00498512  96.27331365 112.26971517  85.6070828  154.73770951\n",
      "  68.09927101  68.40500052  69.69060948  70.24381485  62.94952254]\n",
      "RISULTATI:  [0 0 1 1 1 0 0 1 1 1 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.733, 'Precision': 0.764, 'Recall': 0.733, 'F1': 0.736, 'MCC': 0.491, 'AUC_PR': 0.911, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.778}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, average_precision_score\n",
    "\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"F1\": f1_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba, average='weighted').round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_neighbors=5 , n_jobs=-1, n_estimators=10, n_kernels=10000, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "\n",
    "# print(\"mean_train\", mean_train)\n",
    "# print(\"std_train\", std_train)\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55225ada",
   "metadata": {},
   "source": [
    "#### NORMALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddb2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (53, 250, 1)\n",
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_6284\\2734640787.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "score_test:  (15,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.6, 'Recall': 1.0, 'F1': 0.75, 'MCC': 0.0, 'AUC_PR': 0.6, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "\n",
    "# y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# # Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "# X_train_filtered, y_train_filtered = zip(*[\n",
    "#     (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "# ])\n",
    "# X_train_filtered = np.array(X_train_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=100, n_kernels=100, random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f115",
   "metadata": {},
   "source": [
    "### Più Canali e Miglioramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d81df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 1, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_3636\\290305579.py:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  (130,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test: \u001b[39m\u001b[38;5;124m\"\u001b[39m,y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ======================= PRE-PROCESSING =============================\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[0;32m     78\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "print(X_train.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "# print(\"score_test: \", score_test.shape)\n",
    "# print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# Senza parametri con standard scalar: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.407, 'AUC_ROC': 0.297, 'PREC_N_SCORES': 0.29}\n",
    "# Senza standard scalar e parametri: {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.758, 'F1': 0.603, 'MCC': 0.075, 'AUC_PR': 0.753, 'AUC_ROC': 0.707, 'PREC_N_SCORES': 0.677}\n",
    "# con parametri: {'Accuracy': 0.546, 'Precision': 0.515, 'Recall': 0.823, 'F1': 0.634, 'MCC': 0.137, 'AUC_PR': 0.757, 'AUC_ROC': 0.704, 'PREC_N_SCORES': 0.677}\n",
    "# con standard scaler e parametri: {'Accuracy': 0.492, 'Precision': 0.476, 'Recall': 0.645, 'F1': 0.548, 'MCC': -0.002, 'AUC_PR': 0.407, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29} (10,10000)\n",
    "\n",
    "# scaler (20, 10000): {'Accuracy': 0.531, 'Precision': 0.506, 'Recall': 0.694, 'F1': 0.585, 'MCC': 0.08, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "# solo param: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.758, 'AUC_ROC': 0.705, 'PREC_N_SCORES': 0.677}\n",
    "# (30, 10000): {'Accuracy': 0.5, 'Precision': 0.481, 'Recall': 0.613, 'F1': 0.539, 'MCC': 0.01, 'AUC_PR': 0.406, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (35, 10000): {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.597, 'F1': 0.544, 'MCC': 0.053, 'AUC_PR': 0.407, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (40, 10000): {'Accuracy': 0.454, 'Precision': 0.443, 'Recall': 0.565, 'F1': 0.496, 'MCC': -0.084, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "\n",
    "# (10, 20000): \n",
    "# (10, 20000) + no StandardScalar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221baa2",
   "metadata": {},
   "source": [
    "## ROCKAD su NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33b127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 250, 25)\n",
      "====================================================== End Train\n",
      "TEST:  (31, 250, 25)\n",
      "[188.33394021 184.22106524 180.06022805 189.28653757 187.29862371\n",
      " 175.5046242  178.16808366 192.54167875 190.22890328 185.37333139\n",
      " 170.06519542 180.8084682  194.7262789  190.33104492 194.01060727\n",
      " 171.50184742 182.280231   193.96076413 194.62371264 180.41964899\n",
      " 185.93476758 193.34802666 187.11112602 184.39235773 179.15894819\n",
      " 185.45303522 183.63233041 182.32004255 176.03040784 185.05605738\n",
      " 188.93759156]\n",
      "RISULTATI:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.948, 'PREC_N_SCORES': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OUTPUT_FILE = \"risultatiNASA.csv\"\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(train)\n",
    "print(\"====================================================== End Train\")\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "print(\"TEST: \", Test.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(Test)\n",
    "# print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "print(\"output: \", output)\n",
    "metrics = evaluate_metrics(output, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11911284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: A-1\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[ 82.08860678  87.12421601  90.93584394  80.48139259  71.45142211\n",
      "  74.6544119   81.98119664  94.59427848  75.97818634  78.89177956\n",
      "  85.11613083  98.20369625  94.95345196  98.04699428  82.30666029\n",
      "  88.1495757  100.12795806 100.10228633 102.38824119 101.06413859\n",
      "  60.23225995  77.30714109 100.08891003  87.84888708 100.1317634\n",
      "  75.45570894  94.88611955  90.88757704  95.11520738  88.63607656\n",
      " 100.66794379  77.40310205  78.12922976  99.38087133]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-2\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_21128\\406522708.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_test:  (31,)\n",
      "[176.38885648 171.74225778 166.52652265 179.20425882 180.60368303\n",
      " 157.20211025 165.33034712 176.06493943 180.59473158 168.242037\n",
      " 149.60467947 170.36727694 185.09879657 174.13888365 188.50527012\n",
      " 154.38634166 166.66874982 186.39845609 184.36133353 170.51819394\n",
      " 172.62207172 179.02497991 172.92183004 169.67458123 165.42489943\n",
      " 178.82067604 166.32737339 167.17862812 161.85239785 171.29612081\n",
      " 177.18105371]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.948}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-3\n",
      "==== End Train ====\n",
      "score_test:  (32,)\n",
      "[164.62004849 153.30651663 176.03625783 163.41082848 189.48473854\n",
      " 191.16361843 182.38990999 188.33686639 180.76160846 182.34780951\n",
      " 197.26662421 183.19268482 181.65562883 150.69979683 177.249078\n",
      " 190.25935849 188.00179211 187.42144611 193.28020694 149.5497652\n",
      " 167.72871687 143.85845371 149.36251534 165.29106281 143.45500282\n",
      " 137.90936591 147.32562284 180.73429722 162.45760393 156.44624428\n",
      " 153.04333899 151.35726585]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.938, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.968}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-4\n",
      "==== End Train ====\n",
      "score_test:  (32,)\n",
      "[179.38589476 159.51665853 180.28241642 168.78128549 168.90122196\n",
      " 169.88435892 168.615203   164.15830453 170.68434477 176.29312108\n",
      " 169.52464247 169.15913605 168.54458779 165.35910995 175.01742879\n",
      " 169.10731473 181.17875278 177.65460554 170.37579602 161.27202157\n",
      " 176.25834037 162.34862609 173.66788673 175.57010422 173.66691499\n",
      " 173.63719751 178.87188408 185.20216742 172.70213172 169.40947331\n",
      " 179.14840807 174.36886297]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.969, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.056, 'AUC_ROC': 0.452}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-5\n",
      "==== End Train ====\n",
      "score_test:  (18,)\n",
      "[ 87.75990921  87.75990917  87.75990924  87.75990822  87.75990913\n",
      "  87.75990829  87.75990872  87.75990961  87.75990882  87.75990906\n",
      "  87.75990928 242.54181924 242.76115133 242.76115155 242.76115146\n",
      " 242.76115146 242.76115146 242.76115146]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.944, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.647}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-6\n",
      "==== End Train ====\n",
      "score_test:  (17,)\n",
      "[ 90.27420365  90.27420544  90.2742041   90.27421041  90.27421147\n",
      "  90.27421147  90.27421147 142.52271498  90.27423373  90.27423527\n",
      "  90.27423505  90.27423429  90.27423368  90.27423017  90.2742315\n",
      "  90.27423191  90.27423206]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-7\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[196.98626692 198.78370851 194.52346918 195.74197399 191.5685658\n",
      " 191.95975244 193.87919998 201.034052   201.40551974 183.82000606\n",
      " 202.38351553 199.53376803 212.77633536 204.97658486 202.20921888\n",
      " 192.77646187 193.17153146 198.48032591 191.01964533 207.97518079\n",
      " 209.01181606 191.95068169 193.63207797 200.06345829 199.01826179\n",
      " 123.23950463 123.23950524 123.23950517 123.23950452 123.23950522\n",
      " 123.23950469 123.23950495 123.23950419 123.23950348]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.9, 'F1': 0.947, 'AUC_PR': 0.184, 'AUC_ROC': 0.058}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-8\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[ 96.35689464  97.79126311 201.6201135  102.52626413  97.73021605\n",
      "  68.19936001  68.19935849  68.19935965  68.19935926  68.19935928\n",
      "  68.19935969  68.19935839  68.19935994  68.19935802  68.19935758\n",
      "  68.19935942  68.1993594   68.19935907  68.19935905  68.19935869\n",
      "  68.1993584   68.19936702  68.19937208  68.19937168  68.19936853\n",
      "  68.19936339  68.1993722   68.19937163  68.19936346  68.19935878\n",
      "  68.19935746  68.19935715  68.19939567]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.606, 'Precision': 0.536, 'Recall': 1.0, 'F1': 0.698, 'AUC_PR': 0.456, 'AUC_ROC': 0.489}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-9\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[105.41456087 103.89319038 108.84557893 180.50896924 100.9226662\n",
      "  91.97079014  91.97079215  91.97079292  91.97079529  91.9707961\n",
      "  91.97079685  91.97079685  91.97079612  91.97079519  91.97079533\n",
      "  91.97079529  91.97079437  91.97079505  91.97079729  91.97079593\n",
      "  91.97079632  91.97079613  91.97079657  91.97079557  91.97079662\n",
      "  91.97079724  91.97079705  91.97079774  91.97079809  91.97079761\n",
      "  91.9707981   91.97079657  91.97079649]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.515, 'Precision': 0.484, 'Recall': 1.0, 'F1': 0.652, 'AUC_PR': 0.514, 'AUC_ROC': 0.648}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: B-1\n",
      "==== End Train ====\n",
      "score_test:  (32,)\n",
      "[ 99.28346356 107.54789831  12.88619727   0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 233.79680014 244.55876418 244.55876418 244.55876418 244.55876418\n",
      " 244.55876418 244.55876418 244.55876418 244.55876418 244.55876418\n",
      " 244.55876418 244.55876418]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.406, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.083, 'AUC_ROC': 0.645}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-1\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[154.87787625 158.71698389 259.80149475 259.72308017 165.13742308\n",
      " 178.87173296 127.88723358 235.5098498  123.97064479]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.2, 'Recall': 0.5, 'F1': 0.286, 'AUC_PR': 0.611, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-2\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[145.06130365 265.29115078 246.24197683 211.9464413  200.09713332\n",
      " 228.11973994 167.85840295 218.20124505]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.643, 'AUC_ROC': 0.583}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-1\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[ 95.18345626  99.97208714 109.28095401 106.6385973   93.15627811\n",
      "  89.99950045  89.67799351  84.26113821  97.56176914  72.30083168\n",
      " 100.4290825  100.62854979 101.64120485  98.23354423  96.49758566\n",
      "  92.41205329  97.09203003  90.41646354 104.43833471  90.05539964\n",
      "  90.42913253  80.56687465 213.49736393 239.29239129 239.29239004\n",
      " 239.29239441 239.29239173 239.29239422 239.29239412 239.29239225\n",
      " 239.2923908  239.29239512 239.29239343 239.29239281]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.618, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.953, 'AUC_ROC': 0.927}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-11\n",
      "==== End Train ====\n",
      "score_test:  (29,)\n",
      "[1.21687883e+02 6.84910630e+01 2.22841627e-05 2.10035942e-05\n",
      " 1.87289225e-05 2.51219501e-05 1.90429204e-05 2.30465957e-05\n",
      " 2.18095191e-05 1.87358222e-05 2.34216748e-05 2.11029089e-05\n",
      " 1.71100905e-05 1.94432577e-05 2.32220299e-05 3.58738146e-05\n",
      " 2.06879746e-05 1.94412447e+02 7.16314204e+01 7.16314262e+01\n",
      " 7.16314107e+01 7.16314218e+01 7.16314340e+01 7.16314297e+01\n",
      " 7.16314332e+01 7.16314317e+01 7.16314137e+01 7.16314137e+01\n",
      " 7.16314330e+01]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.897, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-13\n",
      "==== End Train ====\n",
      "score_test:  (30,)\n",
      "[110.75552628 109.51239859 105.92613618 108.67852568  91.67337298\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 217.09064608 261.44925051 261.44925051 261.44925051 261.44925051\n",
      " 261.44925051 261.44925051 261.44925051 261.44925051 261.44925051]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.467, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.1, 'AUC_ROC': 0.69}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-14\n",
      "==== End Train ====\n",
      "score_test:  (10,)\n",
      "[139.93414422 147.39717128 142.8717539  142.06809407 148.02362574\n",
      " 146.87408381 179.08808955 288.7970967  102.49636238 151.61796484]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.9, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-15\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[178.60509986 196.04202955 186.30982545 194.37859717 193.03793894\n",
      " 216.87617385 198.82522278 191.69373992]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.417, 'AUC_ROC': 0.583}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-16\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[150.57651104 140.91683667 200.33490679 207.55169204 170.47572037\n",
      " 245.28343138 234.48430376 241.87188556]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.383, 'AUC_ROC': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-2\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[ 61.58615914  82.68590608  88.74841345  74.54983481  72.31863282\n",
      "  91.10588135  80.03134428  89.72247001  81.11237199  80.46825811\n",
      "  73.22451137  85.0541938   86.21307455  82.21049706  78.89538422\n",
      "  71.29461799  72.97559571  82.30027994 161.80562434 228.16425298\n",
      " 231.45670665 220.15800703 218.70465292 218.58007337 220.18131387\n",
      " 223.52638549 223.46570296 215.57490847 229.73060207 223.64144562\n",
      " 227.85705236 216.0144111  227.69683047 219.45512984]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.985, 'AUC_ROC': 0.979}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-3\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[ 72.30406633  90.8497455   86.17709719  91.95631186  77.36120172\n",
      "  79.76971462  78.04658979  87.94462779  91.30321277  65.35267348\n",
      "  84.45985574  75.76394354  93.09669032  85.16441537  74.50346651\n",
      "  73.84601035  84.71682418  95.03214211  84.92793983  99.37104295\n",
      "  80.77078352  87.08139816  84.51880191  91.54552498 107.34622318\n",
      "  92.84334205 141.50903736 220.93890455 223.57066742 223.48410675\n",
      " 224.02706347 227.55723857 233.62608875 223.7652561 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.412, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.872, 'AUC_ROC': 0.868}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-4\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[ 63.56591168  79.12082894  97.80907047  95.81889172  81.99898134\n",
      "  84.0340636   74.78602426  87.0001326   94.7243598   80.01548614\n",
      "  93.07120979  81.7197101   93.59164794  86.17729242  78.26078657\n",
      "  86.11179895  85.89132089  69.18032027  95.04546851  88.2827802\n",
      "  91.17959268  84.99875296  89.90260231  94.72444754 100.16076806\n",
      " 101.9201256  189.73489878 220.22241507 215.70041371 216.00033526\n",
      " 229.90533977 228.16256891 223.33785079]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.576, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.894, 'AUC_ROC': 0.9}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-5\n",
      "==== End Train ====\n",
      "score_test:  (30,)\n",
      "[229.99217678 221.8126241  221.47743507 230.74961395 216.02991777\n",
      " 224.5102861  225.71229872 230.00421454 228.36199483 221.0726578\n",
      " 223.88725227 211.67060013 232.15155952 233.55249602 218.43408988\n",
      " 218.47907403 222.15109473 220.99618227 225.60107039 221.60872087\n",
      " 218.37185177 219.62759218 229.71409176 232.34210025 224.08393151\n",
      " 219.27875209 218.79004332 222.62080999 221.77988371 220.98735632]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.967, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.056, 'AUC_ROC': 0.414}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-6\n",
      "==== End Train ====\n",
      "score_test:  (31,)\n",
      "[105.52653132 125.83124329 121.17398263 118.33667516 116.02802321\n",
      " 115.99976284 126.03102557 114.54295169 107.77966901 125.52531987\n",
      " 127.91589965 128.87371301 112.06490842 112.63795842 131.39455242\n",
      " 127.51419033 132.0398282  121.19858411 115.10724038 125.7154201\n",
      " 126.81493405 120.83676434 129.15803392 120.97203961 120.42581616\n",
      " 122.47762895 120.48306405 112.68781838 104.38005336 120.78132824\n",
      " 119.11787134]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.032, 'Precision': 0.032, 'Recall': 1.0, 'F1': 0.062, 'AUC_PR': 0.1, 'AUC_ROC': 0.7}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-7\n",
      "==== End Train ====\n",
      "score_test:  (30,)\n",
      "[ 99.55164139  94.94996611  87.99736811  90.42292254  89.08676556\n",
      " 100.56522932  73.56544766  80.30575992 103.09359164  97.31031019\n",
      "  76.81407329  80.16202685  84.53425566  76.68048287  93.05388392\n",
      "  89.59038715  91.98166844  85.54612704  88.04350065 105.48539276\n",
      "  89.91621054  93.63513624 100.33209775 104.27485774 177.69917611\n",
      " 218.15094455 224.69034986 223.53188229 214.89027646 222.75555457]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.467, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.915, 'AUC_ROC': 0.928}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-8\n",
      "==== End Train ====\n",
      "score_test:  (31,)\n",
      "[88.34361614 74.38512379  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.         28.79604073\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.        ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.097, 'Precision': 0.034, 'Recall': 1.0, 'F1': 0.067, 'AUC_PR': 0.333, 'AUC_ROC': 0.933}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-9\n",
      "==== End Train ====\n",
      "score_test:  (29,)\n",
      "[ 99.43760569  67.53059257   0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      " 118.49021759 244.27865541 244.27865541 244.27865541]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.069, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-1\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[118.58605491 137.2191487  143.10246292 126.17381801 103.47313567\n",
      " 144.34889233 126.57822173 138.0376954  137.6989301  122.87098787\n",
      " 131.01612496 142.30015512 146.8294989  134.44559617 118.80995203\n",
      " 122.83655505 150.0088802  125.681392   142.75420677 131.56075438\n",
      " 107.60860774 106.46233515 208.20276379 197.55304506 163.61919948\n",
      " 190.2447504  129.24148729 166.99067403 166.81141723 153.31325777\n",
      " 148.94606399 175.43560166 153.36227685 157.66865787]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.588, 'Precision': 0.083, 'Recall': 0.25, 'F1': 0.125, 'AUC_PR': 0.638, 'AUC_ROC': 0.733}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-10\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[118.65761021 135.88103304 146.82973018 124.84595104 103.21058355\n",
      " 142.13203642 126.10824111 136.97541841 139.23150753 124.46045429\n",
      " 138.19382674 138.97423688 148.04397763 136.35326074 115.55161371\n",
      " 119.70156572 147.46446362 126.83090386 142.79121967 132.63819693\n",
      " 112.96392044 107.2017674  206.99518786 196.36619696 169.90834092\n",
      " 182.96936088 157.63203726 157.37320585 157.58853781 146.28449975\n",
      " 155.64709641 178.88152892 130.49669522 171.01146259]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.618, 'Precision': 0.083, 'Recall': 0.333, 'F1': 0.133, 'AUC_PR': 0.698, 'AUC_ROC': 0.688}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-11\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[117.28023882 134.84149251 144.09617057 125.37929939 101.95134206\n",
      " 143.74654102 126.97441933 136.3018848  138.70108387 124.24687442\n",
      " 138.55277847 141.87287966 145.19269794 135.61946977 118.33364565\n",
      " 120.46833531 146.73184039 124.10114785 143.25475103 130.76140941\n",
      " 108.86841278 104.56032581 207.92851642 198.92424826 160.43689499\n",
      " 185.46179989 134.88739864 159.86298487 168.40882126 164.91449127\n",
      " 137.78732578 166.78576825 152.6113602  166.97623396]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 0.1, 'Recall': 0.333, 'F1': 0.154, 'AUC_PR': 0.698, 'AUC_ROC': 0.688}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-12\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[118.60360637 136.33808856 144.19737256 125.62198854 103.52187102\n",
      " 142.46959301 126.43676906 138.42090194 138.86815957 124.12201976\n",
      " 133.66843857 138.88960686 147.63231853 134.01290648 117.457885\n",
      " 119.42927038 147.07478965 126.85266339 140.79648576 131.99935643\n",
      " 111.72369893 107.47394666 128.50352419 143.3052561  162.72551947\n",
      " 183.79788245 140.41293136 158.15396765 162.86900953 171.49081272\n",
      " 139.84940799 168.9719741  152.49246444 164.26038409]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 0.182, 'Recall': 0.5, 'F1': 0.267, 'AUC_PR': 0.146, 'AUC_ROC': 0.467}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-13\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[116.93411217 131.42159949 119.31164656 144.01842212 153.02833854\n",
      " 130.71208455 129.32027662 132.26569058 129.07618059 130.9667693\n",
      " 140.24669469 121.13676056 129.91974256 128.3734743  137.75446892\n",
      " 129.36145461 147.39102669 137.97651796 107.2389807  126.59076092\n",
      " 117.65098571 120.73088749 135.71431732 128.57931591 106.42789226\n",
      " 128.78690003 121.52975091 116.93389806 139.6902646  142.82948675\n",
      " 148.06249257 131.73613263 130.71966138 138.62507976]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 0.231, 'Recall': 0.75, 'F1': 0.353, 'AUC_PR': 0.11, 'AUC_ROC': 0.358}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-2\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[118.30729453 133.49121221 142.07273015 126.91433112 104.67662511\n",
      " 137.0144877  129.18201099 148.8052452  132.6101254  118.82260992\n",
      " 137.29688575 138.69233232 141.48788157 134.20661899 117.93480667\n",
      " 126.73570145 143.01647447 126.87129092 149.02093752 134.27245464\n",
      " 128.00438804 116.95903769 204.14060427 205.73287781 144.02189337\n",
      " 144.12095227 124.33838977 125.99392602 141.24553427 149.78206135\n",
      " 150.42096867 164.6322267  132.99311315 163.5013027 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.588, 'Precision': 0.167, 'Recall': 0.333, 'F1': 0.222, 'AUC_PR': 0.52, 'AUC_ROC': 0.655}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-3\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[128.71673074 124.31541586 126.79355813 126.20549927 129.95749427\n",
      " 133.81509676 118.6848563  127.7782068  130.27614023 131.64814946\n",
      " 131.29676745 125.06060661 124.53554365 117.5239273  126.69118162\n",
      " 126.72673634 129.87014737 126.49542719 132.32759437 128.25997642\n",
      " 134.79561232 127.36114334 167.23920816 179.04167907 179.04168441\n",
      " 179.04167975 179.04167979 185.67243177 141.72978389 126.64638274\n",
      " 126.64638447 126.6463817  126.64638191]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.424, 'Precision': 0.406, 'Recall': 1.0, 'F1': 0.578, 'AUC_PR': 0.796, 'AUC_ROC': 0.762}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-4\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[135.46302522 157.1823253  172.64319927 156.90252724 144.47484806\n",
      " 139.75314711 140.77140121 178.07657727 134.61117706 138.06445104\n",
      " 169.73315262 161.72018792 170.90584028 165.30913061 143.41341166\n",
      " 142.92222643 182.43969856 127.1440599  174.37748009 150.17847664\n",
      " 165.16317839 152.9622144  144.53651978 115.41503282 121.36569571\n",
      " 109.97885338 106.07782853 133.65917467 144.51740496 158.42465158\n",
      " 172.22087282 158.26501627 136.92406628]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.758, 'Precision': 1.0, 'Recall': 0.333, 'F1': 0.5, 'AUC_PR': 0.285, 'AUC_ROC': 0.29}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-5\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[121.35508765 129.30853715 135.64525126 122.86577493  97.94215401\n",
      " 136.89205543 116.75506375 137.15529915 128.14137824 124.9980679\n",
      " 126.5306496  132.77179915 138.88517128 128.91660923 114.44521113\n",
      " 119.57861299 140.21993904 119.53340098 140.32813263 129.31609862\n",
      " 110.38057471 115.31135306 135.81577544 138.36683484 151.06366741\n",
      " 132.40953843 140.58729702 135.28873017 142.98786113 132.40278248\n",
      " 145.18868667 121.84232962 147.89973767]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.576, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.139, 'AUC_ROC': 0.71}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-6\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[ 75.2624712   87.08452421  91.44409684  82.88378774  73.39466555\n",
      "  92.8508544   83.71462275  94.01193197  81.40778581  81.4540579\n",
      "  86.67702686  89.81412252  92.34820688  87.80404969  83.15409577\n",
      "  81.90424976  95.35738435  76.18160007 102.70945591  83.46893331\n",
      "  81.58080896  76.45594817 107.86175116  66.30454742  66.30454719\n",
      "  66.30454736  66.3045472   66.30454675  66.30454823  66.30454755\n",
      "  66.30454532  66.30454785  66.30454706]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.545, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-7\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[ 84.90063177  99.98826046  81.75675253  87.4233761   82.49833605\n",
      "  88.88002953  79.43867404  89.04964059  80.17962206  96.55119272\n",
      "  95.70516291  89.17678611  83.11845348  81.80839901  84.43051314\n",
      "  90.33064236  92.92038048  94.83165867  92.25731291  71.5402308\n",
      "  86.19458492 122.69857538 129.75467435  71.82868808  71.8286892\n",
      "  71.82868845  71.82868867  71.82868893  71.82868803  71.82868873\n",
      "  71.82868854  71.82868865  71.8286889 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-8\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[136.3474018  147.17563331 165.49235646 148.0567997  124.26244574\n",
      " 157.92503079 145.29832305 165.98956892 154.00483599 127.48845164\n",
      " 162.74749038 152.4347375  154.05120484 158.44623836 136.28054946\n",
      " 145.52721683 161.49303529 131.47155264 160.07359612 144.4004323\n",
      " 127.94691171 143.127632   198.5453584  207.89989548 135.01483275\n",
      " 135.01483276 135.01483569 135.01482993 135.0148309  140.09365082\n",
      " 139.7730942  158.67389924 152.55578589 166.80589164]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.294, 'Precision': 0.043, 'Recall': 0.333, 'F1': 0.077, 'AUC_PR': 0.714, 'AUC_ROC': 0.806}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-9\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[169.74444925 183.43964327 197.46564758 174.84622077 145.91019104\n",
      " 180.77085311 174.59872417 169.93900383 161.16923582 156.74851206\n",
      " 168.1523786  165.38896904 181.20132768 177.62514933 169.09294966\n",
      " 168.15085297 186.36253673 178.13560808 167.23940827 156.50610761\n",
      " 158.06710294 158.47836292 188.56152687 179.99070262 179.99070301\n",
      " 179.99070368 179.99070261 179.99070249 179.99070161 179.99069356\n",
      " 170.54355827 185.36441744 187.79105162]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.091, 'Precision': 0.062, 'Recall': 1.0, 'F1': 0.118, 'AUC_PR': 0.341, 'AUC_ROC': 0.839}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-1\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[147.54594685 157.49681225 155.48647932 155.39351263 153.66908738\n",
      " 168.96456389 164.13776985 158.9507277  159.11979175 162.26984862\n",
      " 168.40113821 164.09814207 161.61263144 161.59791714 171.78410117\n",
      " 166.39834219 164.22070629 171.97830208 162.35325961 160.70458911\n",
      " 167.56233453 174.01069411 168.55919837 162.23637356 159.81440393\n",
      " 173.22580262 166.28375609 158.31627764 178.09845153 153.18651686\n",
      " 173.19868468 161.19804135 157.88335158 160.67900545]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.97}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-2\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[106.57545482 103.60236099  75.82193917  62.80425847  61.51346321\n",
      "  70.57202554  60.45100813  77.58494526  75.41438446  63.51960168\n",
      "  76.3065695   65.40977881  79.24374362  68.24088723  74.32332095\n",
      "  68.17870593  66.14120311  71.26995609  78.05366088  69.2728138\n",
      "  67.45912276  56.31647119  86.5716833   80.24947943  79.49384261\n",
      "  64.60077952  73.44389296  80.76689492  56.31647188  56.31647082\n",
      "  56.31647276  56.3164719   59.62526533  76.87601776]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.382, 'Precision': 0.304, 'Recall': 0.583, 'F1': 0.4, 'AUC_PR': 0.407, 'AUC_ROC': 0.451}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-3\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[ 99.52480486 116.29730555 117.46090665 115.13017033 109.25698852\n",
      " 100.14408338  87.13271352  97.9184098  105.01181109  84.26715718\n",
      "  99.08899214 115.65019069 136.39010453 117.79994919 127.84095048\n",
      " 120.97455023 127.92042042 190.94306834 139.7673605  116.57463826\n",
      " 139.85113265 135.55595209 123.38931051  82.55307155  82.55307229\n",
      "  82.55307122  82.5530726   82.55307033  82.55307061  82.55307089\n",
      "  82.55307316  82.5530718   82.5530706 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.97, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.125, 'AUC_ROC': 0.781}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-4\n",
      "==== End Train ====\n",
      "score_test:  (13,)\n",
      "[146.63372953 275.41291461 213.88956183 165.25747628 218.75324254\n",
      " 300.63996063 184.73917156 177.35674358 176.32912751 232.88369661\n",
      " 275.88620346 299.39709784 227.60742169]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.923, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-5\n",
      "==== End Train ====\n",
      "score_test:  (15,)\n",
      "[148.45529557 273.63675091 256.93416386 164.5037002  142.97695705\n",
      " 227.25696137 214.54424148 121.75104645 155.60991878 240.91781886\n",
      " 232.7417408  209.63275754 278.3837656  316.60923636 275.83620578]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.8, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.857}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-7\n",
      "==== End Train ====\n",
      "score_test:  (20,)\n",
      "[232.10876455 213.24362716 202.23202301 210.47717843 206.06792631\n",
      " 272.07426526 209.29339076 229.08283144 268.74032993 209.95286605\n",
      " 272.45453581 253.52705686 221.86574767 278.26063537 245.25890799\n",
      " 246.11816323 247.57839805 241.36398341 215.10161718 228.14122129]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.7, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.95, 'AUC_ROC': 0.984}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-8\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[163.69236539 183.67405969 135.22345266 199.49502231 121.83734183\n",
      " 143.71998551 167.29483814 194.03072694 241.19485162]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.222, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.833, 'AUC_ROC': 0.929}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-1\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[157.71091244 156.51064496 150.57129512 133.72077224 138.12721801\n",
      " 147.11849033 139.1104859  157.63590801 139.17970552 152.45891894\n",
      " 152.47778929 138.63383549 156.4963723  142.08374878 168.22812845\n",
      " 144.67840908 163.78227691 165.25874877 169.56659305 174.66080689\n",
      " 177.94208804 174.47854659 159.42478854 157.91786969 157.88426522\n",
      " 158.23424746 165.59360394 157.0009261  156.79632077 155.73142248\n",
      " 153.27234532 153.96693182 165.65328636]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.242, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.969}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-2\n",
      "==== End Train ====\n",
      "score_test:  (29,)\n",
      "[ 96.41761273  75.9026839   79.69229293  83.65250289  90.26996904\n",
      "  93.02304092  83.72349896  82.09908754  88.29055261  88.09077\n",
      "  95.30361132  87.99719593  85.87823003  90.40489385  86.80808253\n",
      "  93.79324275  96.33627086  89.15347066  73.29366687  91.14692819\n",
      "  95.24571466  92.50435501  71.55367466  89.45130876  95.92564423\n",
      " 103.06868278  93.1653828   79.01377963  96.41273203]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.966, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.25, 'AUC_ROC': 0.893}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-3\n",
      "==== End Train ====\n",
      "score_test:  (31,)\n",
      "[106.0441851   98.32730512  98.14878333  97.08002391 103.2779117\n",
      "  90.89169542 105.53830586  83.83832804  99.68359515  95.35217032\n",
      "  90.64409586  94.89723477 101.12236808  94.32615122  92.0768467\n",
      " 103.17083696  94.64562489  88.94200941  82.48802539  98.90945885\n",
      "  89.77405002 103.40344753  96.0489714  102.13745606  85.25389753\n",
      "  87.30262885  87.16663491  99.40513072 104.32749385  89.20800368\n",
      "  91.92044057]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.968, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.056, 'AUC_ROC': 0.433}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-4\n",
      "==== End Train ====\n",
      "score_test:  (30,)\n",
      "[16.04458256  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " 34.93458191  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.033, 'Precision': 0.033, 'Recall': 1.0, 'F1': 0.065, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-6\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[1.32136386e+02 1.29970547e+02 2.08949611e+01 4.31225565e-05\n",
      " 4.11537211e-05 4.23198379e-05 4.17265146e-05 4.04573841e-05\n",
      " 4.14639968e-05 4.13217742e-05 3.99520851e-05 4.18529661e-05\n",
      " 4.26281780e-05 4.25924007e-05 4.26809410e-05 4.03862769e-05\n",
      " 4.10609593e-05 3.91885189e-05 4.05575542e-05 3.84441908e-05\n",
      " 4.07050328e-05 4.06757306e-05 2.12467366e+02 3.30250483e-05\n",
      " 3.21482051e-05 3.33281426e-05 3.21939067e-05 3.24318499e-05\n",
      " 3.27282452e-05 3.26202201e-05 3.30544049e-05 3.44775803e-05\n",
      " 3.28292601e-05 3.26630454e-05]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.265, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-7\n",
      "==== End Train ====\n",
      "score_test:  (32,)\n",
      "[107.76448057 108.16749076  46.37926327   2.83596196  45.20569134\n",
      "  46.80062922   2.83596196  36.0465254   44.86803164   2.83596196\n",
      "  53.07143148   2.83596196  25.92167228  43.95549831 138.96232964\n",
      "  39.75289592  44.4858555   40.03184777  20.95563219   2.83596196\n",
      " 100.01821925   2.83596196  45.4231259   28.98829705   2.83596196\n",
      "  37.3704855   25.88402502   2.83596196  61.14124507  20.12718642\n",
      " 144.44996659  55.28136017]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.867, 'AUC_ROC': 0.977}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-1\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[123.01548908 256.4966616  219.59887012 215.35651799 263.09736062\n",
      " 164.2949346  133.40983831 134.59072904 139.59575677]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.619, 'AUC_ROC': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-2\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[174.20234005 251.51228943 267.67211356 135.86866813 254.99216305\n",
      " 285.83598672 292.20249906 292.82010704 296.84356805]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.967, 'AUC_ROC': 0.95}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-3\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[156.20953199 211.45872864 158.52199024 190.76445556 224.18319365\n",
      " 220.23956027 189.88500187 223.76725924]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.714}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-4\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[192.67426851 246.61780807 202.36887143 208.11837103 245.80204455\n",
      " 255.80951928 212.33942234 178.92320846]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-5\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[270.86942691 248.80675901 258.75006457 238.96011444 234.50871465\n",
      " 230.17520631 259.49579532 257.21418539 222.73591725]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.375, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-6\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[144.85561377 140.8282293  146.37787722 144.80022605 260.84079851\n",
      " 272.95560341 284.47905429 309.19702467]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-7\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[154.01043602 145.00886184 134.92899156 202.88377556 148.57377488\n",
      " 137.14031698 139.05009857 152.76010735]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.75, 'AUC_ROC': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-1\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[120.12585939 135.58530722 140.27307899 137.62874155 142.59531967\n",
      " 144.84447156 139.98858757 123.58558407 120.9874336  110.09000193\n",
      " 140.84778359 133.66599095 130.93496136 133.80454486 123.25646409\n",
      " 122.54301624 138.16904015 123.11727088 132.26710158 110.74256592\n",
      " 125.85034419 134.42848957 140.01562441 114.40014975 141.25906574\n",
      " 149.81437394 135.50415592 118.82157675 144.28993868 149.190275\n",
      " 131.78552928 127.1274381  135.96968039 129.5402882 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.667, 'Recall': 0.333, 'F1': 0.444, 'AUC_PR': 0.117, 'AUC_ROC': 0.119}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-10\n",
      "==== End Train ====\n",
      "score_test:  (24,)\n",
      "[104.60196833 100.37669606 100.128938   105.98426704 108.13726187\n",
      " 110.63792927 104.40743451 103.39577986 108.35281317  99.82610779\n",
      " 110.05849401 116.19312677 105.67099778 106.63307905 105.62641795\n",
      " 104.81950918 106.71547038 115.23472987 258.95994699 337.58527649\n",
      " 347.7383072  337.48916922 348.38302158 347.97384635]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.958, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.167, 'AUC_ROC': 0.783}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-11\n",
      "==== End Train ====\n",
      "score_test:  (14,)\n",
      "[78.8299309  61.61594202 98.26941116 77.82563297 92.83649518 62.07757039\n",
      " 63.69207236 79.32672117 75.38953053 89.46274188 48.58799865 85.62285636\n",
      " 53.28391744 76.17449062]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.643, 'Precision': 0.2, 'Recall': 0.5, 'F1': 0.286, 'AUC_PR': 0.191, 'AUC_ROC': 0.458}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-14\n",
      "==== End Train ====\n",
      "score_test:  (24,)\n",
      "[ 70.56305429  63.81350521  61.66862226  67.28121142  63.94120456\n",
      "  68.15548519  57.7244041   58.2761805   71.00228244  62.62666746\n",
      " 101.25881188 107.60724093  95.25307096 100.82941828 105.80743044\n",
      "  99.42313168  97.31980901 112.8516916  260.64206577 342.74349431\n",
      " 352.31185056 343.41872013 353.36458078 352.1795011 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.958, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.167, 'AUC_ROC': 0.783}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-15\n",
      "==== End Train ====\n",
      "score_test:  (11,)\n",
      "[100.80821302  59.30439406  91.1601874   49.08534635  94.74229801\n",
      " 307.27171239 342.53535159 344.3286083  350.33676548 345.47220222\n",
      " 360.39895977]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.818, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': None, 'AUC_ROC': None}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-2\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_21128\\406522708.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_test:  (32,)\n",
      "[ 77.23957743  83.01999614  91.22060966  89.47741489  78.61730655\n",
      "  85.23316723  91.69843132  85.69194247  81.08941818  86.32061762\n",
      "  90.8175685   84.92066854  96.46111216  85.47886919  85.08931903\n",
      "  89.1909      86.00687427  80.78789855  88.10825158  87.30870156\n",
      "  84.35091659 177.39181518 224.24198568 196.92874046 185.88121677\n",
      " 159.37006263  70.92879953  70.92879795  70.92880101  70.92879832\n",
      "  70.92880076  70.92880029]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.562, 'Precision': 0.1, 'Recall': 0.167, 'F1': 0.125, 'AUC_PR': 0.867, 'AUC_ROC': 0.846}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-3\n",
      "==== End Train ====\n",
      "score_test:  (33,)\n",
      "[160.32771171 158.52167988 167.75074043 162.35826605 169.03080029\n",
      " 160.49827032 153.42973484 166.51968761 167.10997326 161.94287234\n",
      " 171.07412957 172.36873685 166.07820552 157.44825285 168.15722454\n",
      " 166.13018266 165.0206841  159.59314375 161.29200495 163.58741999\n",
      " 154.25614418 175.17367759 170.52064491 193.13452051 193.38141395\n",
      " 188.82728291 190.25459266 161.36209882 158.27741858 166.18629294\n",
      " 167.98534441 168.69121071 173.06607608]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.818, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.944, 'AUC_ROC': 0.981}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-4\n",
      "==== End Train ====\n",
      "score_test:  (31,)\n",
      "[85.94463448 87.6826956  88.23643787 86.97361136 93.38935053 78.50020094\n",
      " 78.50020094 78.50020094 86.85923776 95.4770775  78.50020094 78.50020094\n",
      " 78.50020094 78.50020094 78.50020094 78.50020094 78.50020094 78.50020094\n",
      " 78.50020094 81.66348369 78.50020094 78.50020094 78.50020094 78.50020094\n",
      " 78.50020094 78.50020094 78.50020094 78.50020094 78.50020094 78.50020094\n",
      " 78.50020094]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.129, 'Precision': 0.042, 'Recall': 0.2, 'F1': 0.069, 'AUC_PR': 0.778, 'AUC_ROC': 0.946}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-7\n",
      "==== End Train ====\n",
      "score_test:  (32,)\n",
      "[174.19523724 184.25149974 176.52181777 201.83302725 174.2676713\n",
      " 172.53048954 151.85087174 169.78230223 182.56780215 174.87738353\n",
      " 170.84809121 175.11087024 185.10488901 186.63412432 171.150914\n",
      " 160.19237407 187.27376492 199.63710066 172.14854818 156.14105022\n",
      " 175.7912446  204.35272802 204.65923834 168.36188263 170.70115343\n",
      " 174.872175   193.91135032 195.82319864 157.24755113 173.52592678\n",
      " 154.8436382  167.25065782]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.488, 'AUC_ROC': 0.573}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: R-1\n",
      "==== End Train ====\n",
      "score_test:  (28,)\n",
      "[9.69205633e+01 7.24023602e+01 3.84564743e-05 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.84564743e-05 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.84564743e-05 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.84564743e-05 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.02255391e+01 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.84564743e-05 3.84564743e-05\n",
      " 3.84564743e-05 3.84564743e-05 3.84564743e-05 3.84564743e-05]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1': 1.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.926}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-1\n",
      "==== End Train ====\n",
      "score_test:  (29,)\n",
      "[186.70372536 186.07171981 178.5799682  186.76925522 177.34585685\n",
      " 185.93873583 179.90335291 179.06330314 183.16430522 178.34665085\n",
      " 174.37969025 174.10590107 182.23078393 169.54121777 179.23815583\n",
      " 167.88080281 180.12519958 182.56580289 178.3983595  176.44053069\n",
      " 182.59850618 171.02158376 187.14210012 194.76374923 185.31796252\n",
      " 187.99900564 192.84558116 193.74440713 191.57214843]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.897, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.12, 'AUC_ROC': 0.444}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-2\n",
      "==== End Train ====\n",
      "score_test:  (7,)\n",
      "[143.09091771 139.5977049  137.61092678 139.00398638 134.90219846\n",
      " 137.90980993 149.21356032]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': None, 'AUC_ROC': None}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_21128\\406522708.py:109: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_test:  (34,)\n",
      "[124.04228417 134.29313995  89.23306933 128.68907448  90.52792507\n",
      " 106.64698781 125.90837953 101.87721982 136.62800499 103.63987842\n",
      " 111.40465038  93.82619563 119.39686327 112.94589654 128.84189583\n",
      " 104.91476801 125.23165716 123.98969008  57.85910411  97.49399431\n",
      "  87.36893178 115.92193738  50.27360518  94.49333586  87.19194428\n",
      " 112.13757189 153.58844074 104.07594588 136.12473497 102.388933\n",
      " 113.06816116  86.03233538 121.07660964 112.78963854]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.324, 'Precision': 0.241, 'Recall': 0.875, 'F1': 0.378, 'AUC_PR': 0.389, 'AUC_ROC': 0.596}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-12\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[140.26809494 319.87431728 257.69870618 245.99950518 154.68611882\n",
      " 270.74775573 137.37353877 338.5267769  118.80670068]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.25, 'AUC_ROC': 0.625}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-13\n",
      "==== End Train ====\n",
      "score_test:  (9,)\n",
      "[ 93.06945102  86.49427926 107.612165   122.85594513 137.77266246\n",
      "  94.28309487 118.59889028 106.17384891  83.4132417 ]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.556, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667, 'AUC_PR': 0.511, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-2\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[124.65622547 130.8903161  128.28928666 123.88193058 162.15612593\n",
      " 132.44998762 138.86832255 116.1360697   88.92682033  95.14054743\n",
      " 106.02833622  96.7096944  110.51259139 126.33857909 124.63155079\n",
      " 157.95897632 133.20867168 158.69905989 121.52648697 114.26007588\n",
      " 111.9147964   89.66756146  81.86672032  96.2459426  105.56246874\n",
      " 102.34466956 115.14994455 131.59970294 172.33353385 126.28476648\n",
      " 132.84250794  90.04406315  54.89523068  62.89512495]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.176, 'Recall': 0.429, 'F1': 0.25, 'AUC_PR': 0.355, 'AUC_ROC': 0.481}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-3\n",
      "==== End Train ====\n",
      "score_test:  (34,)\n",
      "[6.91329973e+01 7.46713854e+01 2.21957904e-05 2.03962146e-05\n",
      " 2.14523048e-05 2.00520712e-05 2.20117147e-05 2.08314419e-05\n",
      " 2.53116050e+01 2.20967016e-05 2.07709052e-05 2.04011543e-05\n",
      " 1.92849597e-05 1.95732612e-05 2.14441349e-05 1.95456968e-05\n",
      " 2.05842030e-05 2.02799955e-05 2.19391181e-05 2.12928650e-05\n",
      " 2.02694023e-05 2.92247427e+01 2.03993281e-05 2.29354022e-05\n",
      " 2.14326676e-05 2.19229670e-05 2.10369514e-05 2.14676650e-05\n",
      " 2.08429111e-05 2.16776112e-05 2.06108385e-05 2.10870934e-05\n",
      " 2.13158734e-05 2.08199154e-05]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.667, 'F1': 0.8, 'AUC_PR': 0.311, 'AUC_ROC': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-4\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[222.92739902 225.38575237 218.41892851 229.99426201 210.38987182\n",
      " 227.98363841 232.46224192 220.05930994]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.125, 'AUC_ROC': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-5\n",
      "==== End Train ====\n",
      "score_test:  (8,)\n",
      "[123.55507452 122.996476   126.96263106 122.1173918  138.40793966\n",
      " 130.47428636 128.02100281 126.59494936]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-8\n",
      "==== End Train ====\n",
      "score_test:  (6,)\n",
      "[160.08910838 166.5265731  167.16870195 164.6888444  155.51659237\n",
      " 166.38961878]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.417, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Risultati salvati in risultatiNASA.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OUTPUT_FILE = \"risultatiNASA.csv\"\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {}\n",
    "    res[\"Accuracy\"] = (y_test == y_pred).mean().round(digits)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "    res[\"Precision\"] = round(precision, digits)\n",
    "    res[\"Recall\"] = round(recall, digits)\n",
    "    res[\"F1\"] = round(f1, digits)\n",
    "\n",
    "    # Verifica per AUC solo se ci sono entrambe le classi\n",
    "    if y_proba is not None:\n",
    "        unique_classes = np.unique(y_test)\n",
    "        if len(unique_classes) > 1:\n",
    "            res[\"AUC_PR\"] = round(average_precision_score(y_test, y_proba), digits)\n",
    "            res[\"AUC_ROC\"] = round(roc_auc_score(y_test, y_proba), digits)\n",
    "        else:\n",
    "            res[\"AUC_PR\"] = None\n",
    "            res[\"AUC_ROC\"] = None\n",
    "    return res\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "# Itera su tutti i canali del dataset\n",
    "for channel_id in NASA.channel_ids:\n",
    "    if channel_id == \"D-12\" or channel_id == \"T-10\" or channel_id == \"T-9\":\n",
    "        continue\n",
    "    print(f\"Processing channel: {channel_id}\")\n",
    "\n",
    "    # Lista per memorizzare i segmenti di training\n",
    "    X_train_final = []\n",
    "\n",
    "    # Uso del dataset NASA per tutti i canali\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\")\n",
    "    # print(dataset.data.shape)\n",
    "    data = dataset.data\n",
    "    train = []\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        train.append(data[i:i+STEP])\n",
    "\n",
    "    train = np.stack(train)\n",
    "    # print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "    # Inizializza e addestra il modello ROCKAD\n",
    "    rockad = ROCKAD(n_neighbors=1, n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "    rockad.fit(train)\n",
    "    print(\"==== End Train ====\")\n",
    "\n",
    "    # Predict anomaly scores\n",
    "    score_train = rockad.predict_proba(train)\n",
    "    # print(\"Score:\", scores)\n",
    "\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\", train=False)\n",
    "    data = dataset.data\n",
    "    Test = []\n",
    "    output = []\n",
    "    o = np.zeros(data.shape[0])\n",
    "    for start,end in dataset.anomalies:\n",
    "        o[start:end] = 1\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        Test.append(data[i:i+STEP])\n",
    "        output.append(o[i:i+STEP])\n",
    "\n",
    "    output = np.stack(output)\n",
    "    Test = np.stack(Test)\n",
    "    # print(\"TEST: \", Test.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "    decision_func = NearestNeighborOCC().fit(score_train)\n",
    "    \n",
    "    score_test = rockad.predict_proba(Test)\n",
    "    print(\"score_test: \", score_test.shape)\n",
    "    print(score_test)\n",
    "\n",
    "    result = decision_func.predict(score_test)\n",
    "    result_binary = np.where(result == -1, 0, 1)\n",
    "    # print(\"RISULTATI: \", result_binary)\n",
    "    #print(\"RISULTATI: \", result2)\n",
    "\n",
    "\n",
    "    # Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "    threshold = 25 # -> 10%\n",
    "    # Conta il numero di 1 in ogni lista\n",
    "    counts = np.sum(output, axis=1)\n",
    "    output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "    # print(\"output: \", output)\n",
    "    metrics = evaluate_metrics(output, result_binary, score_test)\n",
    "    print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        \"Channel\": channel_id,\n",
    "        \"Accuracy\": metrics.get(\"Accuracy\", 0),\n",
    "        \"Precision\": metrics.get(\"Precision\", 0),\n",
    "        \"Recall\": metrics.get(\"Recall\", 0),\n",
    "        \"MCC\": metrics.get(\"MCC\", 0),\n",
    "        \"AUC_PR\": metrics.get(\"AUC_PR\", 0),\n",
    "        \"AUC_ROC\": metrics.get(\"AUC_ROC\", 0),\n",
    "        \"F1\": metrics.get(\"F1\", 0),\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    print(\"=========================FINE CHANNEL=============================\")\n",
    "# ======================= SALVATAGGIO RISULTATI =============================\n",
    "results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Risultati salvati in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77704e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie delle colonne numeriche:\n",
      "Precision    0.112734\n",
      "Recall       0.219228\n",
      "F1           0.123076\n",
      "MCC          0.000000\n",
      "AUC_ROC      0.726727\n",
      "AUC_PR       0.539104\n",
      "Accuracy     0.657190\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "df = pd.read_csv(\"risultatiNASA.csv\")\n",
    "\n",
    "# Calcola la media delle colonne numeriche\n",
    "column_means = df.mean(numeric_only=True)\n",
    "\n",
    "# Stampa le medie\n",
    "print(\"Medie delle colonne numeriche:\")\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a14d98",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3a9d2",
   "metadata": {},
   "source": [
    "# ROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409f2d8",
   "metadata": {},
   "source": [
    "## Unsupervised con Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ffb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1594, 18)\n",
      "(529, 18)\n",
      "Anomalie rilevate nel training set: [1 0 0 ... 0 0 0]\n",
      "Anomalie rilevate nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.834, 'Precision': 0.963, 'Recall': 0.23, 'F1': 0.371, 'MCC': 0.424}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)\n",
    "# {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e056df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_23708\\2536707538.py:52: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "X_train shape: (53, 250)\n",
      "X_test shape: (15, 250)\n",
      "Anomalie rilevate nel training set: [1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Anomalie rilevate nel test set: [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.467, 'Precision': 1.0, 'Recall': 0.111, 'F1': 0.2, 'MCC': 0.218}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "STEP = 250\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # Appiattimento in 2D\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train, kernels)\n",
    "features_test = apply_kernels(X_test, kernels)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036570a",
   "metadata": {},
   "source": [
    "### KNN ( UNSUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4756a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.843, 'Precision': 0.742, 'Recall': 0.407, 'F1': 0.526, 'MCC': 0.47, 'AUC_PR': 0.612, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.531}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.845, 'Precision': 0.763, 'Recall': 0.398, 'F1': 0.523, 'MCC': 0.475, 'AUC_PR': 0.619, 'AUC_ROC': 0.811, 'PREC_N_SCORES': 0.54}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4c45a",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie ROCKET SUPERVISED\n",
    "Utilizzo di vari algoritmi unsupervised e non con kernel ROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eed09",
   "metadata": {},
   "source": [
    "### Regressione Logistica -> Classificatore lineare ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.972, 'Precision': 0.945, 'Recall': 0.92, 'F1': 0.933, 'MCC': 0.915, 'AUC_PR': 0.975, 'AUC_ROC': 0.993, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539711",
   "metadata": {},
   "source": [
    "### Prova con Dettagli dal GitHub del Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.879, 'Precision': 0.98, 'Recall': 0.442, 'F1': 0.61, 'MCC': 0.611, 'AUC_PR': 0.932, 'AUC_ROC': 0.965, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "anomaly_scores_test = model.predict(features_test)\n",
    "anomaly_scores_train = model.predict(features_train)\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "#  {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501c69b",
   "metadata": {},
   "source": [
    "## LogisticClassifierCV ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.953, 'Precision': 0.958, 'Recall': 0.814, 'F1': 0.88, 'MCC': 0.856, 'AUC_PR': 0.951, 'AUC_ROC': 0.982, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from scipy.special import softmax\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = RidgeClassifierCV(alphas = np.logspace(-3, 3, 10))\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "# Per separare multiclasse o monoclasse\n",
    "if  len(np.unique(y_test)) > 2:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=1)\n",
    "else:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=0)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522d8c",
   "metadata": {},
   "source": [
    "# Test Rocket su NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbedd15",
   "metadata": {},
   "source": [
    "### ROCKET con NASA -> Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a85f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n",
      "Percentuale di anomalie rilevate: 6.451612903225806\n",
      "Predizioni nel test set: [1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Predizioni nel test set: [39.64092497 35.70561627 32.19776285 36.11774726 34.7870494  33.50564345\n",
      " 32.94061702 40.43603223 41.8937987  33.1850267  32.15749916 33.07264037\n",
      " 38.93588249 38.95011728 28.98225487 32.39140272 33.55095576 33.85379715\n",
      " 34.75783895 32.49161262 34.44482286 33.66156825 36.31339999 33.09593429\n",
      " 48.51935544 32.84338117 32.6441178  30.52563874 33.9073376  36.49603825\n",
      " 29.11266832]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.069}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "# input_length = train.shape[0]\n",
    "num_kernels = 10000\n",
    "\n",
    "train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "Test = Test.reshape(Test.shape[0], -1)\n",
    "print(\"train: \", train.shape) \n",
    "print(\"TEST: \", Test.shape)\n",
    "\n",
    "kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "train = train.astype(np.float64)\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "Test = Test.astype(np.float64)\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "threshold = np.percentile(y_proba, 95)  # Soglia al 95° percentile\n",
    "predicted_anomalies = y_proba > threshold\n",
    "print(\"Percentuale di anomalie rilevate:\", predicted_anomalies.mean() * 100)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "print(\"Predizioni nel test set:\", y_proba)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "metrics = evaluate_metrics(output, predicted_anomalies)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec005e",
   "metadata": {},
   "source": [
    "#### Senza KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d276378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "# input_length = train.shape[0]\n",
    "num_kernels = 10000\n",
    "\n",
    "train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "Test = Test.reshape(Test.shape[0], -1)\n",
    "print(\"train: \", train.shape) \n",
    "print(\"TEST: \", Test.shape)\n",
    "\n",
    "kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "train = train.astype(np.float64)\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "Test = Test.astype(np.float64)\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "metrics = evaluate_metrics(output, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d652e18",
   "metadata": {},
   "source": [
    "# Finale Prove ROCKAD -> ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e776fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_26484\\4218593192.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.477, 'Precision': 0.477, 'Recall': 1.0, 'F1': 0.646, 'MCC': 0.0, 'AUC_PR': 0.405, 'AUC_ROC': 0.299, 'PREC_N_SCORES': 0.29}\n"
     ]
    }
   ],
   "source": [
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"]\n",
    "        y_trainS = dfSegment.loc[mask, \"anomaly\"].reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            X_train_final.append(X_trainS[i:i + STEP])\n",
    "            y_train_final.append(y_trainS[i])\n",
    "\n",
    "X_train = np.array(X_train_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_train = np.array(y_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "X_test = np.array(X_test_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_test = np.array(y_test_final)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=20000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "\n",
    "score_train = rockad.predict_proba(X_train).reshape(-1, 1)\n",
    "score_test = rockad.predict_proba(X_test).reshape(-1, 1)\n",
    "\n",
    "decision_func = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "decision_func.fit(score_train, y_train)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
