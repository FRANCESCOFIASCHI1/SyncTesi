{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e613346-c162-4bf8-908d-8e6a5cff0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from pyod.utils.data import precision_n_scores\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Per l'uso della memoria degli algoritmi\n",
    "from memory_profiler import memory_usage\n",
    "# Per la metrica sul tempo di Addestramento e Inferenza\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba180484-282d-4605-990e-bf354cf53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc90bc09-5125-4641-bf52-cd0d4cc7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mean\", \"var\", \"std\", \"len\", \"duration\", \"len_weighted\", \"gaps_squared\", \"n_peaks\",\n",
    "    \"smooth10_n_peaks\", \"smooth20_n_peaks\", \"var_div_duration\", \"var_div_len\",\n",
    "    \"diff_peaks\", \"diff2_peaks\", \"diff_var\", \"diff2_var\", \"kurtosis\", \"skew\",\n",
    "]\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fffebd2-36a4-4c5f-82e0-3b9e4f6c3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "6       0\n",
      "       ..\n",
      "2118    0\n",
      "2120    0\n",
      "2121    0\n",
      "2122    0\n",
      "2123    1\n",
      "Name: anomaly, Length: 1594, dtype: int64\n",
      "X_train (1594, 18)\n",
      "X_test (529, 18)\n",
      "X_train2 (1594, 18)\n",
      "X_test2 (529, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", index_col=\"segment\")\n",
    "\n",
    "X_train, y_train = df.loc[df.train==1, features], df.loc[df.train==1, \"anomaly\"]\n",
    "print(y_train)\n",
    "X_test, y_test = df.loc[df.train==0, features], df.loc[df.train==0, \"anomaly\"]\n",
    "X_train_nominal = df.loc[(df.anomaly==0)&(df.train==1), features]\n",
    "\n",
    "prep = StandardScaler()\n",
    "X_train_nominal2 = prep.fit_transform(X_train_nominal)\n",
    "X_train2 = prep.transform(X_train)\n",
    "X_test2 = prep.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"X_train2\", X_train2.shape)\n",
    "print(\"X_test2\", X_test2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5cb2f65-dba5-431f-a7c1-dc4db5d1fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_numpy(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938333f9",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396d74e0-6e0c-40c2-beaf-ea856b6a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(random_state=2137) \n",
      " {'Accuracy': 0.934, 'Precision': 0.89, 'Recall': 0.788, 'F1': 0.836, 'MCC': 0.797, 'AUC_PR': 0.923, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.841}\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=SEED)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2e8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.957, 'Precision': 0.959, 'Recall': 0.832, 'F1': 0.891, 'MCC': 0.867, 'AUC_PR': 0.961, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_score = model.predict_proba(X_test)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aa6aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.953, 'Precision': 0.94, 'Recall': 0.832, 'F1': 0.883, 'MCC': 0.856, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.predict_proba(X_test_scaled)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f258319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.928, 'Precision': 0.921, 'Recall': 0.726, 'F1': 0.812, 'MCC': 0.777, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LinearSVC()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d90807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.924, 'Precision': 0.92, 'Recall': 0.708, 'F1': 0.8, 'MCC': 0.764, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f6a6",
   "metadata": {},
   "source": [
    "# Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d242",
   "metadata": {},
   "source": [
    "MO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d742e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmo_gaal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MO_GAAL\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\mo_gaal.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install torch first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _utils\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\graph_settings.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[0;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_random_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_sharding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_graph_pipes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe \u001b[38;5;28;01mas\u001b[39;00m dataframe, \u001b[38;5;28miter\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      3\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Concater,\n\u001b[0;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Demultiplexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ZipperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Zipper,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilelister\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     FileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m FileLister,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "model = MO_GAAL()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n",
    " # {'Accuracy': 0.896, 'Precision': 0.939, 'Recall': 0.549, 'F1': 0.693, 'MCC': 0.669, 'AUC_PR': 0.771, 'AUC_ROC': 0.849, 'PREC_N_SCORES': 0.699}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a030",
   "metadata": {},
   "source": [
    "ANO-GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter: 1\n",
      "Train iter: 2\n",
      "Train iter: 3\n",
      "Train iter: 4\n",
      "Train iter: 5\n",
      "Train iter: 6\n",
      "Train iter: 7\n",
      "Train iter: 8\n",
      "Train iter: 9\n",
      "Train iter: 10\n",
      "Train iter: 11\n",
      "Train iter: 12\n",
      "Train iter: 13\n",
      "Train iter: 14\n",
      "Train iter: 15\n",
      "Train iter: 16\n",
      "Train iter: 17\n",
      "Train iter: 18\n",
      "Train iter: 19\n",
      "Train iter: 20\n",
      "Train iter: 21\n",
      "Train iter: 22\n",
      "Train iter: 23\n",
      "Train iter: 24\n",
      "Train iter: 25\n",
      "Train iter: 26\n",
      "Train iter: 27\n",
      "Train iter: 28\n",
      "Train iter: 29\n",
      "Train iter: 30\n",
      "Train iter: 31\n",
      "Train iter: 32\n",
      "Train iter: 33\n",
      "Train iter: 34\n",
      "Train iter: 35\n",
      "Train iter: 36\n",
      "Train iter: 37\n",
      "Train iter: 38\n",
      "Train iter: 39\n",
      "Train iter: 40\n",
      "Train iter: 41\n",
      "Train iter: 42\n",
      "Train iter: 43\n",
      "Train iter: 44\n",
      "Train iter: 45\n",
      "Train iter: 46\n",
      "Train iter: 47\n",
      "Train iter: 48\n",
      "Train iter: 49\n",
      "Train iter: 50\n",
      "Train iter: 51\n",
      "Train iter: 52\n",
      "Train iter: 53\n",
      "Train iter: 54\n",
      "Train iter: 55\n",
      "Train iter: 56\n",
      "Train iter: 57\n",
      "Train iter: 58\n",
      "Train iter: 59\n",
      "Train iter: 60\n",
      "Train iter: 61\n",
      "Train iter: 62\n",
      "Train iter: 63\n",
      "Train iter: 64\n",
      "Train iter: 65\n",
      "Train iter: 66\n",
      "Train iter: 67\n",
      "Train iter: 68\n",
      "Train iter: 69\n",
      "Train iter: 70\n",
      "Train iter: 71\n",
      "Train iter: 72\n",
      "Train iter: 73\n",
      "Train iter: 74\n",
      "Train iter: 75\n",
      "Train iter: 76\n",
      "Train iter: 77\n",
      "Train iter: 78\n",
      "Train iter: 79\n",
      "Train iter: 80\n",
      "Train iter: 81\n",
      "Train iter: 82\n",
      "Train iter: 83\n",
      "Train iter: 84\n",
      "Train iter: 85\n",
      "Train iter: 86\n",
      "Train iter: 87\n",
      "Train iter: 88\n",
      "Train iter: 89\n",
      "Train iter: 90\n",
      "Train iter: 91\n",
      "Train iter: 92\n",
      "Train iter: 93\n",
      "Train iter: 94\n",
      "Train iter: 95\n",
      "Train iter: 96\n",
      "Train iter: 97\n",
      "Train iter: 98\n",
      "Train iter: 99\n",
      "Train iter: 100\n",
      "Train iter: 101\n",
      "Train iter: 102\n",
      "Train iter: 103\n",
      "Train iter: 104\n",
      "Train iter: 105\n",
      "Train iter: 106\n",
      "Train iter: 107\n",
      "Train iter: 108\n",
      "Train iter: 109\n",
      "Train iter: 110\n",
      "Train iter: 111\n",
      "Train iter: 112\n",
      "Train iter: 113\n",
      "Train iter: 114\n",
      "Train iter: 115\n",
      "Train iter: 116\n",
      "Train iter: 117\n",
      "Train iter: 118\n",
      "Train iter: 119\n",
      "Train iter: 120\n",
      "Train iter: 121\n",
      "Train iter: 122\n",
      "Train iter: 123\n",
      "Train iter: 124\n",
      "Train iter: 125\n",
      "Train iter: 126\n",
      "Train iter: 127\n",
      "Train iter: 128\n",
      "Train iter: 129\n",
      "Train iter: 130\n",
      "Train iter: 131\n",
      "Train iter: 132\n",
      "Train iter: 133\n",
      "Train iter: 134\n",
      "Train iter: 135\n",
      "Train iter: 136\n",
      "Train iter: 137\n",
      "Train iter: 138\n",
      "Train iter: 139\n",
      "Train iter: 140\n",
      "Train iter: 141\n",
      "Train iter: 142\n",
      "Train iter: 143\n",
      "Train iter: 144\n",
      "Train iter: 145\n",
      "Train iter: 146\n",
      "Train iter: 147\n",
      "Train iter: 148\n",
      "Train iter: 149\n",
      "Train iter: 150\n",
      "Train iter: 151\n",
      "Train iter: 152\n",
      "Train iter: 153\n",
      "Train iter: 154\n",
      "Train iter: 155\n",
      "Train iter: 156\n",
      "Train iter: 157\n",
      "Train iter: 158\n",
      "Train iter: 159\n",
      "Train iter: 160\n",
      "Train iter: 161\n",
      "Train iter: 162\n",
      "Train iter: 163\n",
      "Train iter: 164\n",
      "Train iter: 165\n",
      "Train iter: 166\n",
      "Train iter: 167\n",
      "Train iter: 168\n",
      "Train iter: 169\n",
      "Train iter: 170\n",
      "Train iter: 171\n",
      "Train iter: 172\n",
      "Train iter: 173\n",
      "Train iter: 174\n",
      "Train iter: 175\n",
      "Train iter: 176\n",
      "Train iter: 177\n",
      "Train iter: 178\n",
      "Train iter: 179\n",
      "Train iter: 180\n",
      "Train iter: 181\n",
      "Train iter: 182\n",
      "Train iter: 183\n",
      "Train iter: 184\n",
      "Train iter: 185\n",
      "Train iter: 186\n",
      "Train iter: 187\n",
      "Train iter: 188\n",
      "Train iter: 189\n",
      "Train iter: 190\n",
      "Train iter: 191\n",
      "Train iter: 192\n",
      "Train iter: 193\n",
      "Train iter: 194\n",
      "Train iter: 195\n",
      "Train iter: 196\n",
      "Train iter: 197\n",
      "Train iter: 198\n",
      "Train iter: 199\n",
      "Train iter: 200\n",
      "Train iter: 201\n",
      "Train iter: 202\n",
      "Train iter: 203\n",
      "Train iter: 204\n",
      "Train iter: 205\n",
      "Train iter: 206\n",
      "Train iter: 207\n",
      "Train iter: 208\n",
      "Train iter: 209\n",
      "Train iter: 210\n",
      "Train iter: 211\n",
      "Train iter: 212\n",
      "Train iter: 213\n",
      "Train iter: 214\n",
      "Train iter: 215\n",
      "Train iter: 216\n",
      "Train iter: 217\n",
      "Train iter: 218\n",
      "Train iter: 219\n",
      "Train iter: 220\n",
      "Train iter: 221\n",
      "Train iter: 222\n",
      "Train iter: 223\n",
      "Train iter: 224\n",
      "Train iter: 225\n",
      "Train iter: 226\n",
      "Train iter: 227\n",
      "Train iter: 228\n",
      "Train iter: 229\n",
      "Train iter: 230\n",
      "Train iter: 231\n",
      "Train iter: 232\n",
      "Train iter: 233\n",
      "Train iter: 234\n",
      "Train iter: 235\n",
      "Train iter: 236\n",
      "Train iter: 237\n",
      "Train iter: 238\n",
      "Train iter: 239\n",
      "Train iter: 240\n",
      "Train iter: 241\n",
      "Train iter: 242\n",
      "Train iter: 243\n",
      "Train iter: 244\n",
      "Train iter: 245\n",
      "Train iter: 246\n",
      "Train iter: 247\n",
      "Train iter: 248\n",
      "Train iter: 249\n",
      "Train iter: 250\n",
      "Train iter: 251\n",
      "Train iter: 252\n",
      "Train iter: 253\n",
      "Train iter: 254\n",
      "Train iter: 255\n",
      "Train iter: 256\n",
      "Train iter: 257\n",
      "Train iter: 258\n",
      "Train iter: 259\n",
      "Train iter: 260\n",
      "Train iter: 261\n",
      "Train iter: 262\n",
      "Train iter: 263\n",
      "Train iter: 264\n",
      "Train iter: 265\n",
      "Train iter: 266\n",
      "Train iter: 267\n",
      "Train iter: 268\n",
      "Train iter: 269\n",
      "Train iter: 270\n",
      "Train iter: 271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AnoGAN(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# per stampare più cose\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     12\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:314\u001b[0m, in \u001b[0;36mAnoGAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    310\u001b[0m latent_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(X_train_sel\u001b[38;5;241m.\u001b[39msize(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim_G, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(latent_noise)\n\u001b[1;32m--> 314\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_data\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    317\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(real_output, torch\u001b[38;5;241m.\u001b[39mones_like(\n\u001b[0;32m    318\u001b[0m     real_output) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:87\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:327\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\"\n",
    "\n",
    "# Ora importa PyOD e usa AnoGAN come prima\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import tensorflow as tf\n",
    "\n",
    "model = AnoGAN(verbose=1) # per stampare più cose\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682324",
   "metadata": {},
   "source": [
    "SO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione X_train: (1594, 18)\n",
      "Dimensione y_train: (1594,)\n",
      "Dimensione X_test: (529, 18)\n",
      "Dimensione y_test: (529,)\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "SO_GAAL(contamination=0.1, lr_d=0.01, lr_g=0.0001, momentum=0.9,\n",
      "    stop_epochs=20) \n",
      " {'Accuracy': 0.887, 'Precision': 0.934, 'Recall': 0.504, 'F1': 0.655, 'MCC': 0.635, 'AUC_PR': 0.757, 'AUC_ROC': 0.839, 'PREC_N_SCORES': 0.681}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "\n",
    "# Verifica le dimensioni dei dati generati\n",
    "print(\"Dimensione X_train:\", X_train.shape)\n",
    "print(\"Dimensione y_train:\", y_train.shape)\n",
    "print(\"Dimensione X_test:\", X_test.shape)\n",
    "print(\"Dimensione y_test:\", y_test.shape)\n",
    "\n",
    "model = SO_GAAL()\n",
    "model.fit(X_train2[:len(X_train2) // 500 * 500])\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4017",
   "metadata": {},
   "source": [
    "RF+ICCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Inizializza e addestra il modello\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Previsioni e probabilità di previsione\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "# Predizione\n",
    "y_test_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae849e",
   "metadata": {},
   "source": [
    "Linear+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.902, 'Precision': 0.969, 'Recall': 0.558, 'F1': 0.708, 'MCC': 0.69, 'AUC_PR': 0.889, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.814}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Inizializza e addestra il modello Ridge Classifier (Linear + L2)\n",
    "model = RidgeClassifier(alpha=1.0)  # 'alpha' è il parametro di regolarizzazione L2\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predizione delle etichette di classe\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "# Ottieni le probabilità della classe positiva per AUC (si utilizza decision_function per ottenere punteggi di decisione)\n",
    "y_test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Calcola e stampa le metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_test_scores)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed48a",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69cca-f485-4810-b146-d00d216c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest(behaviour='old', bootstrap=False, contamination=0.2, max_features=1.0,\n",
      "    max_samples='auto', n_estimators=100, n_jobs=1, random_state=2137,\n",
      "    verbose=0) \n",
      " {'Accuracy': 0.701, 'Precision': 0.297, 'Recall': 0.292, 'F1': 0.295, 'MCC': 0.105, 'AUC_PR': 0.347, 'AUC_ROC': 0.635, 'PREC_N_SCORES': 0.301}\n"
     ]
    }
   ],
   "source": [
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f31c8",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3608e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0) \n",
      " {'Accuracy': 0.849, 'Precision': 0.78, 'Recall': 0.407, 'F1': 0.535, 'MCC': 0.489, 'AUC_PR': 0.658, 'AUC_ROC': 0.852, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28682eb3",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCSVM(cache_size=200, coef0=0.0, contamination=0.1, degree=3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False) \n",
      " {'Accuracy': 0.837, 'Precision': 0.721, 'Recall': 0.389, 'F1': 0.506, 'MCC': 0.447, 'AUC_PR': 0.659, 'AUC_ROC': 0.788, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "model = OCSVM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40d2e",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOD(contamination=0.1, method='fast', n_neighbors=5) \n",
      " {'Accuracy': 0.845, 'Precision': 0.782, 'Recall': 0.381, 'F1': 0.512, 'MCC': 0.472, 'AUC_PR': 0.644, 'AUC_ROC': 0.843, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "\n",
    "model = ABOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03191a05",
   "metadata": {},
   "source": [
    "INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE(contamination=0.1, max_samples='auto', n_estimators=200,\n",
      "   random_state=None) \n",
      " {'Accuracy': 0.832, 'Precision': 0.694, 'Recall': 0.381, 'F1': 0.491, 'MCC': 0.427, 'AUC_PR': 0.636, 'AUC_ROC': 0.805, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "model = INNE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5366a",
   "metadata": {},
   "source": [
    "ALAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALAD(activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
      "   add_disc_zz_loss=True, add_recon_loss=False, batch_size=32,\n",
      "   contamination=0.1, dec_layers=[5, 10, 25], device=device(type='cpu'),\n",
      "   disc_xx_layers=[25, 10, 5], disc_xz_layers=[25, 10, 5],\n",
      "   disc_zz_layers=[25, 10, 5], dropout_rate=0.2, enc_layers=[25, 10, 5],\n",
      "   epochs=200, lambda_recon_loss=0.1, latent_dim=2,\n",
      "   learning_rate_disc=0.0001, learning_rate_gen=0.0001,\n",
      "   output_activation=None, preprocessing=False,\n",
      "   spectral_normalization=False, verbose=0) \n",
      " {'Accuracy': 0.783, 'Precision': 0.485, 'Recall': 0.283, 'F1': 0.358, 'MCC': 0.25, 'AUC_PR': 0.426, 'AUC_ROC': 0.626, 'PREC_N_SCORES': 0.407}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "\n",
    "model = ALAD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff966da",
   "metadata": {},
   "source": [
    "LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDD(contamination=0.1, dis_measure='aad', n_iter=50, random_state=None) \n",
      " {'Accuracy': 0.822, 'Precision': 1.0, 'Recall': 0.168, 'F1': 0.288, 'MCC': 0.37, 'AUC_PR': 0.624, 'AUC_ROC': 0.765, 'PREC_N_SCORES': 0.663}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "\n",
    "model = LMDD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca5e0",
   "metadata": {},
   "source": [
    "SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOD(alpha=0.8, contamination=0.1, n_neighbors=20, ref_set=10) \n",
      " {'Accuracy': 0.826, 'Precision': 0.611, 'Recall': 0.513, 'F1': 0.558, 'MCC': 0.453, 'AUC_PR': 0.621, 'AUC_ROC': 0.797, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sod import SOD\n",
    "\n",
    "model = SOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df12fb",
   "metadata": {},
   "source": [
    "COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578deac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COF(contamination=0.1, method='fast', n_neighbors=20) \n",
      " {'Accuracy': 0.834, 'Precision': 0.667, 'Recall': 0.442, 'F1': 0.532, 'MCC': 0.449, 'AUC_PR': 0.603, 'AUC_ROC': 0.774, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cof import COF\n",
    "\n",
    "model = COF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35130602",
   "metadata": {},
   "source": [
    "LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12782922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LODA(contamination=0.1, n_bins=10, n_random_cuts=100) \n",
      " {'Accuracy': 0.83, 'Precision': 0.689, 'Recall': 0.372, 'F1': 0.483, 'MCC': 0.418, 'AUC_PR': 0.549, 'AUC_ROC': 0.692, 'PREC_N_SCORES': 0.522}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.loda import LODA\n",
    "\n",
    "model = LODA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f73c",
   "metadata": {},
   "source": [
    "LUNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
      "   n_epochs=200, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
      "   scaler=MinMaxScaler(), val_size=0.1, verbose=0, wd=0.1) \n",
      " {'Accuracy': 0.815, 'Precision': 0.742, 'Recall': 0.204, 'F1': 0.319, 'MCC': 0.322, 'AUC_PR': 0.539, 'AUC_ROC': 0.796, 'PREC_N_SCORES': 0.451}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "model = LUNAR()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d43ae",
   "metadata": {},
   "source": [
    "CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
      "   contamination=0.1, n_clusters=8, n_jobs=None, random_state=None,\n",
      "   use_weights=False) \n",
      " {'Accuracy': 0.802, 'Precision': 0.569, 'Recall': 0.292, 'F1': 0.386, 'MCC': 0.304, 'AUC_PR': 0.45, 'AUC_ROC': 0.574, 'PREC_N_SCORES': 0.372}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "\n",
    "model = CBLOF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d538",
   "metadata": {},
   "source": [
    "DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF(batch_size=1000, contamination=0.1, device=device(type='cpu'),\n",
      "  hidden_activation='tanh', hidden_neurons=[500, 100], max_samples=256,\n",
      "  n_ensemble=50, n_estimators=6, random_state=None, representation_dim=20,\n",
      "  skip_connection=False) \n",
      " {'Accuracy': 0.786, 'Precision': 0.5, 'Recall': 0.009, 'F1': 0.017, 'MCC': 0.043, 'AUC_PR': 0.541, 'AUC_ROC': 0.836, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.dif import DIF\n",
    "\n",
    "model = DIF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.predict_proba(X_test2)[:,1]\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12a8b",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(batch_norm=False, batch_size=32, beta=1.0, capacity=0.0,\n",
      "  compile_mode='default', contamination=0.1,\n",
      "  decoder_neuron_list=[32, 64, 128], device=device(type='cpu'),\n",
      "  dropout_rate=0.2, encoder_neuron_list=[128, 64, 32], epoch_num=30,\n",
      "  hidden_activation_name='relu', latent_dim=2, lr=0.001,\n",
      "  optimizer_name='adam', optimizer_params={'weight_decay': 1e-05},\n",
      "  output_activation_name='sigmoid', preprocessing=True, random_state=42,\n",
      "  use_compile=False, verbose=1) \n",
      " {'Accuracy': 0.794, 'Precision': 0.532, 'Recall': 0.292, 'F1': 0.377, 'MCC': 0.283, 'AUC_PR': 0.446, 'AUC_ROC': 0.687, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "model = VAE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c38a",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(contamination=0.1, covariance_type='full', init_params='kmeans',\n",
      "  max_iter=100, means_init=None, n_components=1, n_init=1,\n",
      "  precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001,\n",
      "  warm_start=False, weights_init=None) \n",
      " {'Accuracy': 0.783, 'Precision': 0.482, 'Recall': 0.239, 'F1': 0.32, 'MCC': 0.225, 'AUC_PR': 0.426, 'AUC_ROC': 0.713, 'PREC_N_SCORES': 0.389}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "\n",
    "model = GMM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8a4e4",
   "metadata": {},
   "source": [
    "DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f094a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.17359483242035\n",
      "Epoch 2/100, Loss: 36.19166633486748\n",
      "Epoch 3/100, Loss: 36.2466336786747\n",
      "Epoch 4/100, Loss: 36.13528761267662\n",
      "Epoch 5/100, Loss: 36.165921211242676\n",
      "Epoch 6/100, Loss: 36.13916572928429\n",
      "Epoch 7/100, Loss: 36.189294904470444\n",
      "Epoch 8/100, Loss: 36.17238187789917\n",
      "Epoch 9/100, Loss: 36.2117395401001\n",
      "Epoch 10/100, Loss: 36.185857594013214\n",
      "Epoch 11/100, Loss: 36.13321906328201\n",
      "Epoch 12/100, Loss: 36.1584706902504\n",
      "Epoch 13/100, Loss: 36.17630282044411\n",
      "Epoch 14/100, Loss: 36.17380636930466\n",
      "Epoch 15/100, Loss: 36.25334322452545\n",
      "Epoch 16/100, Loss: 36.1712027490139\n",
      "Epoch 17/100, Loss: 36.12485006451607\n",
      "Epoch 18/100, Loss: 36.4436274766922\n",
      "Epoch 19/100, Loss: 36.22374951839447\n",
      "Epoch 20/100, Loss: 36.2115415930748\n",
      "Epoch 21/100, Loss: 36.16678577661514\n",
      "Epoch 22/100, Loss: 36.20809951424599\n",
      "Epoch 23/100, Loss: 36.228652626276016\n",
      "Epoch 24/100, Loss: 36.154085248708725\n",
      "Epoch 25/100, Loss: 36.138443648815155\n",
      "Epoch 26/100, Loss: 36.5161928832531\n",
      "Epoch 27/100, Loss: 36.136161506175995\n",
      "Epoch 28/100, Loss: 36.181707948446274\n",
      "Epoch 29/100, Loss: 36.141745775938034\n",
      "Epoch 30/100, Loss: 36.1334473490715\n",
      "Epoch 31/100, Loss: 36.193426355719566\n",
      "Epoch 32/100, Loss: 36.15622678399086\n",
      "Epoch 33/100, Loss: 36.199489802122116\n",
      "Epoch 34/100, Loss: 36.11734637618065\n",
      "Epoch 35/100, Loss: 36.160643100738525\n",
      "Epoch 36/100, Loss: 36.1936252117157\n",
      "Epoch 37/100, Loss: 36.16784855723381\n",
      "Epoch 38/100, Loss: 36.19024500250816\n",
      "Epoch 39/100, Loss: 36.2072534263134\n",
      "Epoch 40/100, Loss: 36.19248494505882\n",
      "Epoch 41/100, Loss: 36.18511536717415\n",
      "Epoch 42/100, Loss: 36.156825214624405\n",
      "Epoch 43/100, Loss: 36.18466040492058\n",
      "Epoch 44/100, Loss: 36.14989456534386\n",
      "Epoch 45/100, Loss: 36.18341547250748\n",
      "Epoch 46/100, Loss: 36.13255634903908\n",
      "Epoch 47/100, Loss: 36.44247457385063\n",
      "Epoch 48/100, Loss: 36.20795226097107\n",
      "Epoch 49/100, Loss: 36.16933789849281\n",
      "Epoch 50/100, Loss: 36.155869632959366\n",
      "Epoch 51/100, Loss: 36.17461675405502\n",
      "Epoch 52/100, Loss: 36.14994007349014\n",
      "Epoch 53/100, Loss: 36.176823407411575\n",
      "Epoch 54/100, Loss: 36.16330271959305\n",
      "Epoch 55/100, Loss: 36.18516033887863\n",
      "Epoch 56/100, Loss: 36.17514684796333\n",
      "Epoch 57/100, Loss: 36.11868315935135\n",
      "Epoch 58/100, Loss: 36.16933134198189\n",
      "Epoch 59/100, Loss: 36.193585991859436\n",
      "Epoch 60/100, Loss: 36.30585631728172\n",
      "Epoch 61/100, Loss: 36.124624133110046\n",
      "Epoch 62/100, Loss: 36.41590037941933\n",
      "Epoch 63/100, Loss: 36.16250681877136\n",
      "Epoch 64/100, Loss: 36.13125276565552\n",
      "Epoch 65/100, Loss: 36.290554732084274\n",
      "Epoch 66/100, Loss: 36.19485479593277\n",
      "Epoch 67/100, Loss: 36.192596822977066\n",
      "Epoch 68/100, Loss: 36.19311338663101\n",
      "Epoch 69/100, Loss: 36.15330824255943\n",
      "Epoch 70/100, Loss: 36.15977245569229\n",
      "Epoch 71/100, Loss: 36.17040690779686\n",
      "Epoch 72/100, Loss: 36.20549160242081\n",
      "Epoch 73/100, Loss: 36.14463156461716\n",
      "Epoch 74/100, Loss: 36.23132652044296\n",
      "Epoch 75/100, Loss: 36.14879962801933\n",
      "Epoch 76/100, Loss: 36.246677339076996\n",
      "Epoch 77/100, Loss: 36.14988797903061\n",
      "Epoch 78/100, Loss: 36.13583964109421\n",
      "Epoch 79/100, Loss: 36.220797061920166\n",
      "Epoch 80/100, Loss: 36.12322652339935\n",
      "Epoch 81/100, Loss: 36.13682180643082\n",
      "Epoch 82/100, Loss: 36.136348247528076\n",
      "Epoch 83/100, Loss: 36.21580085158348\n",
      "Epoch 84/100, Loss: 36.154904037714005\n",
      "Epoch 85/100, Loss: 36.17132344841957\n",
      "Epoch 86/100, Loss: 36.27107375860214\n",
      "Epoch 87/100, Loss: 36.149519234895706\n",
      "Epoch 88/100, Loss: 36.13255423307419\n",
      "Epoch 89/100, Loss: 36.17941099405289\n",
      "Epoch 90/100, Loss: 36.24904045462608\n",
      "Epoch 91/100, Loss: 36.19086942076683\n",
      "Epoch 92/100, Loss: 36.16237214207649\n",
      "Epoch 93/100, Loss: 36.12625986337662\n",
      "Epoch 94/100, Loss: 36.16925394535065\n",
      "Epoch 95/100, Loss: 36.25732374191284\n",
      "Epoch 96/100, Loss: 36.15719136595726\n",
      "Epoch 97/100, Loss: 36.21809810400009\n",
      "Epoch 98/100, Loss: 36.19173404574394\n",
      "Epoch 99/100, Loss: 36.41532385349274\n",
      "Epoch 100/100, Loss: 36.2065212726593\n",
      "DeepSVDD(batch_size=32, c=0.0, contamination=0.1, dropout_rate=0.2,\n",
      "     epochs=100, hidden_activation='relu', hidden_neurons=[64, 32],\n",
      "     l2_regularizer=0.1, n_features=18, optimizer='adam',\n",
      "     output_activation='sigmoid', preprocessing=True, random_state=None,\n",
      "     use_ae=False, validation_size=0.1, verbose=1) \n",
      " {'Accuracy': 0.76, 'Precision': 0.394, 'Recall': 0.23, 'F1': 0.291, 'MCC': 0.166, 'AUC_PR': 0.333, 'AUC_ROC': 0.598, 'PREC_N_SCORES': 0.319}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "# Determina il numero di feature\n",
    "n_features = X_train2.shape[1]\n",
    "\n",
    "model = DeepSVDD(n_features=n_features)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48687c",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(contamination=0.1, copy=True, iterated_power='auto', n_components=None,\n",
      "  n_selected_components=None, random_state=None, standardization=True,\n",
      "  svd_solver='auto', tol=0.0, weighted=True, whiten=False) \n",
      " {'Accuracy': 0.779, 'Precision': 0.464, 'Recall': 0.23, 'F1': 0.308, 'MCC': 0.21, 'AUC_PR': 0.373, 'AUC_ROC': 0.612, 'PREC_N_SCORES': 0.363}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.pca import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f4ce",
   "metadata": {},
   "source": [
    "COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.4, 'Recall': 0.177, 'F1': 0.245, 'MCC': 0.147, 'AUC_PR': 0.328, 'AUC_ROC': 0.627, 'PREC_N_SCORES': 0.257}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78adc",
   "metadata": {},
   "source": [
    "SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5) \n",
      " {'Accuracy': 0.758, 'Precision': 0.364, 'Recall': 0.177, 'F1': 0.238, 'MCC': 0.125, 'AUC_PR': 0.308, 'AUC_ROC': 0.524, 'PREC_N_SCORES': 0.274}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sos import SOS\n",
    "\n",
    "model = SOS()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203a345",
   "metadata": {},
   "source": [
    "ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "model = ECOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0d61",
   "metadata": {},
   "source": [
    "# XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:32:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.968, 'Precision': 0.953, 'Recall': 0.894, 'F1': 0.922, 'MCC': 0.903, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n",
    "\n",
    "#n_estimators=50,\n",
    "#max_depth=3,\n",
    "#learning_rate=0.1,\n",
    "#random_state=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d61e",
   "metadata": {},
   "source": [
    "#### Con metiche di Memoria e Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595e73b",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d685",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:13:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3419463634490967 secondi\n",
      "Uso della memoria durante l'addestramento: 815.8125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.605494499206543 secondi\n",
      "Uso della memoria durante l'inferenza: 815.79296875 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a13a44",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised e Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models,\n",
    "              n_estimators=100,\n",
    "              max_depth=3,\n",
    "              learning_rate=0.2,\n",
    "              n_jobs=-1,\n",
    "              random_state=SEED\n",
    "            )\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796179ff",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:14:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.611022472381592 secondi\n",
      "Uso della memoria durante l'addestramento: 816.11328125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.9620587825775146 secondi\n",
      "Uso della memoria durante l'inferenza: 816.078125 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=2137, reg_alpha=0,\n",
      "   reg_lambda=1, scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=100, max_depth=3, learning_rate=0.2, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39459cc5",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Termina l'esecuzione anticipatamente se per un numero prestabilito di round non migliorano più i parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0157b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 12\n",
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100      # Numero massimo di cicli del'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcc0b0",
   "metadata": {},
   "source": [
    "### XGBOD con ricerca iperparametri con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:16:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:17:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.947, 'Precision': 0.989, 'Recall': 0.761, 'F1': 0.86, 'MCC': 0.839, 'AUC_PR': 0.898, 'AUC_ROC': 0.945, 'PREC_N_SCORES': 0.967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia di parametri\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Inizializza il modello\n",
    "model = XGBOD()\n",
    "\n",
    "# Randomized search con meno iterazioni e parallelizzazione\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Riaddestramento del modello con i migliori parametri\n",
    "model = XGBOD(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35867",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8006 - loss: 0.4877 - val_accuracy: 0.8885 - val_loss: 0.2546\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2390 - val_accuracy: 0.9244 - val_loss: 0.1969\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9334 - loss: 0.1862 - val_accuracy: 0.9168 - val_loss: 0.1949\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.1831 - val_accuracy: 0.9452 - val_loss: 0.1793\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.1629 - val_accuracy: 0.9471 - val_loss: 0.1570\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9424 - loss: 0.1595 - val_accuracy: 0.9546 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9514 - loss: 0.1251 - val_accuracy: 0.9509 - val_loss: 0.1471\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.1225 - val_accuracy: 0.9546 - val_loss: 0.1322\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9428 - loss: 0.1436 - val_accuracy: 0.9565 - val_loss: 0.1249\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9612 - loss: 0.1117 - val_accuracy: 0.9622 - val_loss: 0.1135\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "<Sequential name=sequential, built=True> {'Accuracy': 0.962, 'Precision': 0.927, 'Recall': 0.894, 'F1': 0.91, 'MCC': 0.886, 'AUC_PR': 0.966, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.903}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci il modello FCNN\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Poiché si tratta di una classificazione binaria\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestra il modello\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "y_predicted_score = model.predict(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e90800",
   "metadata": {},
   "source": [
    "# Elaborazioni Dati OPS_SAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad5d28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\1746772083.py:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_final.append(y_trainS[i])\n",
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\1746772083.py:55: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (347, 1, 250)\n",
      "X_test shape: (130, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============ Valutazione Metriche ================\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "STEP = 250\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        y_trainS = dfSegment.loc[mask, \"anomaly\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            X_train_final.append(X_trainS[i:i + STEP])\n",
    "            y_train_final.append(y_trainS[i])\n",
    "        \n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "y_train = np.array(y_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed2dc",
   "metadata": {},
   "source": [
    "# Rockad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7c494",
   "metadata": {},
   "source": [
    "### 2° Prova un canale -> miglioramento predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8a5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_9824\\51312176.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (53, 1, 250)\n",
      "X_test shape: (15, 1, 250)\n",
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "End Train\n",
      "score_test:  (15,)\n",
      "[ 59.52569593  97.91287411 104.37438939 306.12021652 117.83811987\n",
      " 206.00498512  96.27331365 112.26971517  85.6070828  154.73770951\n",
      "  68.09927101  68.40500052  69.69060948  70.24381485  62.94952254]\n",
      "RISULTATI:  [0 0 1 1 1 0 0 1 1 1 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.733, 'Precision': 0.764, 'Recall': 0.733, 'F1': 0.736, 'MCC': 0.491, 'AUC_PR': 0.911, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.778}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, average_precision_score\n",
    "\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"F1\": f1_score(y_test, y_pred, average='weighted').round(digits),\n",
    "        \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba, average='weighted').round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "# print(X_train_final.shape)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_neighbors=5 , n_jobs=-1, n_estimators=10, n_kernels=10000, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "\n",
    "# print(\"mean_train\", mean_train)\n",
    "# print(\"std_train\", std_train)\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55225ada",
   "metadata": {},
   "source": [
    "#### NORMALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddb2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (53, 250, 1)\n",
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_6284\\2734640787.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "score_test:  (15,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.6, 'Recall': 1.0, 'F1': 0.75, 'MCC': 0.0, 'AUC_PR': 0.6, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "print(\"X_train:\", X_train.shape)\n",
    "\n",
    "# y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# # Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "# X_train_filtered, y_train_filtered = zip(*[\n",
    "#     (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "# ])\n",
    "# X_train_filtered = np.array(X_train_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=100, n_kernels=100, random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f115",
   "metadata": {},
   "source": [
    "### Più Canali e Miglioramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d81df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(347, 1, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_3636\\290305579.py:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  (130,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test: \u001b[39m\u001b[38;5;124m\"\u001b[39m,y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ======================= PRE-PROCESSING =============================\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[0;32m     78\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     79\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "print(X_train.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "# print(\"score_test: \", score_test.shape)\n",
    "# print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "print(\"RISULTATI: \", result_binary)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# Senza parametri con standard scalar: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.407, 'AUC_ROC': 0.297, 'PREC_N_SCORES': 0.29}\n",
    "# Senza standard scalar e parametri: {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.758, 'F1': 0.603, 'MCC': 0.075, 'AUC_PR': 0.753, 'AUC_ROC': 0.707, 'PREC_N_SCORES': 0.677}\n",
    "# con parametri: {'Accuracy': 0.546, 'Precision': 0.515, 'Recall': 0.823, 'F1': 0.634, 'MCC': 0.137, 'AUC_PR': 0.757, 'AUC_ROC': 0.704, 'PREC_N_SCORES': 0.677}\n",
    "# con standard scaler e parametri: {'Accuracy': 0.492, 'Precision': 0.476, 'Recall': 0.645, 'F1': 0.548, 'MCC': -0.002, 'AUC_PR': 0.407, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29} (10,10000)\n",
    "\n",
    "# scaler (20, 10000): {'Accuracy': 0.531, 'Precision': 0.506, 'Recall': 0.694, 'F1': 0.585, 'MCC': 0.08, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "# solo param: {'Accuracy': 0.454, 'Precision': 0.447, 'Recall': 0.613, 'F1': 0.517, 'MCC': -0.082, 'AUC_PR': 0.758, 'AUC_ROC': 0.705, 'PREC_N_SCORES': 0.677}\n",
    "# (30, 10000): {'Accuracy': 0.5, 'Precision': 0.481, 'Recall': 0.613, 'F1': 0.539, 'MCC': 0.01, 'AUC_PR': 0.406, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (35, 10000): {'Accuracy': 0.523, 'Precision': 0.5, 'Recall': 0.597, 'F1': 0.544, 'MCC': 0.053, 'AUC_PR': 0.407, 'AUC_ROC': 0.304, 'PREC_N_SCORES': 0.29}\n",
    "# (40, 10000): {'Accuracy': 0.454, 'Precision': 0.443, 'Recall': 0.565, 'F1': 0.496, 'MCC': -0.084, 'AUC_PR': 0.409, 'AUC_ROC': 0.305, 'PREC_N_SCORES': 0.29}\n",
    "\n",
    "# (10, 20000): \n",
    "# (10, 20000) + no StandardScalar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221baa2",
   "metadata": {},
   "source": [
    "## ROCKAD su NASA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11911284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: A-1\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 60. 24.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.236, 'AUC_PR': 0.75, 'AUC_ROC': 0.969, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-2\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\2294485134.py:98: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.\n",
      " 60.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.806, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.101, 'AUC_PR': 0.625, 'AUC_ROC': 0.897, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-3\n",
      "==== End Train ====\n",
      "Train:  (50, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 175.  10.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.25, 'Precision': 0.042, 'Recall': 0.5, 'F1': 0.077, 'MCC': -0.149, 'AUC_PR': 0.533, 'AUC_ROC': 0.533, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-4\n",
      "==== End Train ====\n",
      "Train:  (49, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 110.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.969, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.059, 'AUC_ROC': 0.484, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (10, 250, 25)\n",
      "Test:  (18, 250, 25)\n",
      "output:  (18, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.944, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (9, 250, 25)\n",
      "Test:  (17, 250, 25)\n",
      "output:  (17, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-7\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  50. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.9, 'F1': 0.947, 'MCC': 0.93, 'AUC_PR': 0.187, 'AUC_ROC': 0.071, 'PREC_N_SCORES': 0.1}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-8\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.576, 'Precision': 0.6, 'Recall': 0.2, 'F1': 0.3, 'MCC': 0.123, 'AUC_PR': 0.47, 'AUC_ROC': 0.519, 'PREC_N_SCORES': 0.6}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-9\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.455, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.289, 'AUC_PR': 0.684, 'AUC_ROC': 0.822, 'PREC_N_SCORES': 0.733}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: B-1\n",
      "==== End Train ====\n",
      "Train:  (44, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. 70.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.438, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.191, 'AUC_PR': 0.083, 'AUC_ROC': 0.645, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (39, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0. 200.   0.   0.   0.   0.   0. 110.]\n",
      "output:  [0 0 1 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.5, 'Recall': 0.5, 'F1': 0.5, 'MCC': 0.357, 'AUC_PR': 0.361, 'AUC_ROC': 0.429, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-2\n",
      "==== End Train ====\n",
      "Train:  (11, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0. 100.   0.   0.   0.   0.  35.   0.]\n",
      "output:  [0 1 0 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.643, 'AUC_ROC': 0.583, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (52, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.559, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.197, 'AUC_PR': 0.956, 'AUC_ROC': 0.938, 'PREC_N_SCORES': 0.923}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-11\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 60.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.448, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.196, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-12\n",
      "==== End Train ====\n",
      "Train:  (2, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  72. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.129, 'Precision': 0.1, 'Recall': 0.182, 'F1': 0.129, 'MCC': -0.718, 'AUC_PR': 0.828, 'AUC_ROC': 0.873, 'PREC_N_SCORES': 0.818}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-13\n",
      "==== End Train ====\n",
      "Train:  (25, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 160.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.467, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.186, 'AUC_PR': 0.1, 'AUC_ROC': 0.69, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (69, 250, 55)\n",
      "Test:  (10, 250, 55)\n",
      "output:  (10, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.  20. 200.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 1 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.8, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-15\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (37, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.583, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-16\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (25, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0. 150. 250. 250.   0.   0.   0.]\n",
      "output:  [0 0 1 1 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.625, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.383, 'AUC_ROC': 0.4, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-2\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0. 181. 250. 250. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.147, 'Precision': 0.071, 'Recall': 0.059, 'F1': 0.065, 'MCC': -0.717, 'AUC_PR': 0.983, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.941}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  25. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.167, 'Recall': 0.071, 'F1': 0.1, 'MCC': -0.231, 'AUC_PR': 0.849, 'AUC_ROC': 0.825, 'PREC_N_SCORES': 0.714}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-4\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  25. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.299, 'AUC_PR': 0.891, 'AUC_ROC': 0.912, 'PREC_N_SCORES': 0.769}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-5\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0. 50.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.967, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.045, 'AUC_ROC': 0.276, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (47, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0. 80.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.419, 'Precision': 0.053, 'Recall': 1.0, 'F1': 0.1, 'MCC': 0.145, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-7\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  60. 250. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.167, 'Precision': 0.208, 'Recall': 0.455, 'F1': 0.286, 'MCC': -0.657, 'AUC_PR': 0.853, 'AUC_ROC': 0.861, 'PREC_N_SCORES': 0.727}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-8\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.065, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.558, 'AUC_PR': 0.333, 'AUC_ROC': 0.933, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-9\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 250. 250. 250.\n",
      " 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.069, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.783, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-1\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  30.   0. 140. 250.  86.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.091, 'AUC_PR': 0.643, 'AUC_ROC': 0.767, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-10\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 149. 121.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.078, 'AUC_PR': 0.705, 'AUC_ROC': 0.753, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-11\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 136. 107.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.078, 'AUC_PR': 0.701, 'AUC_ROC': 0.72, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-12\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.  50.   0. 140. 250. 141.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.765, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.133, 'AUC_PR': 0.229, 'AUC_ROC': 0.517, 'PREC_N_SCORES': 0.25}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-13\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 101.  40.   0.   0.  51.  69.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.559, 'Precision': 0.077, 'Recall': 0.25, 'F1': 0.118, 'MCC': -0.099, 'AUC_PR': 0.177, 'AUC_ROC': 0.542, 'PREC_N_SCORES': 0.25}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 152. 250. 250. 250. 250. 245.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.595, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-3\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0. 156. 250. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.485, 'Precision': 0.3, 'Recall': 0.231, 'F1': 0.261, 'MCC': -0.127, 'AUC_PR': 0.766, 'AUC_ROC': 0.742, 'PREC_N_SCORES': 0.615}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-4\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  50. 250. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.636, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.277, 'AUC_ROC': 0.274, 'PREC_N_SCORES': 0.083}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 150. 170.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.879, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.065, 'AUC_PR': 0.167, 'AUC_ROC': 0.726, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-6\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0. 65.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.031, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-7\n",
      "==== End Train ====\n",
      "Train:  (51, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 106. 174.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-8\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 100. 250. 250.  22.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.091, 'AUC_PR': 0.589, 'AUC_ROC': 0.725, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-9\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 200. 150.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.361, 'AUC_ROC': 0.871, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.03, 'AUC_PR': 0.111, 'AUC_ROC': 0.758, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.  81. 250. 250. 250. 250. 250.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.471, 'Precision': 0.35, 'Recall': 0.583, 'F1': 0.438, 'MCC': -0.007, 'AUC_PR': 0.388, 'AUC_ROC': 0.439, 'PREC_N_SCORES': 0.417}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0. 40.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.212, 'Precision': 0.037, 'Recall': 1.0, 'F1': 0.071, 'MCC': 0.083, 'AUC_PR': 0.125, 'AUC_ROC': 0.781, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-4\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (13, 250, 55)\n",
      "output:  (13, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50. 20.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.462, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.337, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-5\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 55)\n",
      "Test:  (15, 250, 55)\n",
      "output:  (15, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 150.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.189, 'AUC_PR': 0.333, 'AUC_ROC': 0.857, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-7\n",
      "==== End Train ====\n",
      "Train:  (46, 250, 55)\n",
      "Test:  (20, 250, 55)\n",
      "output:  (20, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 200.   0.   0.   0.   0.  80.  40.   0. 100.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.327, 'AUC_PR': 0.888, 'AUC_ROC': 0.969, 'PREC_N_SCORES': 0.75}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-8\n",
      "==== End Train ====\n",
      "Train:  (62, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.  50. 250.]\n",
      "output:  [0 0 0 0 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.333, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.478, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-1\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0. 120.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.061, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.559, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-2\n",
      "==== End Train ====\n",
      "Train:  (45, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 40.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.034, 'Precision': 0.034, 'Recall': 1.0, 'F1': 0.067, 'MCC': 0.0, 'AUC_PR': 0.125, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-3\n",
      "==== End Train ====\n",
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 50.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.645, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.126, 'AUC_PR': 0.059, 'AUC_ROC': 0.467, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-4\n",
      "==== End Train ====\n",
      "Train:  (47, 250, 25)\n",
      "Test:  (30, 250, 25)\n",
      "output:  (30, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 30.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.0, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-6\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 100.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.206, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.314, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-7\n",
      "==== End Train ====\n",
      "Train:  (44, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 100.   0.   0.   0.   0.   0.  50.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0. 115.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.594, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.217, 'AUC_PR': 0.867, 'AUC_ROC': 0.977, 'PREC_N_SCORES': 0.667}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-1\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0. 140. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.75, 'Recall': 0.6, 'F1': 0.667, 'MCC': 0.35, 'AUC_PR': 0.619, 'AUC_ROC': 0.4, 'PREC_N_SCORES': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-2\n",
      "==== End Train ====\n",
      "Train:  (40, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0. 140. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.967, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.8}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-3\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (36, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.143, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-4\n",
      "==== End Train ====\n",
      "Train:  (37, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (36, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.   0.   0.   0. 250.  50.   0.   0.]\n",
      "output:  [0 0 0 0 0 1 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.375, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-6\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (27, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0. 150.]\n",
      "output:  [0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-7\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (27, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0. 60. 40.  0.  0.  0.]\n",
      "output:  [0 0 0 1 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.833, 'AUC_ROC': 0.917, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0. 101.  99.   0.   0.   0.   0.\n",
      " 211.  29.   0.   0. 214.  94.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.824, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.123, 'AUC_ROC': 0.173, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (82, 250, 55)\n",
      "Test:  (24, 250, 55)\n",
      "output:  (24, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 130.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.958, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.167, 'AUC_ROC': 0.783, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== End Train ====\n",
      "Train:  (75, 250, 55)\n",
      "Test:  (14, 250, 55)\n",
      "output:  (14, 250)\n",
      "counts:  [  0.   0.   0.   0.  12.  94.   0. 120.   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.643, 'Precision': 0.25, 'Recall': 0.333, 'F1': 0.286, 'MCC': 0.055, 'AUC_PR': 0.369, 'AUC_ROC': 0.606, 'PREC_N_SCORES': 0.333}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-14\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 55)\n",
      "Test:  (24, 250, 55)\n",
      "output:  (24, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0. 175.   5.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.792, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.114, 'AUC_PR': 0.267, 'AUC_ROC': 0.818, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-15\n",
      "==== End Train ====\n",
      "Train:  (69, 250, 55)\n",
      "Test:  (11, 250, 55)\n",
      "output:  (11, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0. 20.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.727, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.149, 'AUC_PR': 0.167, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-2\n",
      "==== End Train ====\n",
      "Train:  (52, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 150. 250. 250. 250. 250.  75.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.812, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.868, 'AUC_ROC': 0.853, 'PREC_N_SCORES': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-3\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (33, 250, 25)\n",
      "output:  (33, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.  99. 250. 250. 250. 250. 236.   0.\n",
      "   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.818, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.931, 'AUC_ROC': 0.981, 'PREC_N_SCORES': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-4\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (48, 250, 25)\n",
      "Test:  (31, 250, 25)\n",
      "output:  (31, 250)\n",
      "counts:  [  0.   0.   0.  50.  80.   0.   0.   0. 100. 100.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0. 110.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.]\n",
      "output:  [0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 1.0, 'Recall': 0.2, 'F1': 0.333, 'MCC': 0.416, 'AUC_PR': 0.647, 'AUC_ROC': 0.754, 'PREC_N_SCORES': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-7\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (32, 250, 25)\n",
      "output:  (32, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  50. 250. 250. 250. 250. 250. 250. 100.   0.\n",
      "   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.646, 'AUC_ROC': 0.714, 'PREC_N_SCORES': 0.625}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: R-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (28, 250, 25)\n",
      "output:  (28, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      " 80.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.964, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.926, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (52, 250, 25)\n",
      "Test:  (29, 250, 25)\n",
      "output:  (29, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0. 200. 247.   0.   0.   0.   0.   0.\n",
      "   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.276, 'Precision': 0.087, 'Recall': 1.0, 'F1': 0.16, 'MCC': 0.139, 'AUC_PR': 0.067, 'AUC_ROC': 0.241, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-2\n",
      "==== End Train ====\n",
      "Train:  (14, 250, 55)\n",
      "Test:  (7, 250, 55)\n",
      "output:  (7, 250)\n",
      "counts:  [ 0.  0.  0. 10.  0.  0.  0.]\n",
      "output:  [0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.857, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.833, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-1\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0. 101. 250. 250. 250. 250.\n",
      " 250. 148.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.647, 'Precision': 0.375, 'Recall': 0.75, 'F1': 0.5, 'MCC': 0.311, 'AUC_PR': 0.292, 'AUC_ROC': 0.322, 'PREC_N_SCORES': 0.125}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-12\n",
      "==== End Train ====\n",
      "Train:  (18, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0. 120.   0.   0.   0.   0.   0.   0.]\n",
      "output:  [0 0 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': -0.316, 'AUC_PR': 0.25, 'AUC_ROC': 0.625, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-13\n",
      "==== End Train ====\n",
      "Train:  (18, 250, 55)\n",
      "Test:  (9, 250, 55)\n",
      "output:  (9, 250)\n",
      "counts:  [  0.   0.  60.  40.   0.   0.   0. 100.  50.]\n",
      "output:  [0 0 1 1 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.444, 'Recall': 1.0, 'F1': 0.615, 'MCC': 0.0, 'AUC_PR': 0.635, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-2\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 160.\n",
      " 250. 250. 250. 250. 250. 250.]\n",
      "output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.143, 'Recall': 0.286, 'F1': 0.19, 'MCC': -0.13, 'AUC_PR': 0.383, 'AUC_ROC': 0.608, 'PREC_N_SCORES': 0.286}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-3\n",
      "==== End Train ====\n",
      "Train:  (53, 250, 25)\n",
      "Test:  (34, 250, 25)\n",
      "output:  (34, 250)\n",
      "counts:  [ 0.  0.  0.  0.  0.  0.  0.  0. 82.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0. 50. 50.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "output:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.971, 'Precision': 1.0, 'Recall': 0.667, 'F1': 0.8, 'MCC': 0.804, 'AUC_PR': 0.31, 'AUC_ROC': 0.656, 'PREC_N_SCORES': 0.333}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-4\n",
      "==== End Train ====\n",
      "Train:  (41, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0.  0. 68.  0.  0.  0.]\n",
      "output:  [0 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.143, 'PREC_N_SCORES': 0.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-5\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (41, 250, 55)\n",
      "Test:  (8, 250, 55)\n",
      "output:  (8, 250)\n",
      "counts:  [ 0.  0.  0.  0. 25.  0.  0.  0.]\n",
      "output:  [0 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-8\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (10, 250, 55)\n",
      "Test:  (6, 250, 55)\n",
      "output:  (6, 250)\n",
      "counts:  [ 0.  0.  0. 60.  0. 40.]\n",
      "output:  [0 0 0 1 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.583, 'AUC_ROC': 0.75, 'PREC_N_SCORES': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-9\n",
      "==== End Train ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (4, 250, 55)\n",
      "Test:  (4, 250, 55)\n",
      "output:  (4, 250)\n",
      "counts:  [  0.   0.   0. 110.]\n",
      "output:  [0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0, 'PREC_N_SCORES': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Risultati salvati in risultatiNASA_ROCKAD.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\CodiceTesi_Sync\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OFFSET = 50\n",
    "OUTPUT_FILE = \"risultatiNASA_ROCKAD.csv\"\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "# Itera su tutti i canali del dataset\n",
    "for channel_id in NASA.channel_ids:\n",
    "    # if channel_id == \"D-12\" or channel_id == \"T-10\" or channel_id == \"T-9\":   # Non uso questi perchè NeirestNeigthbor dato che necessita di avere più \n",
    "    #     continue\n",
    "    if channel_id == \"T-10\":\n",
    "        continue\n",
    "    print(f\"Processing channel: {channel_id}\")\n",
    "\n",
    "    # Lista per memorizzare i segmenti di training\n",
    "    X_train_final = []\n",
    "\n",
    "    # Uso del dataset NASA per tutti i canali\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\")\n",
    "    # print(dataset.data.shape)\n",
    "    data = dataset.data\n",
    "    train = []\n",
    "    for i in range(0, data.shape[0] - STEP +1, OFFSET): \n",
    "        train.append(data[i:i+STEP])\n",
    "\n",
    "    train = np.stack(train)\n",
    "    # print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "    # Inizializza e addestra il modello ROCKAD\n",
    "    rockad = ROCKAD(n_neighbors=1, n_estimators=10, n_kernels=1000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "    rockad.fit(train)\n",
    "    print(\"==== End Train ====\")\n",
    "\n",
    "    # Predict anomaly scores\n",
    "    score_train = rockad.predict_proba(train)\n",
    "    # print(\"Score:\", scores)\n",
    "\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\", train=False)\n",
    "    data = dataset.data\n",
    "    Test = []\n",
    "    output = []\n",
    "    o = np.zeros(data.shape[0])\n",
    "    for start,end in dataset.anomalies:\n",
    "        o[start:end] = 1\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        Test.append(data[i:i+STEP])\n",
    "        output.append(o[i:i+STEP])\n",
    "\n",
    "    output = np.stack(output)\n",
    "    Test = np.stack(Test)\n",
    "    # print(\"TEST: \", Test.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "    decision_func = NearestNeighborOCC().fit(score_train)\n",
    "    \n",
    "    print(\"Train: \", train.shape)\n",
    "    print(\"Test: \", Test.shape)\n",
    "    print(\"output: \", output.shape)\n",
    "    score_test = rockad.predict_proba(Test)\n",
    "    # print(\"score_test: \", score_test.shape)\n",
    "    # print(score_test)\n",
    "\n",
    "    result = decision_func.predict(score_test)\n",
    "    result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "\n",
    "    # Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "    threshold = 1 # -> 10%\n",
    "    # Conta il numero di 1 in ogni lista\n",
    "    counts = np.sum(output, axis=1)\n",
    "    output = np.where(counts >= threshold, 1, 0)\n",
    "    print(\"counts: \", counts)\n",
    "    print(\"output: \", output)\n",
    "    metrics = evaluate_metrics(output, result_binary, score_test)\n",
    "    print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "        # Calcolo di VP, VN, FP, FN\n",
    "    TP = ((output == 1) & (result_binary == 1)).sum()\n",
    "    TN = ((output == 0) & (result_binary == 0)).sum()\n",
    "    FP = ((output == 0) & (result_binary == 1)).sum()\n",
    "    FN = ((output == 1) & (result_binary == 0)).sum()\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        \"Channel\": channel_id,\n",
    "        \"Accuracy\": metrics.get(\"Accuracy\", 0),\n",
    "        \"Precision\": metrics.get(\"Precision\", 0),\n",
    "        \"Recall\": metrics.get(\"Recall\", 0),\n",
    "        \"MCC\": metrics.get(\"MCC\", 0),\n",
    "        \"AUC_PR\": metrics.get(\"AUC_PR\", 0),\n",
    "        \"AUC_ROC\": metrics.get(\"AUC_ROC\", 0),\n",
    "        \"F1\": metrics.get(\"F1\", 0),\n",
    "        \"TP\": TP,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    print(\"=========================FINE CHANNEL=============================\")\n",
    "# ======================= SALVATAGGIO RISULTATI =============================\n",
    "results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Risultati salvati in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77704e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie delle colonne numeriche:\n",
      "Accuracy      0.616765\n",
      "Precision     0.093679\n",
      "Recall        0.145272\n",
      "F1            0.086543\n",
      "MCC          -0.099062\n",
      "AUC_ROC       0.745765\n",
      "AUC_PR        0.589877\n",
      "TP            0.716049\n",
      "TN           14.123457\n",
      "FP            6.555556\n",
      "FN            3.320988\n",
      "dtype: float64\n",
      "========= CALCOLI CON TP...\n",
      "Accuracy: 0.6003996003996004\n",
      "Precision: 0.09847198641765705\n",
      "Recall: 0.17737003058103976\n",
      "F1 Score: 0.12663755458515283\n",
      "MCC: -0.13373121449680916\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import math\n",
    "file_path = \"risultatiNASA_ROCKAD.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calcola la media delle colonne numeriche\n",
    "column_means = df.mean(numeric_only=True)\n",
    "\n",
    "# Stampa le medie\n",
    "print(\"Medie delle colonne numeriche:\")\n",
    "print(column_means)\n",
    "\n",
    "print(\"========= CALCOLI CON TP...\")\n",
    "df = pd.read_csv(file_path)\n",
    "TP = df[\"TP\"].sum()\n",
    "TN = df[\"TN\"].sum()\n",
    "FP = df[\"FP\"].sum()\n",
    "FN = df[\"FN\"].sum()\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "mcc = (TP*TN-FP*FN)/(math.sqrt((TP + FP)*(TP+FN)*(TN+TP)*(TN+FN)))\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"MCC:\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a14d98",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3a9d2",
   "metadata": {},
   "source": [
    "# ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafd0adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train:  (347, 20000)\n",
      "features_test:  (130, 20000)\n",
      "(array([0, 1]), array([127,   3], dtype=int64))\n",
      "Anomalie rilevate nel training set: 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "342    1\n",
      "343    0\n",
      "344    0\n",
      "345    0\n",
      "346    0\n",
      "Length: 347, dtype: int32\n",
      "Anomalie rilevate nel test set: 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "125    0\n",
      "126    0\n",
      "127    0\n",
      "128    0\n",
      "129    0\n",
      "Length: 130, dtype: int32\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.546, 'Precision': 1.0, 'Recall': 0.048, 'F1': 0.092, 'MCC': 0.161}\n"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "import numpy as np\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "print(\"features_train: \", features_train.shape)\n",
    "print(\"features_test: \", features_test.shape)\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "print(np.unique(anomaly_labels_test, return_counts=True))\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)\n",
    "# {'Accuracy': 0.546, 'Precision': 1.0, 'Recall': 0.048, 'F1': 0.092, 'MCC': 0.161}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036570a",
   "metadata": {},
   "source": [
    "### KNN ( UNSUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4756a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: (130,)\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.531, 'Precision': 0.6, 'Recall': 0.048, 'F1': 0.09, 'MCC': 0.049, 'AUC_PR': 0.407, 'AUC_ROC': 0.294, 'PREC_N_SCORES': 0.29}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "num_kernels = 10000\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred.shape)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.531, 'Precision': 0.6, 'Recall': 0.048, 'F1': 0.09, 'MCC': 0.049, 'AUC_PR': 0.407, 'AUC_ROC': 0.294, 'PREC_N_SCORES': 0.29}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4c45a",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie ROCKET SUPERVISED\n",
    "Utilizzo di vari algoritmi unsupervised e non con kernel ROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eed09",
   "metadata": {},
   "source": [
    "### Regressione Logistica -> Classificatore lineare ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2b43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.777, 'Precision': 0.704, 'Recall': 0.919, 'F1': 0.797, 'MCC': 0.584, 'AUC_PR': 0.773, 'AUC_ROC': 0.837, 'PREC_N_SCORES': 0.774}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "num_kernels = 10000\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539711",
   "metadata": {},
   "source": [
    "### Prova con Dettagli dal GitHub del Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ee46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.538, 'Precision': 1.0, 'Recall': 0.032, 'F1': 0.062, 'MCC': 0.131, 'AUC_PR': 0.779, 'AUC_ROC': 0.844, 'PREC_N_SCORES': 0.758}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "num_kernels = 10000\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "anomaly_scores_test = model.predict(features_test)\n",
    "anomaly_scores_train = model.predict(features_train)\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "#  {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501c69b",
   "metadata": {},
   "source": [
    "## LogisticClassifierCV ( SUPERVISED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train:          0         1      2         3      4         5      6         7      \\\n",
      "0    0.000000 -0.506472  0.116  3.032165  0.264  1.902368  0.605  2.077979   \n",
      "1    0.000000 -0.506838  0.116  3.042692  0.264  1.903324  0.605  2.079800   \n",
      "2    0.000000 -0.505849  0.116  3.041053  0.264  1.904561  0.605  2.081079   \n",
      "3    0.000000 -0.506395  0.116  3.044194  0.264  1.903279  0.605  2.078549   \n",
      "4    0.000000 -0.506292  0.116  3.041560  0.264  1.905488  0.605  2.078733   \n",
      "..        ...       ...    ...       ...    ...       ...    ...       ...   \n",
      "342  0.070833  1.137780  0.192  5.092749  0.384  9.027847  1.000  5.501685   \n",
      "343  0.004167  0.001192  0.480  1.313994  0.432  4.937717  0.700  4.773400   \n",
      "344  0.000000 -0.817197  0.060  3.949451  0.220  1.828851  0.580  0.880673   \n",
      "345  0.016667  0.545645  0.236  4.010215  0.436  8.576636  1.000  4.302028   \n",
      "346  0.000000 -0.620669  0.060  2.056262  0.236  1.762436  0.640  1.574257   \n",
      "\n",
      "        8         9      ...  19990     19991  19992     19993     19994  \\\n",
      "0    1.000000  1.621546  ...  0.244  2.003530  0.020  3.189048  0.881443   \n",
      "1    1.000000  1.623431  ...  0.244  2.012137  0.020  3.190195  0.881443   \n",
      "2    1.000000  1.625300  ...  0.244  2.014681  0.020  3.188392  0.881443   \n",
      "3    1.000000  1.622380  ...  0.244  2.012524  0.020  3.192134  0.881443   \n",
      "4    1.000000  1.624734  ...  0.244  2.013606  0.020  3.188865  0.881443   \n",
      "..        ...       ...  ...    ...       ...    ...       ...       ...   \n",
      "342  0.655844  1.046760  ...  0.280  4.706920  0.128  2.594887  0.927835   \n",
      "343  0.610390  3.304975  ...  0.484  2.498261  0.100  0.485302  0.577320   \n",
      "344  1.000000  1.436868  ...  0.228  2.301463  0.024  4.460943  1.000000   \n",
      "345  0.649351  0.929269  ...  0.228  4.241814  0.044  2.045030  0.850515   \n",
      "346  1.000000  1.944064  ...  0.412  1.943669  0.024  1.881685  0.809278   \n",
      "\n",
      "        19995     19996     19997  19998     19999  \n",
      "0    2.272753  0.921739  0.484067  0.296  0.370013  \n",
      "1    2.273638  0.921739  0.484335  0.296  0.370939  \n",
      "2    2.275183  0.921739  0.484489  0.296  0.372374  \n",
      "3    2.273391  0.921739  0.484115  0.296  0.370807  \n",
      "4    2.277514  0.921739  0.484557  0.296  0.371457  \n",
      "..        ...       ...       ...    ...       ...  \n",
      "342  3.092981  0.800000  0.952038  0.384  1.502883  \n",
      "343  2.945722  0.482609  0.580023  0.260  1.249424  \n",
      "344  1.972339  1.000000  0.496193  0.120  0.671088  \n",
      "345  2.380123  0.743478  0.676771  0.304  1.106408  \n",
      "346  1.908112  0.982609  0.414046  0.112  0.438568  \n",
      "\n",
      "[347 rows x 20000 columns]\n",
      "Predizioni nel test set: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.769, 'Precision': 0.695, 'Recall': 0.919, 'F1': 0.792, 'MCC': 0.571, 'AUC_PR': 0.79, 'AUC_ROC': 0.828, 'PREC_N_SCORES': 0.726}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from scipy.special import softmax\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = rocket_transformer.fit_transform(X_train)\n",
    "features_test = rocket_transformer.transform(X_test)\n",
    "print(\"features_train: \", features_train)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = RidgeClassifierCV(alphas = np.logspace(-3, 3, 10))\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "# Per separare multiclasse o monoclasse\n",
    "if  len(np.unique(y_test)) > 2:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=1)\n",
    "else:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=0)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1688a488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train:  (53, 20000)\n",
      "y_train:  (0,)\n"
     ]
    }
   ],
   "source": [
    "print(\"features_train: \", features_train.shape)\n",
    "print(\"y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522d8c",
   "metadata": {},
   "source": [
    "# Test Rocket su NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbedd15",
   "metadata": {},
   "source": [
    "### ROCKET con NASA -> Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a85f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing channel: A-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.735, 'Precision': 0.1, 'Recall': 1.0, 'F1': 0.182, 'AUC_PR': 0.333, 'AUC_ROC': 0.939}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\1949300218.py:115: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.161, 'Precision': 0.071, 'Recall': 1.0, 'F1': 0.133, 'AUC_PR': 0.571, 'AUC_ROC': 0.793}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.938, 'Precision': 0.333, 'Recall': 1.0, 'F1': 0.5, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-4\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.969, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.968}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-5\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.143, 'Recall': 1.0, 'F1': 0.25, 'AUC_PR': 0.143, 'AUC_ROC': 0.647}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-6\n",
      "Output:  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1': 1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-7\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.059, 'Precision': 0.042, 'Recall': 0.1, 'F1': 0.059, 'AUC_PR': 0.18, 'AUC_ROC': 0.021}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-8\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.545, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.464, 'AUC_ROC': 0.504}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: A-9\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.515, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.713, 'AUC_ROC': 0.852}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: B-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.594, 'Precision': 0.071, 'Recall': 1.0, 'F1': 0.133, 'AUC_PR': 0.083, 'AUC_ROC': 0.645}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-1\n",
      "Output:  [0 0 1 0 0 0 0 0 1]\n",
      "predicted_anomalies:  [0 0 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.5, 'Recall': 0.5, 'F1': 0.5, 'AUC_PR': 0.375, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: C-2\n",
      "Output:  [0 1 0 0 0 0 1 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.25, 'Precision': 0.25, 'Recall': 1.0, 'F1': 0.4, 'AUC_PR': 0.643, 'AUC_ROC': 0.583}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.853, 'Precision': 0.75, 'Recall': 0.923, 'F1': 0.828, 'AUC_PR': 0.959, 'AUC_ROC': 0.945}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-11\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.966, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-12\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 0.818, 'Recall': 0.818, 'F1': 0.818, 'AUC_PR': 0.828, 'AUC_ROC': 0.873}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-13\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.533, 'Precision': 0.067, 'Recall': 1.0, 'F1': 0.125, 'AUC_PR': 0.1, 'AUC_ROC': 0.69}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-14\n",
      "Output:  [0 0 0 0 0 0 0 1 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.2, 'Precision': 0.111, 'Recall': 1.0, 'F1': 0.2, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-15\n",
      "Output:  [0 0 0 0 0 0 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.75, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.75}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-16\n",
      "Output:  [0 0 1 1 1 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.625, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.383, 'AUC_ROC': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-2\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.941, 'Recall': 0.941, 'F1': 0.941, 'AUC_PR': 0.973, 'AUC_ROC': 0.952}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 0.579, 'Recall': 0.786, 'F1': 0.667, 'AUC_PR': 0.89, 'AUC_ROC': 0.861}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-4\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.818, 'Precision': 0.769, 'Recall': 0.769, 'F1': 0.769, 'AUC_PR': 0.907, 'AUC_ROC': 0.919}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-5\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.033, 'Precision': 0.033, 'Recall': 1.0, 'F1': 0.065, 'AUC_PR': 0.043, 'AUC_ROC': 0.241}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-6\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.968, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.25, 'AUC_ROC': 0.9}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-7\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.833, 'Precision': 0.875, 'Recall': 0.636, 'F1': 0.737, 'AUC_PR': 0.85, 'AUC_ROC': 0.866}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-8\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.903, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.933}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: D-9\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      "predicted_anomalies:  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.966, 'Precision': 0.8, 'Recall': 1.0, 'F1': 0.889, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.706, 'Precision': 0.25, 'Recall': 0.75, 'F1': 0.375, 'AUC_PR': 0.647, 'AUC_ROC': 0.792}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-10\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.765, 'Precision': 0.222, 'Recall': 0.667, 'F1': 0.333, 'AUC_PR': 0.714, 'AUC_ROC': 0.806}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-11\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.676, 'Precision': 0.167, 'Recall': 0.667, 'F1': 0.267, 'AUC_PR': 0.708, 'AUC_ROC': 0.774}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-12\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.618, 'Precision': 0.154, 'Recall': 0.5, 'F1': 0.235, 'AUC_PR': 0.186, 'AUC_ROC': 0.625}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-13\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.529, 'Precision': 0.125, 'Recall': 0.5, 'F1': 0.2, 'AUC_PR': 0.164, 'AUC_ROC': 0.492}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-2\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.794, 'Precision': 0.429, 'Recall': 0.5, 'F1': 0.462, 'AUC_PR': 0.596, 'AUC_ROC': 0.786}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.758, 'Precision': 1.0, 'Recall': 0.385, 'F1': 0.556, 'AUC_PR': 0.841, 'AUC_ROC': 0.865}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-4\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.424, 'Precision': 0.316, 'Recall': 0.5, 'F1': 0.387, 'AUC_PR': 0.367, 'AUC_ROC': 0.365}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-5\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.909, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.292, 'AUC_ROC': 0.871}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-6\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1': 1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-7\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-8\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.667, 'Recall': 0.667, 'F1': 0.667, 'AUC_PR': 0.722, 'AUC_ROC': 0.839}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: E-9\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.788, 'Precision': 0.143, 'Recall': 0.5, 'F1': 0.222, 'AUC_PR': 0.16, 'AUC_ROC': 0.742}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.412, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.034, 'AUC_ROC': 0.152}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-2\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.647, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.34, 'AUC_ROC': 0.394}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.939, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.812}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-4\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 1 0 0 0 0 1 1 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.769, 'Precision': 0.25, 'Recall': 1.0, 'F1': 0.4, 'AUC_PR': 0.333, 'AUC_ROC': 0.833}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-5\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "predicted_anomalies:  [0 0 1 0 0 0 0 0 0 1 1 0 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.167, 'Recall': 1.0, 'F1': 0.286, 'AUC_PR': 0.5, 'AUC_ROC': 0.929}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-7\n",
      "Output:  [0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.9, 'Precision': 0.667, 'Recall': 1.0, 'F1': 0.8, 'AUC_PR': 0.854, 'AUC_ROC': 0.953}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: F-8\n",
      "Output:  [0 0 0 0 0 0 0 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.889, 'Precision': 1.0, 'Recall': 0.5, 'F1': 0.667, 'AUC_PR': 0.833, 'AUC_ROC': 0.929}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.97, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-2\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.897, 'Precision': 0.25, 'Recall': 1.0, 'F1': 0.4, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.968, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.056, 'AUC_ROC': 0.433}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-4\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.967, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-6\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.941, 'Precision': 0.333, 'Recall': 1.0, 'F1': 0.5, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: G-7\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.429, 'Recall': 1.0, 'F1': 0.6, 'AUC_PR': 0.867, 'AUC_ROC': 0.977}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-1\n",
      "Output:  [0 0 0 0 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.444, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.619, 'AUC_ROC': 0.4}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-2\n",
      "Output:  [0 0 0 0 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 1 1 0 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.714, 'Recall': 1.0, 'F1': 0.833, 'AUC_PR': 0.943, 'AUC_ROC': 0.9}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-3\n",
      "Output:  [0 0 0 0 0 1 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.5, 'AUC_ROC': 0.857}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-4\n",
      "Output:  [0 0 0 0 0 1 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 1 1 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.5, 'Recall': 1.0, 'F1': 0.667, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-5\n",
      "Output:  [0 0 0 0 0 1 1 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.778, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.643, 'AUC_ROC': 0.643}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-6\n",
      "Output:  [0 0 0 0 0 0 0 1]\n",
      "predicted_anomalies:  [0 0 0 0 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.625, 'Precision': 0.25, 'Recall': 1.0, 'F1': 0.4, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: M-7\n",
      "Output:  [0 0 0 1 1 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 1 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 1.0, 'Recall': 0.5, 'F1': 0.667, 'AUC_PR': 0.833, 'AUC_ROC': 0.917}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-1\n",
      "Output:  [0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.735, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.292}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-10\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.042, 'Precision': 0.042, 'Recall': 1.0, 'F1': 0.08, 'AUC_PR': 0.167, 'AUC_ROC': 0.783}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-11\n",
      "Output:  [0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.857, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.216, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-14\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.458, 'Precision': 0.071, 'Recall': 1.0, 'F1': 0.133, 'AUC_PR': 0.167, 'AUC_ROC': 0.783}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-15\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.455, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': None, 'AUC_ROC': None}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\1949300218.py:115: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.938, 'Precision': 0.833, 'Recall': 0.833, 'F1': 0.833, 'AUC_PR': 0.866, 'AUC_ROC': 0.84}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-3\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.182, 'Precision': 0.182, 'Recall': 1.0, 'F1': 0.308, 'AUC_PR': 0.958, 'AUC_ROC': 0.988}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-4\n",
      "Output:  [0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.871, 'Precision': 0.667, 'Recall': 0.4, 'F1': 0.5, 'AUC_PR': 0.673, 'AUC_ROC': 0.923}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: P-7\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 1 1 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.5, 'Precision': 0.333, 'Recall': 1.0, 'F1': 0.5, 'AUC_PR': 0.707, 'AUC_ROC': 0.844}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: R-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.929, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.333, 'AUC_ROC': 0.926}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-1\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.759, 'Precision': 0.143, 'Recall': 0.5, 'F1': 0.222, 'AUC_PR': 0.136, 'AUC_ROC': 0.444}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: S-2\n",
      "Output:  [0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.714, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': None, 'AUC_ROC': None}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_28516\\1949300218.py:115: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  [0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.735, 'Precision': 0.333, 'Recall': 0.125, 'F1': 0.182, 'AUC_PR': 0.331, 'AUC_ROC': 0.481}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-12\n",
      "Output:  [0 0 1 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 1 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.25, 'AUC_ROC': 0.625}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-13\n",
      "Output:  [0 0 1 1 0 0 0 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.556, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.51, 'AUC_ROC': 0.45}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-2\n",
      "Output:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.794, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.164, 'AUC_ROC': 0.307}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-3\n",
      "Output:  [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.912, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.31, 'AUC_ROC': 0.656}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-4\n",
      "Output:  [0 0 0 0 1 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.875, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'AUC_PR': 0.143, 'AUC_ROC': 0.143}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-5\n",
      "Output:  [0 0 0 0 1 0 0 0]\n",
      "predicted_anomalies:  [0 0 0 0 1 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1': 1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-8\n",
      "Output:  [0 0 0 1 0 1]\n",
      "predicted_anomalies:  [1 0 0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.667, 'Precision': 0.5, 'Recall': 0.5, 'F1': 0.5, 'AUC_PR': 0.45, 'AUC_ROC': 0.5}\n",
      "=========================FINE CHANNEL=============================\n",
      "Processing channel: T-9\n",
      "Output:  [0 0 0 1]\n",
      "predicted_anomalies:  [0 0 0 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1': 1.0, 'AUC_PR': 1.0, 'AUC_ROC': 1.0}\n",
      "=========================FINE CHANNEL=============================\n",
      "Risultati salvati in risultatiNASA_ROCKET.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "OFFSET = 20\n",
    "OUTPUT_FILE = \"risultatiNASA_ROCKET.csv\"\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {}\n",
    "    res[\"Accuracy\"] = (y_test == y_pred).mean().round(digits)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "    res[\"Precision\"] = round(precision, digits)\n",
    "    res[\"Recall\"] = round(recall, digits)\n",
    "    res[\"F1\"] = round(f1, digits)\n",
    "\n",
    "    # Verifica per AUC solo se ci sono entrambe le classi\n",
    "    if y_proba is not None:\n",
    "        unique_classes = np.unique(y_test)\n",
    "        if len(unique_classes) > 1:\n",
    "            res[\"AUC_PR\"] = round(average_precision_score(y_test, y_proba), digits)\n",
    "            res[\"AUC_ROC\"] = round(roc_auc_score(y_test, y_proba), digits)\n",
    "        else:\n",
    "            res[\"AUC_PR\"] = None\n",
    "            res[\"AUC_ROC\"] = None\n",
    "    return res\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"Channel\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"MCC\", \"AUC_ROC\", \"AUC_PR\"])\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "# Itera su tutti i canali del dataset\n",
    "for channel_id in NASA.channel_ids:\n",
    "    # if channel_id == \"D-12\" or channel_id == \"T-10\" or channel_id == \"T-9\":   # Non uso questi perchè NeirestNeigthbor dato che necessita di avere più \n",
    "    #     continue\n",
    "    if channel_id == \"T-10\":\n",
    "        continue\n",
    "    print(f\"Processing channel: {channel_id}\")\n",
    "\n",
    "    # Lista per memorizzare i segmenti di training\n",
    "    X_train_final = []\n",
    "\n",
    "    # Uso del dataset NASA per tutti i canali\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\")\n",
    "    # print(dataset.data.shape)\n",
    "    data = dataset.data\n",
    "    train = []\n",
    "    for i in range(0, data.shape[0] - STEP +1, OFFSET): \n",
    "        train.append(data[i:i+STEP])\n",
    "\n",
    "    train = np.stack(train)\n",
    "    # print(\"train: \", train.shape)  # Mostra le prime 5 righe dell'array\n",
    "\n",
    "    # ======================= FIT e PREDICT e SCORE =============================\n",
    "    dataset = NASA(\"./datasets\", channel_id, mode=\"anomaly\", train=False)\n",
    "    data = dataset.data\n",
    "    Test = []\n",
    "    output = []\n",
    "    o = np.zeros(data.shape[0])\n",
    "    for start,end in dataset.anomalies:\n",
    "        o[start:end] = 1\n",
    "    for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "        Test.append(data[i:i+STEP])\n",
    "        output.append(o[i:i+STEP])\n",
    "\n",
    "    output = np.stack(output)\n",
    "    Test = np.stack(Test)\n",
    "\n",
    "    num_kernels = 10000\n",
    "    rocket_transformer = Rocket(num_kernels = num_kernels, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Applica i kernel alle serie temporali\n",
    "    features_train = rocket_transformer.fit_transform(train)\n",
    "    features_test = rocket_transformer.transform(Test)\n",
    "\n",
    "    # Addestramento del modello supervisionato\n",
    "    # Addestramento del modello supervisionato\n",
    "    model = KNN(n_neighbors=2)\n",
    "    model.fit(features_train)\n",
    "\n",
    "    # Predizione delle anomalie nei dati di test\n",
    "    y_pred = model.predict(features_test)\n",
    "    y_proba = model.decision_function(features_test)\n",
    "\n",
    "    # Visualizzazione dei risultati\n",
    "    # print(\"Predizioni nel test set:\", y_pred)\n",
    "    # print(\"Predizioni nel test set:\", y_proba)\n",
    "\n",
    "    # Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "    threshold = 25 # -> 10%\n",
    "    # Conta il numero di 1 in ogni lista\n",
    "    counts = np.sum(output, axis=1)\n",
    "    output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "    print(\"Output: \", output)\n",
    "    print(\"predicted_anomalies: \", y_pred)\n",
    "    metrics = evaluate_metrics(output, y_pred, y_proba)\n",
    "    print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "    # Calcolo di VP, VN, FP, FN\n",
    "    TP = ((output == 1) & (y_pred == 1)).sum()\n",
    "    TN = ((output == 0) & (y_pred == 0)).sum()\n",
    "    FP = ((output == 0) & (y_pred == 1)).sum()\n",
    "    FN = ((output == 1) & (y_pred == 0)).sum()\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        \"Channel\": channel_id,\n",
    "        \"Accuracy\": metrics.get(\"Accuracy\", 0),\n",
    "        \"Precision\": metrics.get(\"Precision\", 0),\n",
    "        \"Recall\": metrics.get(\"Recall\", 0),\n",
    "        \"MCC\": metrics.get(\"MCC\", 0),\n",
    "        \"AUC_PR\": metrics.get(\"AUC_PR\", 0),\n",
    "        \"AUC_ROC\": metrics.get(\"AUC_ROC\", 0),\n",
    "        \"F1\": metrics.get(\"F1\", 0),\n",
    "        \"TP\": TP,\n",
    "        \"TN\": TN,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    print(\"=========================FINE CHANNEL=============================\")\n",
    "# ======================= SALVATAGGIO RISULTATI =============================\n",
    "results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"Risultati salvati in {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "933b5236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie delle colonne numeriche:\n",
      "Accuracy      0.725074\n",
      "Precision     0.307284\n",
      "Recall        0.548975\n",
      "F1            0.338012\n",
      "MCC           0.000000\n",
      "AUC_ROC       0.751304\n",
      "AUC_PR        0.575190\n",
      "TP            2.024691\n",
      "TN           15.987654\n",
      "FP            4.802469\n",
      "FN            1.901235\n",
      "dtype: float64\n",
      "========= CALCOLI CON TP...\n",
      "Accuracy: 0.6003996003996004\n",
      "Precision: 0.09847198641765705\n",
      "Recall: 0.17737003058103976\n",
      "F1 Score: 0.12663755458515283\n",
      "MCC: -0.13373121449680916\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "df = pd.read_csv(\"risultatiNASA_ROCKET.csv\")\n",
    "\n",
    "# Calcola la media delle colonne numeriche\n",
    "column_means = df.mean(numeric_only=True)\n",
    "\n",
    "# Stampa le medie\n",
    "print(\"Medie delle colonne numeriche:\")\n",
    "print(column_means)\n",
    "\n",
    "print(\"========= CALCOLI CON TP...\")\n",
    "df = pd.read_csv(file_path)\n",
    "TP = df[\"TP\"].sum()\n",
    "TN = df[\"TN\"].sum()\n",
    "FP = df[\"FP\"].sum()\n",
    "FN = df[\"FN\"].sum()\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "mcc = (TP*TN-FP*FN)/(math.sqrt((TP + FP)*(TP+FN)*(TN+TP)*(TN+FN)))\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"MCC:\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec005e",
   "metadata": {},
   "source": [
    "#### Senza KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d276378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2648, 25)\n",
      "train:  (10, 6250)\n",
      "TEST:  (31, 6250)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.935, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from NASA.nasa import NASA\n",
    "from valutazione_metriche import evaluate_metrics\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Uso del dataset NASA per tutti i canali\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\")\n",
    "print(dataset.data.shape)\n",
    "data = dataset.data\n",
    "train = []\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    train.append(data[i:i+STEP])\n",
    "\n",
    "train = np.stack(train)\n",
    "\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "\n",
    "dataset = NASA(\"./datasets\", NASA.channel_ids[1], mode=\"anomaly\", train=False)\n",
    "data = dataset.data\n",
    "Test = []\n",
    "output = []\n",
    "o = np.zeros(data.shape[0])\n",
    "for start,end in dataset.anomalies:\n",
    "    o[start:end] = 1\n",
    "for i in range(0, data.shape[0] - STEP +1, STEP): \n",
    "    Test.append(data[i:i+STEP])\n",
    "    output.append(o[i:i+STEP])\n",
    "\n",
    "output = np.stack(output)\n",
    "Test = np.stack(Test)\n",
    "\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "# input_length = train.shape[0]\n",
    "num_kernels = 10000\n",
    "\n",
    "train = train.reshape(train.shape[0], -1)  # Da 3D a 2D\n",
    "Test = Test.reshape(Test.shape[0], -1)\n",
    "print(\"train: \", train.shape) \n",
    "print(\"TEST: \", Test.shape)\n",
    "\n",
    "kernels = generate_kernels(STEP, num_kernels)\n",
    "\n",
    "train = train.astype(np.float64)\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "Test = Test.astype(np.float64)\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(train, kernels)\n",
    "\n",
    "features_test = apply_kernels(Test, kernels)\n",
    "\n",
    "\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  \n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  \n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Scegliere se una sequenda è un anomalia o no -> 10%\n",
    "threshold = 25 # -> 10%\n",
    "# Conta il numero di 1 in ogni lista\n",
    "counts = np.sum(output, axis=1)\n",
    "output = np.where(counts >= threshold, 1, 0)\n",
    "\n",
    "\n",
    "print(output)\n",
    "metrics = evaluate_metrics(output, anomaly_labels_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d652e18",
   "metadata": {},
   "source": [
    "# Finale Prove ROCKAD -> ModelSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e776fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_26484\\4218593192.py:30: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.477, 'Precision': 0.477, 'Recall': 1.0, 'F1': 0.646, 'MCC': 0.0, 'AUC_PR': 0.405, 'AUC_ROC': 0.299, 'PREC_N_SCORES': 0.29}\n"
     ]
    }
   ],
   "source": [
    "# ======================= ELABORAZIONE DATI TRAINING =============================\n",
    "X_train_final = []\n",
    "y_train_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"]\n",
    "        y_trainS = dfSegment.loc[mask, \"anomaly\"].reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            X_train_final.append(X_trainS[i:i + STEP])\n",
    "            y_train_final.append(y_trainS[i])\n",
    "\n",
    "X_train = np.array(X_train_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_train = np.array(y_train_final)\n",
    "\n",
    "# ======================= ELABORAZIONE DATI TEST =============================\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "X_test = np.array(X_test_final).reshape(-1, STEP, 1).transpose(0, 2, 1)\n",
    "y_test = np.array(y_test_final)\n",
    "\n",
    "# ======================= PRE-PROCESSING =============================\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[2])).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[2])).reshape(X_test.shape)\n",
    "\n",
    "# ======================= FIT e PREDICT e SCORE =============================\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=20000, n_jobs=-1, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "\n",
    "score_train = rockad.predict_proba(X_train).reshape(-1, 1)\n",
    "score_test = rockad.predict_proba(X_test).reshape(-1, 1)\n",
    "\n",
    "decision_func = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "decision_func.fit(score_train, y_train)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "result_binary = np.where(result == -1, 0, 1)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result_binary, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
